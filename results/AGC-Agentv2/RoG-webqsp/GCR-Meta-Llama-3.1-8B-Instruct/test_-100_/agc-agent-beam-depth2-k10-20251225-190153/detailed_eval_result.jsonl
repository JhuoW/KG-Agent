{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Belize\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Storm Keith"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician"], "ground_truth": ["Speaker of the United States House of Representatives", "United States Representative", "Governor of Tennessee"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.47058823529411764, "path_precision": 0.4444444444444444, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0110s7g9\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Darkness More Than Night\n# Answer:\nDetective"], "ground_truth": ["Hannah Gunn", "Melinda McGraw", "Ilyssa Fradin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["Miami Heat", "LSU Tigers men's basketball", "Phoenix Suns", "Orlando Magic", "Cleveland Cavaliers", "Los Angeles Lakers", "Boston Celtics"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.included_in_group -> European American\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Chandler Muriel Bing\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\nItalian American"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.countries_spoken_in -> Laos\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.region -> Asia\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Mon Language", "Phu Thai language", "Cham language", "Mlabri Language", "Thai Language", "Nyaw Language", "Saek language", "Akha Language", "Vietnamese Language", "Lao Language", "Hmong language", "Khmer language", "Malay, Pattani Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 1.0, "ans_recall": 0.3076923076923077, "path_f1": 0.3428571428571429, "path_precision": 0.6666666666666666, "path_recall": 0.23076923076923078, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 1.0, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Karen Kempner\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Facebook, Inc.\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social Network (Consumer)\n# Answer:\nFacebook"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer -> common.topic.notable_types -> Profession\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician"], "ground_truth": ["Archaeologist", "Architect", "Teacher", "Inventor", "Farmer", "Writer", "Statesman", "Lawyer", "Author", "Philosopher"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.509090909090909, "path_ans_precision": 0.7, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Biology\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> London\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Evolution by natural selection", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Correspondence of Charles Darwin, Volume 7", "The autobiography of Charles Darwin, 1809-1882", "The Correspondence of Charles Darwin, Volume 9: 1861", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Correspondence of Charles Darwin, Volume 10: 1862", "The foundations of the Origin of species", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "On the Movements and Habits of Climbing Plants", "Origin of Species (Everyman's University Paperbacks)", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The descent of man and selection in relation to sex.", "Motsa ha-minim", "The expression of the emotions in man and animals", "On the origin of species by means of natural selection", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Proiskhozhdenie vidov", "Tesakneri tsagume\u030c", "The Autobiography of Charles Darwin", "The living thoughts of Darwin", "From So Simple a Beginning", "The Origin of Species (Great Books : Learning Channel)", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Essential Darwin", "The Autobiography of Charles Darwin (Dodo Press)", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Leben und Briefe von Charles Darwin", "On evolution", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 3", "Beagle letters", "Origin of Species (Harvard Classics, Part 11)", "Rejse om jorden", "The Voyage of the Beagle (Mentor)", "The Autobiography of Charles Darwin (Large Print)", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Structure and Distribution of Coral Reefs", "The origin of species : complete and fully illustrated", "The Origin of Species (Enriched Classics)", "More Letters of Charles Darwin", "The Voyage of the Beagle (Adventure Classics)", "Darwin en Patagonia", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "H.M.S. Beagle in South America", "Darwin's Ornithological notes", "A student's introduction to Charles Darwin", "The expression of the emotions in man and animals.", "Darwin", "Darwin's insects", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "Fertilisation of Orchids", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 6", "Voyage of the Beagle (Dover Value Editions)", "Autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 4", "Diary of the voyage of H.M.S. Beagle", "The autobiography of Charles Darwin", "Les moyens d'expression chez les animaux", "The Different Forms of Flowers on Plants of the Same Species", "A Darwin Selection", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Correspondence of Charles Darwin, Volume 10", "The education of Darwin", "Voyage of the Beagle", "La vie et la correspondance de Charles Darwin", "The voyage of Charles Darwin", "Voyage Of The Beagle", "The Correspondence of Charles Darwin, Volume 15: 1867", "Opsht\u0323amung fun menshen", "Die geschlechtliche Zuchtwahl", "The Expression of the Emotions in Man And Animals", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 18: 1870", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Memorias y epistolario i\u0301ntimo", "Questions about the breeding of animals", "On Natural Selection", "Diario del Viaje de Un Naturalista Alrededor", "Darwin's notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Darwin Darwin", "The Correspondence of Charles Darwin, Volume 11: 1863", "Charles Darwin's natural selection", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "On a remarkable bar of sandstone off Pernambuco", "Darwin Compendium", "The structure and distribution of coral reefs.", "Evolution and natural selection", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 13", "The structure and distribution of coral reefs", "Charles Darwin's marginalia", "The Descent of Man and Selection in Relation to Sex", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The voyage of the Beagle.", "The Origin of Species (Oxford World's Classics)", "The principal works", "The Correspondence of Charles Darwin, Volume 5", "Kleinere geologische Abhandlungen", "The Variation of Animals and Plants under Domestication", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Origin of Species (World's Classics)", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Del Plata a Tierra del Fuego", "The Origin Of Species", "The Autobiography of Charles Darwin [EasyRead Edition]", "Voyage of the Beagle (NG Adventure Classics)", "The Autobiography of Charles Darwin, and selected letters", "Voyage d'un naturaliste autour du monde", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 9", "The Correspondence of Charles Darwin, Volume 12", "monograph on the sub-class Cirripedia", "The descent of man, and selection in relation to sex.", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "red notebook of Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "On the tendency of species to form varieties", "Darwin and Henslow", "The Correspondence of Charles Darwin, Volume 14", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 8", "Darwin for Today", "The Voyage of the Beagle (Unabridged Classics)", "Insectivorous Plants", "To the members of the Down Friendly Club", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Notes on the fertilization of orchids", "Geological Observations on South America", "The Expression Of The Emotions In Man And Animals", "Notebooks on transmutation of species", "Darwin on humus and the earthworm", "The Darwin Reader Second Edition", "The Orgin of Species", "El Origin De Las Especies", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Die fundamente zur entstehung der arten", "The Voyage of the Beagle (Everyman Paperbacks)", "ontstaan der soorten door natuurlijke teeltkeus", "The Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 12: 1864", "Metaphysics, Materialism, & the evolution of mind", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 17: 1869", "Reise um die Welt 1831 - 36", "From so simple a beginning", "Origin of Species", "Reise eines Naturforschers um die Welt", "The\u0301orie de l'e\u0301volution", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Structure And Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 1", "Voyage of the Beagle (Harvard Classics, Part 29)", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Autobiography Of Charles Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Darwinism stated by Darwin himself", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Evolution", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Cartas de Darwin 18251859", "The Voyage of the Beagle (Great Minds Series)", "Resa kring jorden", "The portable Darwin", "Origins", "Darwin's journal", "Charles Darwin on the routes of male humble bees", "The Descent of Man, and Selection in Relation to Sex", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 2", "The origin of species", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Origin of Species", "The geology of the voyage of H.M.S. Beagle", "The collected papers of Charles Darwin", "La facult\u00e9 motrice dans les plantes", "Works", "The Origin of Species (Mentor)", "vari\u00eberen der huisdieren en cultuurplanten", "Charles Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Charles Darwin's letters", "The Origin of Species (Variorum Reprint)", "Les mouvements et les habitudes des plantes grimpantes", "The Correspondence of Charles Darwin, Volume 11", "genese\u014ds t\u014dn eid\u014dn", "The action of carbonate of ammonia on the roots of certain plants", "The descent of man, and selection in relation to sex", "Het uitdrukken van emoties bij mens en dier", "The Darwin Reader First Edition", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Correspondence of Charles Darwin, Volume 15", "The Origin of Species (Collector's Library)", "The Origin of Species (Great Minds Series)"], "ans_acc": 0.06542056074766354, "ans_hit": 1, "ans_f1": 0.018348623853211007, "ans_precission": 0.5, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.12280701754385964, "path_ans_precision": 1.0, "path_ans_recall": 0.06542056074766354}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_hints.included_types -> Person\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> type.type.expected_by -> Person\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_hints.included_types -> Topic\n# Answer:\nOrganization leader"], "ground_truth": ["Florida Gators football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Denver Broncos"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> base.locations.countries.continent -> Europe\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia -> location.location.containedby -> Eurasia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> \u013dadov\u00fd \u0161t\u00edt -> location.location.containedby -> Europe\n# Answer:\n\u013dadov\u00fd \u0161t\u00edt", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Czech Republic\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> location.location.containedby -> Eurasia\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic -> location.location.containedby -> Eurasia\n# Answer:\nCzech Republic"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Poet\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Bard", "Writer", "Poet", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> common.topic.notable_types -> City/Town/Village\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05ckldy\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.locations -> Iraqi Kurdistan\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05cklf2\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05cklf7\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> common.topic.notable_for -> g.125cxty6s\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Saudi Arabia", "Argentina", "United Kingdom", "Iraq", "Australia", "France", "United States of America"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 1.0, "ans_recall": 0.14285714285714285, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.25, "path_ans_precision": 1.0, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-119", "prediction": [], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> common.topic.notable_for -> g.1255tjcrg\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio"], "ground_truth": ["Return J. Meigs, Jr.", "John Kasich", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": [], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> g.11b674hjl7\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> common.topic.article -> m.0r4xz\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Salamanca 2008\n# Answer:\nSalamanca"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Blaenau Gwent\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Bridgend County Borough\n# Answer:\nWales"], "ground_truth": ["Northern Ireland", "Scotland", "Wales", "England"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon -> location.location.partially_containedby -> Arkansas\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Lake Maurepas\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_contained_by -> m.0wjpmv2\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> common.topic.notable_for -> g.125bn2xvk\n# Answer:\nBayou Bartholomew"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> people.person.profession -> Philosopher\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Physician\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.nationality -> United Kingdom\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Philosopher\n# Answer:\nAlvin Plantinga"], "ground_truth": ["Philosopher", "Writer", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan"], "ground_truth": ["Francine Lons", "Leon Cole", "Sal Gibson"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7659574468085106, "ans_precission": 0.9, "ans_recall": 0.6666666666666666, "path_f1": 0.7659574468085106, "path_precision": 0.9, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7659574468085106, "path_ans_precision": 0.9, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.image -> Dexter Avenue Baptist\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> influence.influence_node.influenced_by -> Arthur Conan Doyle\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel -> book.author.school_or_movement -> Modernism\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Novelist\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore -> influence.influence_node.influenced_by -> Arthur Machen\n# Answer:\nAlan Moore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> freebase.valuenotation.has_value -> Education\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> common.topic.image -> Edgar Allan Poe signature\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.geolocation -> m.0clwfck\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> g.11b7v_3l1h\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport"], "ground_truth": ["Fernbank Museum of Natural History", "Turner Field", "Georgia Aquarium", "Centennial Olympic Park", "Martin Luther King, Jr. National Historic Site", "Georgia World Congress Center", "Atlanta Jewish Film Festival", "Center for Puppetry Arts", "Six Flags White Water", "The Tabernacle", "Atlanta Marriott Marquis", "Omni Coliseum", "Woodruff Arts Center", "Georgia Dome", "World of Coca-Cola", "Hyatt Regency Atlanta", "CNN Center", "Atlanta Cyclorama & Civil War Museum", "Philips Arena", "Cobb Energy Performing Arts Centre", "Atlanta History Center", "Fernbank Science Center", "Fox Theatre", "Underground Atlanta", "Jimmy Carter Library and Museum", "Atlanta Symphony Orchestra", "Zoo Atlanta", "Variety Playhouse", "Margaret Mitchell House & Museum", "Peachtree Road Race", "Four Seasons Hotel Atlanta", "Masquerade", "Georgia State Capitol", "Six Flags Over Georgia", "Atlanta Ballet", "Arbor Place Mall"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.1509433962264151, "ans_precission": 0.8, "ans_recall": 0.08333333333333333, "path_f1": 0.1509433962264151, "path_precision": 0.8, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19512195121951217, "path_ans_precision": 0.8, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> David Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> Alakdana\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> media_common.literary_genre.books_in_this_genre -> Amis and Amiloun\n# Answer:\nChivalric romance"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Federal monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.location.time_zones -> Cocos Islands\u00a0Time Zone\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> g.11bymn224r\n# Answer:\nChristmas Island"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton"], "ground_truth": ["Carolina Panthers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11x1cfntl\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> common.topic.notable_for -> g.125dtp7bg\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.ethnicity -> White people\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney"], "ground_truth": ["Ralph Nader", "Michael Peroutka", "Gene Amondson", "John Kerry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nThomas Ferguson"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> base.aareas.schema.administrative_area.administrative_parent -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.partially_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Bartolom\u00e9 Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBartolom\u00e9 Island"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.album -> Mistletoe -> music.album.release_type -> Single\n# Answer:\nMistletoe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Dre & Vidal\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.album.release_type -> Single\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Andre Harris\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> common.topic.notable_for -> g.1yl5r3f4x\n# Answer:\nAll Bad"], "ground_truth": ["Thought Of You", "All Bad", "Recovery", "Bigger", "Never Let You Go", "Lolly", "Right Here", "Eenie Meenie", "Hold Tight", "As Long as You Love Me", "Bad Day", "Wait for a Minute", "Beauty And A Beat", "Change Me", "Heartbreaker", "All Around The World", "Pray", "Home to Mama", "Live My Life", "Boyfriend", "All That Matters", "First Dance", "Baby", "#thatPower", "Somebody to Love", "Roller Coaster", "Turn to You (Mother's Day Dedication)", "Die in Your Arms", "PYD", "Never Say Never", "Confident"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.12, "ans_precission": 0.8571428571428571, "ans_recall": 0.06451612903225806, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.12, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> common.topic.notable_types -> Transit Stop\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman"], "ground_truth": ["Publisher", "Physician", "Journalist", "Writer", "Statesman"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": [], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Iowa\n# Answer:\nUnited States of America"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles County\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Chris Snail\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Deborah Lurie\n# Answer:\nMission Hills"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.923076923076923, "ans_precission": 0.8571428571428571, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> base.aareas.schema.administrative_area.administrative_parent -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Textron\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Time Warner\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> common.topic.article -> m.0h6dr\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir"], "ground_truth": ["Masih ad-Dajjal", "Islamic view of angels", "Prophets in Islam", "God in Islam", "Qiyamah", "\u1e6c\u016bb\u0101", "Sharia", "Entering Heaven alive", "Tawhid", "Predestination in Islam", "Mahdi", "Islamic holy books", "Monotheism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_for -> g.12599cgh4\n# Answer:\nA dirty joke is a sort of mental rebellion."], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.25, "path_recall": 0.09090909090909091, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter"], "ground_truth": ["Actor", "Songwriter", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.geometry -> m.057rh7c\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.locations.states_and_provences.country -> United States of America\n# Answer:\nKansas"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": [], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> comic_strips.comic_strip_creator.comic_strips_written -> m.0gwbjrm\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.expected_by -> Appears in plays\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_winner.awards_won -> m.0_5xrrh\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010by0k1\n# Answer:\nThomas Meehan"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Mobile County\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> common.topic.notable_types -> Author\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt -> people.deceased_person.place_of_death -> Scottsdale\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> This I remember -> book.written_work.subjects -> 20th century\n# Answer:\nThis I remember", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world -> book.book_edition.isbn -> 9780231111812\n# Answer:\nCourage in a dangerous world"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": [], "ground_truth": ["Islam", "Catholicism", "Hinduism", "Protestantism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Anna Lindh assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.contains -> American College of Technology\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Rifle\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.children -> Charles Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nFezziwig", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.010p_33b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.dates -> m.0c0z7kp\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> Gustave Flaubert\n# Answer:\nAlphonse Daudet", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> film.film_character.portrayed_in_films -> m.010p_0hz\n# Answer:\nFezziwig", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.011cd25b\n# Answer:\nGhost of Christmas Present"], "ground_truth": ["A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (BBC Audio Series)", "Great Expectations", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Longman Classics, Stage 2)", "The Pickwick Papers", "A Christmas Carol (Limited Editions)", "Dombey and son", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Dramatized)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A CHRISTMAS CAROL", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (R)", "A Tale of Two Cities (Wordsworth Classics)", "Bleak House.", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Radio Theatre)", "Great Expectations.", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Classics Illustrated)", "Our mutual friend.", "Dombey and Son", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Read & Listen Books)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Enriched Classic)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Christmas Carol (Soundings)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Masterworks)", "A Christmas Carol", "The cricket on the hearth", "A Tale of Two Cities (Longman Fiction)", "Great expectations", "A Tale of Two Cities (Paperback Classics)", "Dombey and Son.", "A Tale of Two Cities (Barnes & Noble Classics Series)", "Great expectations.", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Nelson Graded Readers)", "David Copperfield", "A Tale of Two Cities (Classic Literature with Classical Music)", "The old curiosity shop", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Piccolo Books)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Classics Illustrated Notes)", "Sketches by Boz", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "The Pickwick papers", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Acting Edition)", "The old curiosity shop.", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Cassette (1 Hr).)", "David Copperfield.", "A Tale of Two Cities (Dramascripts S.)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Watermill Classics)", "The Mystery of Edwin Drood", "A Tale of Two Cities", "A Christmas Carol (Value Books)", "A Christmas Carol. (Lernmaterialien)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Tor Classics)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Penguin Popular Classics)", "Bleak house", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol (Take Part)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (The Classic Collection)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Watermill Classic)", "Martin Chuzzlewit", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Simple English)", "The mystery of Edwin Drood", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Christmas Carol (Classic Collection)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "Hard times", "Our mutual friend", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (Adopted Classic)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Illustrated Classics)", "Oliver Twist", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Cyber Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Collector's Library)", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Acting Edition)", "The Old Curiosity Shop", "A Tale of Two Cities (10 Cassettes)", "Little Dorrit", "A Christmas Carol (Clear Print)", "The life and adventures of Nicholas Nickleby", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Puffin Choice)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "Bleak House", "A Christmas Carol (Penguin Readers, Level 2)"], "ans_acc": 0.011834319526627219, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.21176470588235294, "path_precision": 0.3, "path_recall": 0.16363636363636364, "path_ans_f1": 0.022770398481973434, "path_ans_precision": 0.3, "path_ans_recall": 0.011834319526627219}
{"id": "WebQTest-65", "prediction": [], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> common.topic.article -> m.09nqr\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.04kq24y\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.geographic_scope -> United States of America\n# Answer:\nFrontpoint Security Solutions"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Cholecalciferol\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.risk_factor.diseases -> Ptosis\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Adjuvant\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> common.topic.webpage -> m.09v_zjv\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Blumer's shelf\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Bone pain\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> common.topic.webpage -> m.09wswdq\n# Answer:\nLung cancer"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4m5\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.02h9cb0\n# Answer:\nKnight Rider - Season 4"], "ground_truth": ["William Daniels"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.population -> g.11b674pwdr\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1ct3wq\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy -> location.location.geolocation -> m.0127s0kf\n# Answer:\nCarver Academy"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.39999999999999997, "path_precision": 0.42857142857142855, "path_recall": 0.375, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of Port Republic\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> time.event.included_in_event -> American Civil War\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.locations -> Shenandoah Valley\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> military.military_conflict.military_personnel_involved -> Adolph von Steinwehr\n# Answer:\nJackson's Valley Campaign"], "ground_truth": ["Battle of Chancellorsville", "Battle of White Oak Swamp", "Battle of McDowell", "Battle of Hancock", "Battle of Port Republic", "Second Battle of Bull Run", "Manassas Station Operations", "Battle of Hoke's Run", "American Civil War", "Battle of Front Royal", "Romney Expedition", "Battle of Chantilly", "First Battle of Winchester", "Battle of Harpers Ferry", "Battle of Cedar Mountain", "How Few Remain", "Jackson's Valley Campaign", "First Battle of Rappahannock Station", "First Battle of Kernstown"], "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.8571428571428571, "ans_recall": 0.15789473684210525, "path_f1": 0.11764705882352941, "path_precision": 0.2857142857142857, "path_recall": 0.07407407407407407, "path_ans_f1": 0.40268456375838924, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Peter Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.deceased_person.place_of_burial -> Granary Burying Ground\n# Answer:\nAbiah Folger"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.symptoms -> Abdominal mass\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.symptoms -> Abdominal pain\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.symptoms -> Anorexia\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subjects -> Fabrice de Villeneuve\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.media -> Gesso\n# Answer:\nThe Last Supper"], "ground_truth": ["The Holy Infants Embracing", "Horse and Rider", "La belle ferronni\u00e8re", "Virgin of the Rocks", "The Virgin and Child with St Anne and St John the Baptist", "The Last Supper", "St. John the Baptist", "Lady with an Ermine", "The Battle of Anghiari", "Portrait of a man in red chalk", "g.1213jb_b", "g.1219sb0g", "St. Jerome in the Wilderness", "Annunciation", "Vitruvian Man", "Mona Lisa", "Sala delle Asse", "Portrait of a Young Fianc\u00e9e", "g.120vt1gz", "g.121wt37c", "Benois Madonna", "Ginevra de' Benci", "Portrait of Isabella d'Este", "Adoration of the Magi", "g.12215rxg", "Bacchus", "g.121yh91r", "g.1224tf0c", "Leonardo's horse", "Lucan portrait of Leonardo da Vinci", "Medusa", "Madonna of the Yarnwinder", "Leda and the Swan", "g.12314dm1", "Madonna of the Carnation", "Madonna of Laroque", "Head of a Woman", "The Virgin and Child with St. Anne", "Drapery for a Seated Figure", "Madonna and Child with St Joseph", "Madonna Litta", "Portrait of a Musician", "Salvator Mundi", "The Baptism of Christ", "g.1239jd9p"], "ans_acc": 0.044444444444444446, "ans_hit": 1, "ans_f1": 0.07741935483870968, "ans_precission": 0.3, "ans_recall": 0.044444444444444446, "path_f1": 0.07894736842105263, "path_precision": 0.3, "path_recall": 0.045454545454545456, "path_ans_f1": 0.07741935483870968, "path_ans_precision": 0.3, "path_ans_recall": 0.044444444444444446}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> common.topic.notable_types -> Administrative Division\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> Anticancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> biology.hybrid_parent_gender.hybrids -> m.0blp57t\n# Answer:\nFemale"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> An Armonica\n# Answer:\nGlass harmonica"], "ground_truth": ["Glass harmonica", "Franklin stove", "Lightning rod", "Bifocals"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States, with Territories\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nColorado"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.012ggp0l\n# Answer:\nComposer"], "ground_truth": ["Librettist", "Musician", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Gamprin\n# Answer:\nLiechtenstein"], "ground_truth": ["Luxembourg", "Liechtenstein", "East Germany", "Austria", "Germany", "Belgium", "Switzerland"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5783132530120482, "ans_precission": 0.8888888888888888, "ans_recall": 0.42857142857142855, "path_f1": 0.417910447761194, "path_precision": 0.7777777777777778, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.subject_of -> Stephen Melton\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.notable_types -> Musical genre\n# Answer:\nPop music"], "ground_truth": ["Art rock", "Pop rock", "Rock music", "Psychedelic rock", "Soft rock", "Blues rock", "Experimental rock", "Pop music", "Experimental music"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6870229007633587, "path_ans_precision": 0.9, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.general_election -> United States Senate elections, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Parliamentary system\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark -> location.location.contains -> Denmark\n# Answer:\nKingdom of Denmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.containedby -> United States of America\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> location.location.containedby -> United States of America\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1201 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geometry -> m.055f4wk\n# Answer:\n98102"], "ground_truth": ["98155", "98158", "98184", "98171", "98138", "98119", "98108", "98126", "98104", "98103", "98198", "98133", "98178", "98154", "98105", "98174", "98107", "98122", "98118", "98177", "98136", "98144", "98164", "98117", "98125", "98131", "98127", "98146", "98199", "98111", "98160", "98195", "98119-4114", "98102", "98113", "98170", "98124", "98190", "98148", "98109", "98101", "98166", "98134", "98188", "98112", "98191", "98175", "98115", "98129", "98168", "98194", "98116", "98161", "98121", "98181", "98106", "98132", "98139", "98114", "98185", "98145", "98165", "98141"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.09032258064516129, "ans_precission": 0.875, "ans_recall": 0.047619047619047616, "path_f1": 0.09032258064516129, "path_precision": 0.875, "path_recall": 0.047619047619047616, "path_ans_f1": 0.09032258064516129, "path_ans_precision": 0.875, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_sy\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_t2\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Old Mandarin\n# Answer:\n'Phags-pa script"], "ground_truth": ["Chinese characters", "Simplified Chinese character", "Traditional Chinese characters", "'Phags-pa script", "N\u00fcshu script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6774193548387097, "ans_precission": 0.7777777777777778, "ans_recall": 0.6, "path_f1": 0.6774193548387097, "path_precision": 0.7777777777777778, "path_recall": 0.6, "path_ans_f1": 0.6774193548387097, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> common.topic.notable_for -> g.125b3gc_h\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Sarah Ann Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Donald Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Damon Evans", "Sherman Hemsley", "Jay Hammer", "Roxie Roker", "Zara Cully", "Marla Gibbs", "Paul Benedict", "Franklin Cover", "Berlinda Tolbert", "Isabel Sanford", "Mike Evans"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.newspaper.issues -> San Francisco Bay Guardian, 24 Nov 1999\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906"], "ground_truth": ["San Francisco Chronicle", "Dock of the Bay", "Sing Tao Daily", "Bay Area Reporter", "San Francisco Bay Guardian", "Street Sheet", "Free Society", "The San Francisco Examiner", "Synapse", "California Star", "San Francisco Call", "The Daily Alta California", "San Francisco Bay View", "San Francisco Daily", "San Francisco News-Call Bulletin Newspaper", "San Francisco Bay Times", "San Francisco Foghorn", "San Francisco Business Times", "AsianWeek", "The Golden Era"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24705882352941178, "ans_precission": 0.7, "ans_recall": 0.15, "path_f1": 0.24705882352941178, "path_precision": 0.7, "path_recall": 0.15, "path_ans_f1": 0.24705882352941178, "path_ans_precision": 0.7, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Verapamil\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Evolution by natural selection", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Correspondence of Charles Darwin, Volume 9: 1861", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Correspondence of Charles Darwin, Volume 10: 1862", "The foundations of the Origin of species", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "On the Movements and Habits of Climbing Plants", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Wu zhong qi yuan", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Motsa ha-minim", "On the origin of species by means of natural selection", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Proiskhozhdenie vidov", "Tesakneri tsagume\u030c", "The Autobiography of Charles Darwin", "The living thoughts of Darwin", "Voyage d'un naturaliste autour du monde", "The Essential Darwin", "Leben und Briefe von Charles Darwin", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "On evolution", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Beagle letters", "Rejse om jorden", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Structure and Distribution of Coral Reefs", "More Letters of Charles Darwin", "Darwin en Patagonia", "H.M.S. Beagle in South America", "Darwin's Ornithological notes", "A student's introduction to Charles Darwin", "Darwin's insects", "Darwin", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "South American Geology", "Fertilisation of Orchids", "The Formation of Vegetable Mould through the Action of Worms", "Diary of the voyage of H.M.S. Beagle", "Les moyens d'expression chez les animaux", "The Different Forms of Flowers on Plants of the Same Species", "The Life and Letters of Charles Darwin Volume 1", "A Darwin Selection", "The education of Darwin", "La vie et la correspondance de Charles Darwin", "The voyage of Charles Darwin", "The Correspondence of Charles Darwin, Volume 15: 1867", "Opsht\u0323amung fun menshen", "Die geschlechtliche Zuchtwahl", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 18: 1870", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Memorias y epistolario i\u0301ntimo", "Questions about the breeding of animals", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Evolutionary Writings: Including the Autobiographies", "On Natural Selection", "Diario del Viaje de Un Naturalista Alrededor", "Darwin's notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Darwin Darwin", "The Correspondence of Charles Darwin, Volume 11: 1863", "Charles Darwin's natural selection", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "On a remarkable bar of sandstone off Pernambuco", "Darwin Compendium", "Evolution and natural selection", "Gesammelte kleinere Schriften", "Charles Darwin's marginalia", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The principal works", "Kleinere geologische Abhandlungen", "Geological Observations on the Volcanic Islands", "The Variation of Animals and Plants under Domestication", "Del Plata a Tierra del Fuego", "Volcanic Islands", "monograph on the sub-class Cirripedia", "The Life and Letters of Charles Darwin Volume 2", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "red notebook of Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "On the tendency of species to form varieties", "Darwin and Henslow", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Insectivorous Plants", "Darwin for Today", "To the members of the Down Friendly Club", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Notes on the fertilization of orchids", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Geological Observations on South America", "Notebooks on transmutation of species", "Darwin on humus and the earthworm", "The Darwin Reader Second Edition", "The Orgin of Species", "El Origin De Las Especies", "Die fundamente zur entstehung der arten", "ontstaan der soorten door natuurlijke teeltkeus", "The Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 12: 1864", "Metaphysics, Materialism, & the evolution of mind", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 17: 1869", "Reise um die Welt 1831 - 36", "From so simple a beginning", "Reise eines Naturforschers um die Welt", "The\u0301orie de l'e\u0301volution", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 14: 1866", "Darwinism stated by Darwin himself", "Darwin from Insectivorous Plants to Worms", "Evolution", "Cartas de Darwin 18251859", "Resa kring jorden", "The portable Darwin", "Origins", "Darwin's journal", "Charles Darwin on the routes of male humble bees", "The Descent of Man, and Selection in Relation to Sex", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The geology of the voyage of H.M.S. Beagle", "The collected papers of Charles Darwin", "La facult\u00e9 motrice dans les plantes", "Works", "vari\u00eberen der huisdieren en cultuurplanten", "Charles Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Charles Darwin's letters", "Les mouvements et les habitudes des plantes grimpantes", "genese\u014ds t\u014dn eid\u014dn", "The action of carbonate of ammonia on the roots of certain plants", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Het uitdrukken van emoties bij mens en dier", "The Darwin Reader First Edition", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct"], "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> I've Been Good to You -> music.composition.recordings -> I've Been Good To You\n# Answer:\nI've Been Good to You", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness -> music.recording.releases -> Gettin' Ready\n# Answer:\nLittle Miss Sweetness"], "ground_truth": ["The Tracks of My Tears", "Going to a Go-Go", "Blame It On Love (Duet with Barbara Mitchell)", "I Second That Emotions", "Christmas Greeting", "Let Me Be The Clock", "More Love", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Photograph in My Mind", "I Hear The Children Singing", "You Made Me Feel Love", "Jesus Told Me To Love You", "Time Flies", "Jasmin", "Cruisin'", "Ain't That Peculiar", "You Take Me Away", "Yes It's You Lady", "I'll Keep My Light In My Window", "The Family Song", "I Love Your Face", "Girl I'm Standing There", "Pops, We Love You", "Noel", "Crusin'", "Nearness of You", "I've Made Love To You A Thousand Times", "Christmas Everyday", "Asleep on My Love", "Did You Know (Berry's Theme)", "So Bad", "Don't Play Another Love Song", "It's Her Turn to Live", "Just My Soul Responding", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Crusin", "The Track of My Tears", "Hanging on by a Thread", "Tell Me Tomorrow", "Speak Low", "Baby Come Close", "Fallin'", "I Can't Get Enough", "Tears of a Clown", "Driving Thru Life in the Fast Lane", "Being With You", "That Place", "I\u2019ve Got You Under My Skin", "Deck the Halls", "Heavy On Pride (Light On Love)", "You've Really Got a Hold on Me", "The Christmas Song", "We Are The Warriors", "Love Don't Give No Reason", "Some People Will Do Anything for Love", "Pops, We Love You (disco)", "Everything for Christmas", "Melody Man", "Walk on By", "Just Another Kiss", "I Second That Emotion", "Because of You It's the Best It's Ever Been", "One Time", "And I Love Her", "Wanna Know My Mind", "I Can't Find", "Christmas Every Day", "Get Ready", "Quiet Storm (single version)", "The Agony And The Ecstasy", "Winter Wonderland", "The Tears of a Clown", "Rack Me Back", "Ebony Eyes (Duet with Rick James)", "Happy (Love Theme From Lady Sings the Blues)", "Come to Me Soon", "Take Me Through The Night", "With Your Love Came", "You Really Got a Hold on Me", "Love Is The Light", "The Tracks of My Heart", "Te Quiero Como Si No Hubiera Un Manana", "Just Like You", "Satisfy You", "Coincidentally", "He Can Fix Anything", "The Love Between Me and My Kids", "Double Good Everything", "We've Saved the Best for Last", "I Am I Am", "Will You Love Me Tomorrow?", "Share It", "I Want You Back", "It's Time to Stop Shoppin' Around", "Mother's Son", "She's Only a Baby Herself", "If You Wanna Make Love", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Love Letters", "It's Christmas Time", "Daylight & Darkness", "When Smokey Sings Tears Of A Clown", "Blame It on Love", "Time After Time", "More Than You Know", "You've Really Go a Hold on Me", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "My Guy", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Be Kind To The Growing Mind (with The Temptations)", "Fly Me to the Moon (In Other Words)", "Rewind", "Our Love Is Here to Stay", "Be Careful What You Wish For (instrumental)", "Aqui Con Tigo (Being With You)", "Will You Still Love Me Tomorrow", "Baby That's Backatcha", "Virgin Man", "My Girl", "Cruisin", "I Praise & Worship You Father", "I Like Your Face", "Gone Forever", "Ooo Baby Baby (live)", "I Can't Give You Anything but Love", "Going to a Go Go", "A Tattoo", "Quiet Storm (Groove Boutique remix)", "If You Want My Love", "I've Got You Under My Skin", "The Tracks Of My Tears", "Quiet Storm (Groove Boutique Chill Jazz mix)", "No Time to Stop Believing", "Just Passing Through", "We\u2019ve Come Too Far to End It Now", "Fulfill Your Need", "The Road to Damascus", "Sweet Harmony", "Shoe Soul", "Going to a Gogo", "Don't Know Why", "I Know You by Heart", "Theme From the Big Time", "There Will Come A Day ( I'm Gonna Happen To You )", "Close Encounters of the First Kind", "No\u00ebl", "I'm Glad There Is You", "Let Your Light Shine On Me", "My World", "Tracks of My Tears", "The Tears Of A Clown", "Love' n Life", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "You Are Forever", "In My Corner", "Yester Love", "Will You Love Me Tomorrow", "Wedding Song", "Sleepless Nights", "It's A Good Night", "Let Me Be the Clock", "The Tracks of My Tears (live)", "Why", "Same Old Love", "Bad Girl", "Wishful Thinking", "If You Can Want", "One Heartbeat", "As You Do", "Just a Touch Away", "God Rest Ye Merry Gentlemen", "Please Don't Take Your Love (feat. Carlos Santana)", "Who's Sad", "Holly", "Really Gonna Miss You", "I Am, I Am", "Can't Fight Love", "You Don't Know What It's Like", "You're Just My Life (feat. India.Arie)", "Be Careful What You Wish For", "Quiet Storm", "Away in the Manger / Coventry Carol", "Be Who You Are", "Whatcha Gonna Do", "The Hurt's On You", "Love Don' Give No Reason (12 Inch Club Mix)", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "I Care About Detroit", "Hold on to Your Love", "Please Come Home for Christmas", "Tears Of A Clown", "(It's The) Same Old Love", "Keep Me", "You Go to My Head", "Never My Love / Never Can Say Goodbye", "Love Brought Us Here", "Unless You Do It Again", "Vitamin U", "Ooo Baby Baby", "Standing On Jesus", "Night and Day", "I've Made Love to You a Thousand Times", "Girlfriend", "Little Girl, Little Girl", "Santa Claus is Coming to Town", "There Will Come a Day (I'm Gonna Happen to You)", "Why Are You Running From My Love", "Why Do Happy Memories Hurt So Bad", "Tea for Two", "I'm in the Mood for Love", "Mickey's Monkey", "It's Fantastic", "And I Don't Love You", "Tell Me Tomorrow (12\\\" extended mix)", "Just to See Her", "Skid Row", "I Love The Nearness Of You", "You Cannot Laugh Alone", "If You Wanna Make Love (Come 'round Here)", "Season's Greetings from Smokey Robinson", "Ooh Baby Baby", "Open", "Jingle Bells", "Tracks Of My Tears (Live)", "Gang Bangin'", "You Are So Beautiful (feat. Dave Koz)", "What's Too Much", "Come by Here (Kum Ba Ya)", "Little Girl Little Girl", "Ever Had A Dream", "Shop Around", "A Silent Partner in a Three-Way Love Affair", "Train of Thought", "When A Woman Cries", "Love So Fine", "I Have Prayed On It", "Ebony Eyes", "You're the One for Me (feat. Joss Stone)", "Save Me", "A Child Is Waiting", "Tears of a Sweet Free Clown", "Just To See Her Again", "It's a Good Feeling", "Tell Me Tomorrow, Part 1", "Easy", "And I Don't Love You (Larry Levan instrumental dub)", "The Agony and the Ecstasy", "The Way You Do (The Things You Do)", "Medley: Never My Love / Never Can Say Goodbye", "Don't Wanna Be Just Physical", "Be Kind to the Growing Mind", "Love Bath", "I Can\u2019t Stand to See You Cry (Commercial version)", "Everything You Touch", "Tracks of my Tears", "Food For Thought"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.properties -> President number\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_hints.included_types -> Topic\n# Answer:\nArtwork"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> time.event.locations -> Czech Republic\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak\n# Answer:\nTorlakian dialect"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}

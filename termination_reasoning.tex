\paragraph{Termination Reasoning.} After extending the path with action $a_t$, the termination predictor evaluates the updated state $s_{t+1}$ and outputs one of three decisions:
\begin{equation}
\pi(s_{t+1}) \in \left\{\textsc{Answer}, \textsc{Continue}, \textsc{Backtrack}\right\},
\end{equation}
where \textsc{Answer} indicates that $e_{t+1}$ directly answers the question, \textsc{Continue} signals that further exploration is needed, and \textsc{Backtrack} suggests the current path is unpromising. The predictor is implemented by prompting the LLM with the reasoning context---comprising the question $q$, the reasoning path $\mathbf{p}_{0:t}$, and the current entity $e_{t+1}$---and constraining the output to one of the three action tokens. The confidence score $\mathrm{conf}(\pi) = P_\theta(\pi \mid s_{t+1})$ is extracted from the softmax distribution over the first generated token's logits. To prevent premature termination, \textsc{Answer} is accepted only when $\mathrm{conf}(\textsc{Answer}) > \delta$, where $\delta$ is a threshold hyperparameter; otherwise, the decision defaults to \textsc{Continue}. For efficiency, we skip the termination check at depth zero since topic entities rarely constitute valid answers, forcing immediate exploration. When \textsc{Backtrack} is selected, the beam reverts to its previous state and incurs a multiplicative penalty $\gamma < 1$ on its cumulative score, discouraging excessive backtracking while preserving recovery from unproductive paths.
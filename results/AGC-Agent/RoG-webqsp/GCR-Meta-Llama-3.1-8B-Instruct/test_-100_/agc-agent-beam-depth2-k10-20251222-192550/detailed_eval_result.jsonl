{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> fictional_universe.character_occupation.characters_with_this_occupation -> Alice Dingle\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> base.descriptive_names.names.descriptive_name -> m.0101h50d\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> location.location.time_zones -> Central Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> people.profession.specializations -> Cowboy\n# Answer:\nFarmer"], "ground_truth": ["United States Representative", "Governor of Tennessee", "Speaker of the United States House of Representatives"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.25, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.region -> Americas\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.923076923076923, "ans_precission": 0.8571428571428571, "ans_recall": 1.0, "path_f1": 0.923076923076923, "path_precision": 0.8571428571428571, "path_recall": 1.0, "path_ans_f1": 0.923076923076923, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0110s7g9\n# Answer:\nDetective"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.season -> 1992\u201393 NBA season\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["Orlando Magic", "LSU Tigers men's basketball", "Phoenix Suns", "Boston Celtics", "Los Angeles Lakers", "Miami Heat", "Cleveland Cavaliers"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1111111111111111, "path_precision": 0.3333333333333333, "path_recall": 0.06666666666666667, "path_ans_f1": 0.2, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> Mulberry Street NYC c1900 LOC 3g04637u edit\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Al Giardello\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nm.04fk_g9"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Khmer language", "Lao Language", "Thai Language", "Nyaw Language", "Akha Language", "Saek language", "Mlabri Language", "Vietnamese Language", "Hmong language", "Malay, Pattani Language", "Mon Language", "Cham language", "Phu Thai language"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.25974025974025977, "ans_precission": 0.8333333333333334, "ans_recall": 0.15384615384615385, "path_f1": 0.125, "path_precision": 0.3333333333333333, "path_recall": 0.07692307692307693, "path_ans_f1": 0.25974025974025977, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07 -> award.award_nomination.award -> NAACP Image Award for Outstanding Supporting Actor in a Motion Picture\n# Answer:\nm.0_rfq07", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07 -> award.award_nomination.award_nominee -> Justin Timberlake\n# Answer:\nm.0_rfq07", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkghb -> award.award_honor.award -> National Board of Review Award for Best Actor\n# Answer:\nm.0fpkghb", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkghb -> award.award_honor.award_winner -> Jesse Eisenberg\n# Answer:\nm.0fpkghb"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician"], "ground_truth": ["Statesman", "Writer", "Inventor", "Archaeologist", "Author", "Lawyer", "Architect", "Philosopher", "Farmer", "Teacher"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.17142857142857143, "ans_precission": 0.6, "ans_recall": 0.1, "path_f1": 0.17142857142857143, "path_precision": 0.6, "path_recall": 0.1, "path_ans_f1": 0.17142857142857143, "path_ans_precision": 0.6, "path_ans_recall": 0.1}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Biology\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> location.location.contains -> Charles Darwin University, Casuarina Campus\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.country -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["The principal works", "Geological Observations on South America", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Correspondence of Charles Darwin, Volume 12: 1864", "The education of Darwin", "Motsa ha-minim", "Darwin for Today", "ontstaan der soorten door natuurlijke teeltkeus", "The Origin of Species (Variorum Reprint)", "The Origin of Species (World's Classics)", "From So Simple a Beginning", "The Correspondence of Charles Darwin, Volume 13", "Evolution by natural selection", "The Correspondence of Charles Darwin, Volume 8", "Origins", "The Autobiography of Charles Darwin (Great Minds Series)", "The Power of Movement in Plants", "Gesammelte kleinere Schriften", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The structure and distribution of coral reefs", "A Darwin Selection", "The collected papers of Charles Darwin", "Kleinere geologische Abhandlungen", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Autobiography Of Charles Darwin", "The foundations of the Origin of species", "The Origin Of Species", "The Descent of Man and Selection in Relation to Sex", "Metaphysics, Materialism, & the evolution of mind", "The Voyage of the Beagle (Unabridged Classics)", "The Origin of Species (Oxford World's Classics)", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 2", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 8: 1860", "Les moyens d'expression chez les animaux", "Evolution and natural selection", "Notebooks on transmutation of species", "The structure and distribution of coral reefs.", "Voyage of the Beagle (NG Adventure Classics)", "The living thoughts of Darwin", "Darwin en Patagonia", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Origin of Species (Enriched Classics)", "red notebook of Charles Darwin", "On Natural Selection", "Origin of Species", "Voyage d'un naturaliste autour du monde", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Correspondence of Charles Darwin, Volume 15", "Reise um die Welt 1831 - 36", "To the members of the Down Friendly Club", "Darwin on humus and the earthworm", "The descent of man, and selection in relation to sex", "Les mouvements et les habitudes des plantes grimpantes", "The origin of species : complete and fully illustrated", "The Correspondence of Charles Darwin, Volume 3", "The Origin of Species (Collector's Library)", "Leben und Briefe von Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Beagle letters", "Darwinism stated by Darwin himself", "monograph on the sub-class Cirripedia", "Charles Darwin's marginalia", "La vie et la correspondance de Charles Darwin", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 11: 1863", "Origin of Species (Harvard Classics, Part 11)", "Les r\u00e9cifs de corail, leur structure et leur distribution", "genese\u014ds t\u014dn eid\u014dn", "The Origin of Species", "Voyage of the Beagle (Harvard Classics, Part 29)", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Expression Of The Emotions In Man And Animals", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The voyage of Charles Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Origin of Species (Mentor)", "The Correspondence of Charles Darwin, Volume 18: 1870", "The autobiography of Charles Darwin", "Darwin's notebooks on transmutation of species", "Darwin's Ornithological notes", "The origin of species", "The autobiography of Charles Darwin, 1809-1882", "Darwin's insects", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 5", "The Orgin of Species", "Darwin Darwin", "On evolution", "Darwin's journal", "The Autobiography of Charles Darwin, and selected letters", "The expression of the emotions in man and animals", "Works", "Proiskhozhdenie vidov", "The Darwin Reader First Edition", "Human nature, Darwin's view", "Darwin and Henslow", "The Expression of the Emotions in Man And Animals", "The Voyage of the Beagle", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 7", "Charles Darwin's letters", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 12", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Variation of Animals and Plants under Domestication", "Die geschlechtliche Zuchtwahl", "The Autobiography of Charles Darwin", "On the tendency of species to form varieties", "The Correspondence of Charles Darwin, Volume 10: 1862", "Cartas de Darwin 18251859", "Memorias y epistolario i\u0301ntimo", "Resa kring jorden", "The Structure And Distribution of Coral Reefs", "The Descent of Man, and Selection in Relation to Sex", "Notes on the fertilization of orchids", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 15: 1867", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Rejse om jorden", "The descent of man and selection in relation to sex.", "Opsht\u0323amung fun menshen", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Structure and Distribution of Coral Reefs", "The Voyage of the Beagle (Adventure Classics)", "Evolution", "The geology of the voyage of H.M.S. Beagle", "Reise eines Naturforschers um die Welt", "Die fundamente zur entstehung der arten", "On the Movements and Habits of Climbing Plants", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 6", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Diary of the voyage of H.M.S. Beagle", "The\u0301orie de l'e\u0301volution", "The Voyage of the Beagle (Great Minds Series)", "The Life of Erasmus Darwin", "Part I: Contributions to the Theory of Natural Selection / Part II", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Darwin", "Fertilisation of Orchids", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "El Origin De Las Especies", "Autobiography of Charles Darwin", "The Autobiography of Charles Darwin (Large Print)", "The Autobiography of Charles Darwin [EasyRead Edition]", "Questions about the breeding of animals", "The expression of the emotions in man and animals.", "Tesakneri tsagume\u030c", "Voyage of the Beagle (Dover Value Editions)", "The Expression of the Emotions in Man and Animals", "H.M.S. Beagle in South America", "The Formation of Vegetable Mould through the Action of Worms", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Charles Darwin's natural selection", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "More Letters of Charles Darwin", "Origin of Species (Everyman's University Paperbacks)", "The Voyage of the Beagle (Everyman Paperbacks)", "The Correspondence of Charles Darwin, Volume 11", "On a remarkable bar of sandstone off Pernambuco", "The Correspondence of Charles Darwin, Volume 14", "On the origin of species by means of natural selection", "Volcanic Islands", "The Autobiography of Charles Darwin (Dodo Press)", "Wu zhong qi yuan", "Charles Darwin on the routes of male humble bees", "The Correspondence of Charles Darwin, Volume 10", "The Darwin Reader Second Edition", "The action of carbonate of ammonia on the roots of certain plants", "The Origin of Species (Great Books : Learning Channel)", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Correspondence of Charles Darwin, Volume 13: 1865", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 4", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Essential Darwin", "The Origin of Species (Great Minds Series)", "The descent of man, and selection in relation to sex.", "The Voyage of the Beagle (Mentor)", "Voyage Of The Beagle", "From Darwin's unpublished notebooks", "Charles Darwin", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The portable Darwin", "Darwin-Wallace", "The voyage of the Beagle.", "Diario del Viaje de Un Naturalista Alrededor", "Voyage of the Beagle", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 9", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "vari\u00eberen der huisdieren en cultuurplanten", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 1"], "ans_acc": 0.02336448598130841, "ans_hit": 1, "ans_f1": 0.018390804597701146, "ans_precission": 0.5714285714285714, "ans_recall": 0.009345794392523364, "path_f1": 0.12903225806451613, "path_precision": 1.0, "path_recall": 0.06896551724137931, "path_ans_f1": 0.045662100456621, "path_ans_precision": 1.0, "path_ans_recall": 0.02336448598130841}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.institution -> University of Florida\n# Answer:\nm.05v4mnf", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.major_field_of_study -> Family, Youth and Community Sciences\n# Answer:\nm.05v4mnf", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.containedby -> Europe\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.passes -> Buz\u0103u Pass -> common.topic.notable_types -> Mountain pass\n# Answer:\nBuz\u0103u Pass", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> geography.mountain.mountain_range -> Beskids\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.passes -> Buz\u0103u Pass -> common.topic.article -> m.07jjrg\n# Answer:\nBuz\u0103u Pass", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.partially_contained_by -> m.0wg8s31\n# Answer:\nBabia G\u00f3ra"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> Hope-coventina01a\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_hints.included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_profile.published -> Published\n# Answer:\nArtwork", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.ethnicity -> Irish people in Great Britain\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> common.topic.webpage -> m.0b6rc9v\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Bard", "Poet", "Writer", "Author"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.3333333333333333, "ans_recall": 0.25, "path_f1": 0.28571428571428575, "path_precision": 0.3333333333333333, "path_recall": 0.25, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.5}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_universe.species -> Droid\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_universe.locations -> Alderaan\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.book.genre -> Science Fiction\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.spouse -> Ashley Thompson\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Jeremy Bieber\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05ckldy\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.locations -> Iraqi Kurdistan\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> common.topic.notable_for -> g.125cxty6s\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> base.locations.countries.continent -> Asia\n# Answer:\nSaudi Arabia"], "ground_truth": ["Argentina", "United Kingdom", "France", "Iraq", "United States of America", "Australia", "Saudi Arabia"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.8571428571428571, "ans_recall": 0.2857142857142857, "path_f1": 0.06611570247933884, "path_precision": 0.5714285714285714, "path_recall": 0.03508771929824561, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.character -> London Tipton\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> freebase.valuenotation.has_value -> Award Nominee\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol -> common.topic.article -> m.0j94h3g\n# Answer:\nA London Carol", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol -> common.topic.notable_types -> TV Episode\n# Answer:\nA London Carol", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol -> common.topic.notable_for -> g.125973965\n# Answer:\nA London Carol"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.image -> Hurricane Bob on July 10, 1979\n# Answer:\nHurricane Bob"], "ground_truth": ["Return J. Meigs, Jr.", "Ted Strickland", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nm.0_z851f", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.episodes_appeared_in -> Sport Relief 2014 Special\n# Answer:\nm.0_z851f"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.2, "path_recall": 0.14285714285714285, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.administrative_divisions -> Andalusia\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> base.aareas.schema.administrative_area.administrative_children -> Andalusia\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.second_level_divisions -> Asturias\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> common.topic.article -> m.0r4xz\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> g.11b674hjl7\n# Answer:\nCoronado"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> book.author.works_written -> A Nation of Immigrants -> common.topic.notable_types -> Book\n# Answer:\nA Nation of Immigrants", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> book.author.works_written -> A Nation of Immigrants -> book.book.genre -> Non-fiction\n# Answer:\nA Nation of Immigrants"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\ng.12cp_jvpx", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nm.0ghc35h"], "ground_truth": ["England", "Northern Ireland", "Scotland", "Wales"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.5, "ans_recall": 0.25, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.5, "path_ans_recall": 0.25}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Lake Maurepas\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8__h\n# Answer:\nAmite River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently. -> common.topic.notable_types -> Quotation\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.nationality -> United Kingdom\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently. -> media_common.quotation.subjects -> Truth\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> book.author.works_written -> A short history of ethics\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Philosopher", "Writer", "Physician"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.16666666666666666, "ans_recall": 0.3333333333333333, "path_f1": 0.2222222222222222, "path_precision": 0.16666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4444444444444444, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0 -> award.award_nomination.nominated_for -> Heaven Sent\n# Answer:\nm.05cqdz0", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.releases -> Definition of Real\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.tracks -> #1 Fan (feat. Keyshia Cole & J. Holiday)\n# Answer:\n#1 Fan (feat. Keyshia Cole & J. Holiday)", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0 -> award.award_nomination.ceremony -> 51st Annual Grammy Awards\n# Answer:\nm.05cqdz0"], "ground_truth": ["Francine Lons", "Leon Cole", "Sal Gibson"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.35294117647058826, "ans_precission": 0.375, "ans_recall": 0.3333333333333333, "path_f1": 0.35294117647058826, "path_precision": 0.375, "path_recall": 0.3333333333333333, "path_ans_f1": 0.35294117647058826, "path_ans_precision": 0.375, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.3333333333333333, "ans_recall": 0.5, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.5}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.011j_4sh"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Novelist\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.gender -> Female\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> freebase.valuenotation.has_value -> Education\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> common.topic.image -> Edgar Allan Poe signature\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41 -> common.webpage.resource -> m.0blf53m\n# Answer:\nm.03ldb41", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> common.topic.notable_types -> Airport\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41 -> common.webpage.category -> Official Website\n# Answer:\nm.03ldb41", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport"], "ground_truth": ["Six Flags Over Georgia", "The Tabernacle", "Arbor Place Mall", "Centennial Olympic Park", "Peachtree Road Race", "Georgia Aquarium", "Georgia World Congress Center", "Woodruff Arts Center", "Hyatt Regency Atlanta", "Atlanta History Center", "Zoo Atlanta", "Cobb Energy Performing Arts Centre", "Four Seasons Hotel Atlanta", "Martin Luther King, Jr. National Historic Site", "Six Flags White Water", "Atlanta Cyclorama & Civil War Museum", "Variety Playhouse", "CNN Center", "Atlanta Jewish Film Festival", "World of Coca-Cola", "Fernbank Museum of Natural History", "Atlanta Ballet", "Fox Theatre", "Philips Arena", "Underground Atlanta", "Center for Puppetry Arts", "Atlanta Symphony Orchestra", "Georgia Dome", "Turner Field", "Omni Coliseum", "Jimmy Carter Library and Museum", "Masquerade", "Atlanta Marriott Marquis", "Georgia State Capitol", "Fernbank Science Center", "Margaret Mitchell House & Museum"], "ans_acc": 0.027777777777777776, "ans_hit": 1, "ans_f1": 0.05063291139240506, "ans_precission": 0.2857142857142857, "ans_recall": 0.027777777777777776, "path_f1": 0.05063291139240506, "path_precision": 0.2857142857142857, "path_recall": 0.027777777777777776, "path_ans_f1": 0.05063291139240506, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.027777777777777776}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.education -> m.04hrrdm -> education.education.institution -> University of Queensland\n# Answer:\nm.04hrrdm", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.education -> m.04hrrdm -> education.education.major_field_of_study -> Social science\n# Answer:\nm.04hrrdm"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> media_common.literary_genre.books_in_this_genre -> Amis and Amiloun\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> book.book_subject.works -> Go with Me\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> tv.multipart_tv_episode.episodes -> Fri 31 December, 2010 [Episode 1]\n# Answer:\nFri 31 Dec 2010"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> common.topic.article -> m.0dg0pnd\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> freebase.type_hints.included_types -> Person\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> base.descriptive_names.names.descriptive_name -> m.0104hg70\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.resource -> m.0blsygc\n# Answer:\nm.04lt3gf"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.gender -> Male\n# Answer:\nCecil Newton, Sr."], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.14285714285714285, "path_recall": 0.3333333333333333, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer -> common.topic.notable_types -> Author\n# Answer:\nAlfred G. Mayer", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer -> common.topic.notable_for -> g.125byfgkm\n# Answer:\nAlfred G. Mayer", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer -> common.topic.article -> m.0fq0_8r\n# Answer:\nAlfred G. Mayer"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_types -> Book\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_holder -> Spencer Cox\n# Answer:\nm.010flwmg"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> common.topic.image -> BUSHLAURA\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> common.topic.notable_for -> g.1257w3www\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nm.010l29pk"], "ground_truth": ["Ralph Nader", "John Kerry", "Michael Peroutka", "Gene Amondson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7 -> film.personal_film_appearance.film -> The Ascent of Money\n# Answer:\nm.0h4nmc7"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> base.aareas.schema.administrative_area.administrative_parent -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> common.topic.notable_types -> Island\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Baltra Island -> common.topic.notable_types -> Island\n# Answer:\nBaltra Island"], "ground_truth": ["Pacific Ocean", "Ecuador", "Gal\u00e1pagos Province"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 0.42857142857142855, "ans_recall": 0.3333333333333333, "path_f1": 0.375, "path_precision": 0.42857142857142855, "path_recall": 0.3333333333333333, "path_ans_f1": 0.689655172413793, "path_ans_precision": 0.7142857142857143, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx"], "ground_truth": ["Baby", "Boyfriend", "Change Me", "Home to Mama", "Lolly", "Hold Tight", "All Bad", "Heartbreaker", "#thatPower", "Never Say Never", "Wait for a Minute", "Somebody to Love", "Pray", "Never Let You Go", "Live My Life", "Die in Your Arms", "All That Matters", "Recovery", "Turn to You (Mother's Day Dedication)", "Right Here", "Thought Of You", "As Long as You Love Me", "Beauty And A Beat", "All Around The World", "Roller Coaster", "PYD", "Confident", "Bad Day", "First Dance", "Bigger", "Eenie Meenie"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.0588235294117647, "ans_precission": 0.3333333333333333, "ans_recall": 0.03225806451612903, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.1142857142857143, "path_ans_precision": 0.5, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> base.descriptive_names.names.descriptive_name -> m.0105cq_d\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> common.topic.notable_types -> Transit Stop\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> location.location.containedby -> Paris\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau"], "ground_truth": ["Statesman", "Writer", "Physician", "Publisher", "Journalist"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.26086956521739135, "ans_precission": 0.375, "ans_recall": 0.2, "path_f1": 0.26086956521739135, "path_precision": 0.375, "path_recall": 0.2, "path_ans_f1": 0.26086956521739135, "path_ans_precision": 0.375, "path_ans_recall": 0.2}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1gvc"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.perpetrator -> Jay-Z\n# Answer:\nm.063y0bl"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0cs3c3w", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.category -> Official Website\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w -> film.personal_film_appearance.film -> Brown Is the New Green: George Lopez and the American Dream\n# Answer:\nm.0cs3c3w"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> 700-160\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> common.topic.article -> m.0h6dr\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_khq\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_for -> g.1q6hmhsk5\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.article -> m.0hr6vbt\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> common.topic.notable_for -> g.125cc3_fl\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.image -> Allah-eser2\n# Answer:\nAllah"], "ground_truth": ["Islamic view of angels", "Tawhid", "Predestination in Islam", "Sharia", "Entering Heaven alive", "God in Islam", "Prophets in Islam", "Qiyamah", "Monotheism", "Islamic holy books", "Masih ad-Dajjal", "\u1e6c\u016bb\u0101", "Mahdi"], "ans_acc": 0.07692307692307693, "ans_hit": 1, "ans_f1": 0.11764705882352941, "ans_precission": 0.25, "ans_recall": 0.07692307692307693, "path_f1": 0.11764705882352941, "path_precision": 0.25, "path_recall": 0.07692307692307693, "path_ans_f1": 0.11764705882352941, "path_ans_precision": 0.25, "path_ans_recall": 0.07692307692307693}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.prevention_factors -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> common.topic.notable_types -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nGeorge Orwell -> common.topic.notable_types -> Author -> common.topic.subject_of -> Kiersten Fay\n# Answer:\nAuthor", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_for -> g.12599cgh4\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nA dirty joke is a sort of mental rebellion."], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.periodical_publisher.periodicals_published -> m.0106rcrc\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352942, "path_precision": 0.16666666666666666, "path_recall": 0.09090909090909091, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\ng.11b7_lvdf2", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.resource -> m.0bl181r\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9"], "ground_truth": ["Songwriter", "Singer", "Actor"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.25, "ans_recall": 0.3333333333333333, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.25, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.locations.states_and_provences.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66101 -> location.location.containedby -> Wyandotte County\n# Answer:\n66101"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbp_k", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.season -> 1955 Major League Baseball season\n# Answer:\nm.06sbp_k", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> freebase.valuenotation.is_reviewed -> Player\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Abraham Cohn\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.article -> m.03vbmy\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.position -> First baseman\n# Answer:\nm.0gggrzr"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.nationality -> United States of America\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> freebase.type_hints.included_types -> Topic\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> rdf-schema#range -> Theatrical Composer\n# Answer:\nComposer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.07919ln", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\nm.07919ln"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_for -> g.125b58fny\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.person.spouse_s -> m.03jnwjs\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.person.sibling_s -> m.03jnwm_\n# Answer:\nAnna Roosevelt Halsted"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1"], "ground_truth": ["Protestantism", "Islam", "Catholicism", "Hinduism"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.3333333333333333, "path_recall": 0.2, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.25}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.containedby -> Buchanan County\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.citytown.postal_codes -> 64501\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nUnited States of America"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> common.topic.notable_types -> US President\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.dates -> m.0c0z7kp\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> people.person.nationality -> United Kingdom\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.previous_sentence -> \\\"Pictures from Italy\\\"\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nAldous Huxley"], "ground_truth": ["A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Acting Edition)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Cyber Classics)", "Great expectations", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Classics Illustrated Notes)", "Great expectations.", "Dombey and son", "A Christmas Carol (Illustrated Classics)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Whole Story)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (Adopted Classic)", "Great Expectations.", "A Christmas Carol (Watermill Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Collector's Library)", "A Tale of Two Cities (Compact English Classics)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Everyman's Library (Paper))", "The Pickwick Papers", "A Christmas Carol (Usborne Young Reading)", "The cricket on the hearth", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Penguin Readers, Level 2)", "Hard times", "Little Dorrit", "A Tale of Two Cities (Longman Fiction)", "The life and adventures of Nicholas Nickleby", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Dramascripts S.)", "A Christmas Carol (Apple Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Bantam Classic)", "Sketches by Boz", "A Christmas Carol", "A Christmas Carol (Classic, Picture, Ladybird)", "David Copperfield", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "Bleak House", "A Tale of Two Cities (Naxos AudioBooks)", "The Old Curiosity Shop", "Our mutual friend", "A Tale of Two Cities (The Greatest Historical Novels)", "The Mystery of Edwin Drood", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Gollancz Children's Classics)", "Great Expectations", "Dombey and Son.", "A Christmas Carol (Take Part)", "A Christmas Carol (Penguin Student Editions)", "Bleak House.", "A Christmas Carol (Reissue)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Enriched Classic)", "Martin Chuzzlewit", "A Christmas Carol (Nelson Graded Readers)", "Bleak house", "David Copperfield.", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (Ladybird Classics)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Illustrated Junior Library)", "The Pickwick papers", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Children's Classics)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Acting Edition)", "The mystery of Edwin Drood", "A Tale of Two Cities (Clear Print)", "Oliver Twist", "A TALE OF TWO CITIES", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Masterworks)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "Dombey and Son", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Dramascripts)", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Large Print)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (BBC Audio Series)", "A CHRISTMAS CAROL", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "The old curiosity shop", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Courage Literary Classics)", "The old curiosity shop.", "Our mutual friend.", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (R)", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (The Kennett Library)"], "ans_acc": 0.011834319526627219, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07894736842105263, "path_precision": 0.14285714285714285, "path_recall": 0.05454545454545454, "path_ans_f1": 0.02185792349726776, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 0.011834319526627219}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> 58th Army\n# Answer:\n1940\u201344 insurgency in Chechnya", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> military.military_conflict.combatants -> m.064ykvf\n# Answer:\n1940\u201344 insurgency in Chechnya", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> common.topic.notable_types -> Military Conflict\n# Answer:\n1940\u201344 insurgency in Chechnya"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\ng.11b6ddwl64", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Board members\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> common.topic.subject_of -> NYSE\n# Answer:\nDDR Corp."], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.risk_factor.diseases -> Ptosis\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Head pressing\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> common.topic.notable_types -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer", "# Reasoning Path:\nCarl Wilson -> common.topic.notable_types -> Composer -> base.descriptive_names.names.descriptive_name -> m.0105bzh0\n# Answer:\nComposer", "# Reasoning Path:\nCarl Wilson -> common.topic.notable_types -> Composer -> common.topic.webpage -> m.09w_34f\n# Answer:\nComposer"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Behind the Scenes\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> common.topic.notable_for -> g.125fby47j\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.resource -> NBC Sked Scoop: 'Medium' returns, 'ER' ends, 'Knight Rider' trimmed, and more!\n# Answer:\nm.09w0_mw"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> common.topic.notable_types -> City/Town/Village -> base.descriptive_names.names.descriptive_name -> m.0105y_05\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nBrentwood -> common.topic.notable_types -> City/Town/Village -> common.topic.article -> m.03bnd2c\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nBrentwood -> common.topic.notable_types -> City/Town/Village -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCity/Town/Village"], "ground_truth": ["Williamson County"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0102h0md\n# Answer:\nBotanist", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist -> common.topic.notable_for -> g.12568993_\n# Answer:\nBotanist", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist -> common.topic.notable_types -> Profession\n# Answer:\nBotanist"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nm.07nvttg", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nm.07nvttg", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.ceremony -> 52nd Primetime Emmy Awards\n# Answer:\nm.07nvttg"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23529411764705882, "path_precision": 0.2222222222222222, "path_recall": 0.25, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War"], "ground_truth": ["First Battle of Kernstown", "Battle of Front Royal", "Battle of McDowell", "Battle of Cedar Mountain", "How Few Remain", "Romney Expedition", "Battle of Port Republic", "Jackson's Valley Campaign", "Battle of Harpers Ferry", "First Battle of Winchester", "Battle of White Oak Swamp", "Battle of Chancellorsville", "Battle of Hoke's Run", "American Civil War", "Battle of Chantilly", "Battle of Hancock", "First Battle of Rappahannock Station", "Manassas Station Operations", "Second Battle of Bull Run"], "ans_acc": 0.10526315789473684, "ans_hit": 1, "ans_f1": 0.09677419354838708, "ans_precission": 0.6, "ans_recall": 0.05263157894736842, "path_f1": 0.13559322033898305, "path_precision": 0.8, "path_recall": 0.07407407407407407, "path_ans_f1": 0.18604651162790697, "path_ans_precision": 0.8, "path_ans_recall": 0.10526315789473684}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Geographic distribution\n# Answer:\nEthnicity", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> freebase.type_profile.strict_included_types -> Abstract\n# Answer:\nEthnicity"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> type.type.domain -> Film\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> base.descriptive_names.names.descriptive_name -> m.010h52df\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> type.type.expected_by -> Actor\n# Answer:\nFilm actor"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Lorenzo di Credi -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nLorenzo di Credi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_form -> Mural\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Lorenzo di Credi -> visual_art.visual_artist.artworks -> Adoration of the Shepherds\n# Answer:\nLorenzo di Credi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Lorenzo di Credi -> people.person.places_lived -> m.03pg9zy\n# Answer:\nLorenzo di Credi"], "ground_truth": ["Medusa", "Portrait of a man in red chalk", "Benois Madonna", "La belle ferronni\u00e8re", "g.120vt1gz", "The Virgin and Child with St Anne and St John the Baptist", "Drapery for a Seated Figure", "Madonna of the Yarnwinder", "Madonna of Laroque", "Horse and Rider", "Madonna of the Carnation", "Head of a Woman", "g.1239jd9p", "St. John the Baptist", "Sala delle Asse", "The Holy Infants Embracing", "The Virgin and Child with St. Anne", "St. Jerome in the Wilderness", "Madonna and Child with St Joseph", "g.121wt37c", "g.12314dm1", "Vitruvian Man", "Leonardo's horse", "Virgin of the Rocks", "The Battle of Anghiari", "Mona Lisa", "g.12215rxg", "g.1224tf0c", "Lucan portrait of Leonardo da Vinci", "Ginevra de' Benci", "g.1213jb_b", "Annunciation", "Salvator Mundi", "Adoration of the Magi", "The Baptism of Christ", "Madonna Litta", "Portrait of a Musician", "The Last Supper", "g.121yh91r", "Leda and the Swan", "Portrait of a Young Fianc\u00e9e", "Bacchus", "g.1219sb0g", "Lady with an Ermine", "Portrait of Isabella d'Este"], "ans_acc": 0.022222222222222223, "ans_hit": 1, "ans_f1": 0.04123711340206186, "ans_precission": 0.2857142857142857, "ans_recall": 0.022222222222222223, "path_f1": 0.042105263157894736, "path_precision": 0.2857142857142857, "path_recall": 0.022727272727272728, "path_ans_f1": 0.04123711340206186, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.022222222222222223}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nEva Per\u00f3n -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Discursos completos -> common.topic.notable_for -> g.1jmcc8fyn\n# Answer:\nDiscursos completos", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Discursos completos -> common.topic.notable_types -> Book\n# Answer:\nDiscursos completos"], "ground_truth": ["Cervical cancer"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> type.type.properties -> Religious Leadership\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> freebase.type_hints.included_types -> Person\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> base.descriptive_names.names.descriptive_name -> m.011mcpf3\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBuddha Memorial Center"], "ground_truth": ["Nepal"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Bifocals", "Lightning rod", "Glass harmonica", "Franklin stove"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.3333333333333333, "ans_recall": 0.25, "path_f1": 0.28571428571428575, "path_precision": 0.3333333333333333, "path_recall": 0.25, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.25}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> location.location.containedby -> Colorado\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> common.topic.notable_types -> City/Town/Village -> base.descriptive_names.names.descriptive_name -> m.0105y_05\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nGreeley -> common.topic.notable_types -> City/Town/Village -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> education.educational_institution.faculty -> m.0k7sk9c\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> common.topic.notable_types -> City/Town/Village -> type.type.domain -> Location\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> education.university.domestic_tuition -> m.0k7gb23\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_for -> g.12599cm43\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_types -> Book\n# Answer:\nAndante cantabile from quartet in D major, op. 11"], "ground_truth": ["Composer", "Musician", "Librettist"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.75, "ans_recall": 0.3333333333333333, "path_f1": 0.35294117647058826, "path_precision": 0.375, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.75, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.location.partially_contains -> Alps\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nm.0102xvg7", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.degree -> PhD\n# Answer:\nm.0102xvg7", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.rosetta_document.document_class -> Universal Declaration of Human Rights\n# Answer:\nGerman, Standard"], "ground_truth": ["East Germany", "Luxembourg", "Austria", "Germany", "Switzerland", "Belgium", "Liechtenstein"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.21428571428571427, "ans_precission": 0.42857142857142855, "ans_recall": 0.14285714285714285, "path_f1": 0.21428571428571427, "path_precision": 0.42857142857142855, "path_recall": 0.14285714285714285, "path_ans_f1": 0.2285714285714286, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.starring -> m.0w0m5l0\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> 50's\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.directed_by -> Yoko Ono\n# Answer:\nApotheosis"], "ground_truth": ["Blues rock", "Experimental rock", "Rock music", "Soft rock", "Art rock", "Pop music", "Experimental music", "Psychedelic rock", "Pop rock"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.16, "ans_precission": 0.2857142857142857, "ans_recall": 0.1111111111111111, "path_f1": 0.16, "path_precision": 0.2857142857142857, "path_recall": 0.1111111111111111, "path_ans_f1": 0.16, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.winner -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.general_election -> Colorado state elections, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> military.military_unit.unit_size -> Regiment\n# Answer:\n1st Colorado Cavalry Regiment"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.first_level_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk", "# Reasoning Path:\nGreenland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc3bn1j\n# Answer:\ng.1hhc3bn1j", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.location.containedby -> King County\n# Answer:\nAlki Point"], "ground_truth": ["98190", "98124", "98108", "98191", "98105", "98170", "98171", "98106", "98154", "98177", "98199", "98178", "98165", "98103", "98121", "98185", "98115", "98145", "98198", "98144", "98188", "98134", "98164", "98104", "98168", "98184", "98166", "98129", "98126", "98136", "98113", "98148", "98127", "98131", "98161", "98119-4114", "98122", "98194", "98119", "98181", "98138", "98132", "98118", "98114", "98109", "98102", "98139", "98146", "98195", "98111", "98101", "98117", "98125", "98175", "98155", "98133", "98112", "98160", "98107", "98116", "98141", "98158", "98174"], "ans_acc": 0.015873015873015872, "ans_hit": 1, "ans_f1": 0.03061224489795918, "ans_precission": 0.42857142857142855, "ans_recall": 0.015873015873015872, "path_f1": 0.03061224489795918, "path_precision": 0.42857142857142855, "path_recall": 0.015873015873015872, "path_ans_f1": 0.03061224489795918, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.015873015873015872}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> common.topic.notable_types -> Musical Artist -> type.type.expected_by -> Artist\n# Answer:\nMusical Artist", "# Reasoning Path:\nWillow Smith -> common.topic.notable_types -> Musical Artist -> base.descriptive_names.names.descriptive_name -> m.010h53pp\n# Answer:\nMusical Artist", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> common.topic.notable_types -> Musical Artist -> type.type.properties -> Active as Musical Artist (end)\n# Answer:\nMusical Artist", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> common.topic.notable_types -> Musical genre\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> #Musik.Main on RauteMusik.FM\n# Answer:\n#Musik.Main on RauteMusik.FM"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.degree -> PhD\n# Answer:\nm.0104b7h1", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.institution -> University of Copenhagen\n# Answer:\nm.0104b7h1", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0104b7h1"], "ground_truth": ["Simplified Chinese character", "Chinese characters", "Traditional Chinese characters", "N\u00fcshu script", "'Phags-pa script"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.3333333333333333, "ans_recall": 0.2, "path_f1": 0.25, "path_precision": 0.3333333333333333, "path_recall": 0.2, "path_ans_f1": 0.25, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.2}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.125, "path_recall": 0.14285714285714285, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_types -> TV Season\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.07shryn -> award.award_honor.award -> Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series\n# Answer:\nm.07shryn", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.07shryn -> award.award_honor.award_winner -> Isabel Sanford\n# Answer:\nm.07shryn"], "ground_truth": ["Damon Evans", "Marla Gibbs", "Zara Cully", "Roxie Roker", "Mike Evans", "Paul Benedict", "Franklin Cover", "Isabel Sanford", "Sherman Hemsley", "Berlinda Tolbert", "Jay Hammer"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.030769230769230767, "path_precision": 0.16666666666666666, "path_recall": 0.01694915254237288, "path_ans_f1": 0.11764705882352942, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> book.periodical.frequency_or_issues_per_year -> m.09s58j6\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.notable_for -> g.125d1300_\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.webpage -> m.03kz_rr\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.original_language -> English Language\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> common.topic.notable_for -> g.125d1w2fz\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> common.topic.notable_types -> Postal Code\n# Answer:\n94101"], "ground_truth": ["California Star", "San Francisco Call", "Synapse", "San Francisco Business Times", "San Francisco Bay Guardian", "San Francisco Foghorn", "San Francisco Daily", "Free Society", "The San Francisco Examiner", "The Golden Era", "AsianWeek", "Bay Area Reporter", "Dock of the Bay", "San Francisco Bay Times", "The Daily Alta California", "San Francisco Chronicle", "Street Sheet", "San Francisco News-Call Bulletin Newspaper", "San Francisco Bay View", "Sing Tao Daily"], "ans_acc": 0.05, "ans_hit": 1, "ans_f1": 0.08695652173913045, "ans_precission": 0.3333333333333333, "ans_recall": 0.05, "path_f1": 0.08695652173913045, "path_precision": 0.3333333333333333, "path_recall": 0.05, "path_ans_f1": 0.08695652173913045, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.05}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> organization.organization_scope.organizations_with_this_scope -> Commonwealth of Independent States\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Be a Man -> music.album.genre -> Hip hop music\n# Answer:\nBe a Man"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Biology\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> common.topic.notable_for -> g.1254zbncs\n# Answer:\nDarwin"], "ground_truth": ["The principal works", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Life and Letters of Charles Darwin Volume 1", "The education of Darwin", "Motsa ha-minim", "Darwin for Today", "ontstaan der soorten door natuurlijke teeltkeus", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Evolution by natural selection", "Origins", "The Power of Movement in Plants", "Gesammelte kleinere Schriften", "Kleinere geologische Abhandlungen", "A Darwin Selection", "The collected papers of Charles Darwin", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The foundations of the Origin of species", "Metaphysics, Materialism, & the evolution of mind", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 8: 1860", "Les moyens d'expression chez les animaux", "Evolution and natural selection", "Notebooks on transmutation of species", "The living thoughts of Darwin", "Darwin en Patagonia", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "red notebook of Charles Darwin", "On Natural Selection", "Voyage d'un naturaliste autour du monde", "Reise um die Welt 1831 - 36", "To the members of the Down Friendly Club", "Darwin on humus and the earthworm", "Les mouvements et les habitudes des plantes grimpantes", "Leben und Briefe von Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Beagle letters", "Darwinism stated by Darwin himself", "monograph on the sub-class Cirripedia", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Charles Darwin's marginalia", "La vie et la correspondance de Charles Darwin", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 11: 1863", "Les r\u00e9cifs de corail, leur structure et leur distribution", "genese\u014ds t\u014dn eid\u014dn", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The voyage of Charles Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Correspondence of Charles Darwin, Volume 18: 1870", "Darwin's notebooks on transmutation of species", "Darwin's Ornithological notes", "Insectivorous Plants", "The Orgin of Species", "Darwin's insects", "Darwin Darwin", "On evolution", "Darwin's journal", "Works", "Proiskhozhdenie vidov", "The Darwin Reader First Edition", "Human nature, Darwin's view", "Darwin and Henslow", "The Voyage of the Beagle", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Charles Darwin's letters", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Different Forms of Flowers on Plants of the Same Species", "Die geschlechtliche Zuchtwahl", "The Variation of Animals and Plants under Domestication", "The Autobiography of Charles Darwin", "On the tendency of species to form varieties", "The Correspondence of Charles Darwin, Volume 10: 1862", "Cartas de Darwin 18251859", "Memorias y epistolario i\u0301ntimo", "The Life and Letters of Charles Darwin Volume 2", "Resa kring jorden", "The Descent of Man, and Selection in Relation to Sex", "Notes on the fertilization of orchids", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 15: 1867", "Rejse om jorden", "Opsht\u0323amung fun menshen", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The Structure and Distribution of Coral Reefs", "Evolution", "The geology of the voyage of H.M.S. Beagle", "Reise eines Naturforschers um die Welt", "Darwin from Insectivorous Plants to Worms", "On the Movements and Habits of Climbing Plants", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Diary of the voyage of H.M.S. Beagle", "The\u0301orie de l'e\u0301volution", "The Life of Erasmus Darwin", "Part I: Contributions to the Theory of Natural Selection / Part II", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Darwin", "Fertilisation of Orchids", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "El Origin De Las Especies", "Questions about the breeding of animals", "Geological Observations on the Volcanic Islands", "Tesakneri tsagume\u030c", "The Expression of the Emotions in Man and Animals", "H.M.S. Beagle in South America", "The Formation of Vegetable Mould through the Action of Worms", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Charles Darwin's natural selection", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "More Letters of Charles Darwin", "On a remarkable bar of sandstone off Pernambuco", "Volcanic Islands", "On the origin of species by means of natural selection", "Wu zhong qi yuan", "Charles Darwin on the routes of male humble bees", "The Darwin Reader Second Edition", "The action of carbonate of ammonia on the roots of certain plants", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Correspondence of Charles Darwin, Volume 9: 1861", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Del Plata a Tierra del Fuego", "Evolutionary Writings: Including the Autobiographies", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Essential Darwin", "From Darwin's unpublished notebooks", "Charles Darwin", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The portable Darwin", "Darwin-Wallace", "Diario del Viaje de Un Naturalista Alrededor", "South American Geology", "Darwin Compendium", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "vari\u00eberen der huisdieren en cultuurplanten", "From so simple a beginning", "Die fundamente zur entstehung der arten"], "ans_acc": 0.0392156862745098, "ans_hit": 1, "ans_f1": 0.025559105431309907, "ans_precission": 0.5714285714285714, "ans_recall": 0.013071895424836602, "path_f1": 0.19999999999999998, "path_precision": 1.0, "path_recall": 0.1111111111111111, "path_ans_f1": 0.07547169811320754, "path_ans_precision": 1.0, "path_ans_recall": 0.0392156862745098}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.person.place_of_birth -> Yorba Linda -> location.location.containedby -> California\n# Answer:\nYorba Linda", "# Reasoning Path:\nRichard Nixon -> people.person.place_of_birth -> Yorba Linda -> travel.travel_destination.tourist_attractions -> Richard Nixon Library & Birthplace\n# Answer:\nYorba Linda", "# Reasoning Path:\nRichard Nixon -> people.person.place_of_birth -> Yorba Linda -> location.location.people_born_here -> Donald Nixon\n# Answer:\nYorba Linda", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.notable_types -> Musical Artist -> freebase.type_hints.included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nSmokey Robinson -> common.topic.notable_types -> Musical Artist -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nSmokey Robinson -> common.topic.notable_types -> Musical Artist -> freebase.type_profile.equivalent_topic -> Musician\n# Answer:\nMusical Artist", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor"], "ground_truth": ["Be Kind To The Growing Mind (with The Temptations)", "Little Girl, Little Girl", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Why Do Happy Memories Hurt So Bad", "Love Is The Light", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Nearness of You", "More Than You Know", "And I Don't Love You (Larry Levan instrumental dub)", "I Can\u2019t Stand to See You Cry (Commercial version)", "Be Kind to the Growing Mind", "You Really Got a Hold on Me", "Unless You Do It Again", "If You Wanna Make Love", "Just Another Kiss", "And I Don't Love You", "Just Like You", "When Smokey Sings Tears Of A Clown", "You Take Me Away", "Let Your Light Shine On Me", "Just To See Her Again", "You Cannot Laugh Alone", "Jesus Told Me To Love You", "Away in the Manger / Coventry Carol", "Aqui Con Tigo (Being With You)", "I Am I Am", "I've Made Love to You a Thousand Times", "If You Can Want", "I'll Keep My Light In My Window", "And I Love Her", "The Tracks of My Heart", "Heavy On Pride (Light On Love)", "Being With You", "So Bad", "Going to a Go Go", "Love Bath", "Don't Wanna Be Just Physical", "Satisfy You", "No Time to Stop Believing", "It's Fantastic", "Photograph in My Mind", "Blame It on Love", "Ooo Baby Baby", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Can't Fight Love", "Asleep on My Love", "Baby Come Close", "You Are Forever", "Te Quiero Como Si No Hubiera Un Manana", "Love Letters", "The Hurt's On You", "I Second That Emotion", "Bad Girl", "In My Corner", "Christmas Greeting", "My Guy", "We've Saved the Best for Last", "Going to a Gogo", "Happy (Love Theme From Lady Sings the Blues)", "He Can Fix Anything", "It's A Good Night", "Walk on By", "The Tears Of A Clown", "Quiet Storm (Groove Boutique remix)", "When A Woman Cries", "Love Don' Give No Reason (12 Inch Club Mix)", "With Your Love Came", "The Tracks of My Tears (live)", "Rack Me Back", "Let Me Be the Clock", "Close Encounters of the First Kind", "There Will Come A Day ( I'm Gonna Happen To You )", "Cruisin'", "Train of Thought", "Will You Still Love Me Tomorrow", "Let Me Be The Clock", "Quiet Storm", "Everything You Touch", "Deck the Halls", "Standing On Jesus", "My Girl", "Wishful Thinking", "The Road to Damascus", "You Don't Know What It's Like", "(It's The) Same Old Love", "Theme From the Big Time", "Love Don't Give No Reason", "Some People Will Do Anything for Love", "Tears of a Clown", "Please Don't Take Your Love (feat. Carlos Santana)", "Did You Know (Berry's Theme)", "I\u2019ve Got You Under My Skin", "Why", "Whatcha Gonna Do", "I Love The Nearness Of You", "A Silent Partner in a Three-Way Love Affair", "Sweet Harmony", "The Family Song", "I'm Glad There Is You", "Going to a Go-Go", "Holly", "Tell Me Tomorrow, Part 1", "Daylight & Darkness", "Fly Me to the Moon (In Other Words)", "Christmas Everyday", "I Like Your Face", "The Agony And The Ecstasy", "Rewind", "Vitamin U", "The Love Between Me and My Kids", "Be Who You Are", "I Second That Emotions", "The Christmas Song", "What's Too Much", "Never My Love / Never Can Say Goodbye", "Will You Love Me Tomorrow?", "It's Time to Stop Shoppin' Around", "Crusin", "Coincidentally", "Virgin Man", "I Care About Detroit", "Wedding Song", "God Rest Ye Merry Gentlemen", "You Are So Beautiful (feat. Dave Koz)", "Ever Had A Dream", "Mickey's Monkey", "Everything for Christmas", "Easy", "Just to See Her", "It's Her Turn to Live", "Santa Claus is Coming to Town", "That Place", "Same Old Love", "Driving Thru Life in the Fast Lane", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "The Way You Do (The Things You Do)", "Pops, We Love You (disco)", "Blame It On Love (Duet with Barbara Mitchell)", "Fulfill Your Need", "Tea for Two", "Baby That's Backatcha", "Hanging on by a Thread", "I Know You by Heart", "It's Christmas Time", "The Tracks of My Tears", "No\u00ebl", "A Child Is Waiting", "I Hear The Children Singing", "Open", "Jingle Bells", "Love So Fine", "Double Good Everything", "Tears Of A Clown", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Keep Me", "One Heartbeat", "The Tears of a Clown", "Tracks of My Tears", "Don't Know Why", "Will You Love Me Tomorrow", "You Made Me Feel Love", "Pops, We Love You", "Cruisin", "Tell Me Tomorrow", "The Tracks Of My Tears", "Little Girl Little Girl", "I Want You Back", "Share It", "Shoe Soul", "Why Are You Running From My Love", "If You Want My Love", "Tracks of my Tears", "Girlfriend", "Just a Touch Away", "Food For Thought", "Just My Soul Responding", "Melody Man", "Speak Low", "Wanna Know My Mind", "It's a Good Feeling", "Yester Love", "You Go to My Head", "Get Ready", "Ebony Eyes (Duet with Rick James)", "I Love Your Face", "Because of You It's the Best It's Ever Been", "More Love", "Just Passing Through", "Noel", "I'm in the Mood for Love", "Don't Play Another Love Song", "I Have Prayed On It", "If You Wanna Make Love (Come 'round Here)", "I Can't Get Enough", "Take Me Through The Night", "Tracks Of My Tears (Live)", "As You Do", "Our Love Is Here to Stay", "I Can't Find", "Season's Greetings from Smokey Robinson", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Tell Me Tomorrow (12\\\" extended mix)", "Who's Sad", "Come to Me Soon", "Shop Around", "Ooo Baby Baby (live)", "Christmas Every Day", "We Are The Warriors", "You've Really Got a Hold on Me", "Really Gonna Miss You", "I Can't Give You Anything but Love", "She's Only a Baby Herself", "I Am, I Am", "Fallin'", "I Praise & Worship You Father", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Quiet Storm (single version)", "You're the One for Me (feat. Joss Stone)", "Girl I'm Standing There", "The Track of My Tears", "Night and Day", "Winter Wonderland", "Gone Forever", "Yes It's You Lady", "Come by Here (Kum Ba Ya)", "Be Careful What You Wish For", "Skid Row", "Crusin'", "One Time", "Love Brought Us Here", "Ooh Baby Baby", "Tears of a Sweet Free Clown", "Time After Time", "My World", "We\u2019ve Come Too Far to End It Now", "There Will Come a Day (I'm Gonna Happen to You)", "Gang Bangin'", "You're Just My Life (feat. India.Arie)", "Mother's Son", "Time Flies", "Please Come Home for Christmas", "Save Me", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Jasmin", "Sleepless Nights", "I've Made Love To You A Thousand Times", "I've Got You Under My Skin", "A Tattoo", "Ain't That Peculiar", "Be Careful What You Wish For (instrumental)", "Hold on to Your Love", "You've Really Go a Hold on Me", "The Agony and the Ecstasy", "Ebony Eyes", "Medley: Never My Love / Never Can Say Goodbye", "Love' n Life"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nIrish American"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.article -> m.0l8p56r\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.sports -> Basketball\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.02wj9fq -> location.adjoining_relationship.adjoins -> Romania\n# Answer:\nm.02wj9fq"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}

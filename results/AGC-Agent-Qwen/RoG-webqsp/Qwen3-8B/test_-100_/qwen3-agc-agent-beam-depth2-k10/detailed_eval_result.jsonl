{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica\n# Answer:\nJamaica", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk\n# Answer:\nJames K. Polk", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kc", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> At 28, Polk was elected to the North Carolina State House of Representatives, where he served two years. -> base.kwebbase.kwsentence.next_sentence -> He opposed the old-style politicians who supported land speculators and bankers, and was a follower of Andrew Jackson, a popular military hero.\n# Answer:\nAt 28, Polk was elected to the North Carolina State House of Representatives, where he served two years.", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nm.04j60kc", "# Reasoning Path:\nJames K. Polk -> people.person.places_lived -> m.03phtbg -> people.place_lived.location -> North Carolina\n# Answer:\nm.03phtbg", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04469y8"], "ground_truth": ["Governor of Tennessee", "United States Representative", "Speaker of the United States House of Representatives"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.2222222222222222, "path_recall": 0.3333333333333333, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti\n# Answer:\nHaiti", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon\n# Answer:\nBarbara Gordon"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal\n# Answer:\nShaquille O'Neal", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0j2dtly -> sports.sports_team_roster.team -> Boston Celtics\n# Answer:\nm.0j2dtly", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nm.04fw72b", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72v -> sports.sports_team_roster.team -> Miami Heat\n# Answer:\nm.04fw72v", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0j2dtly -> sports.sports_team_roster.position -> Center\n# Answer:\nm.0j2dtly", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.position -> Center\n# Answer:\nm.04fw72b", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72v -> sports.sports_team_roster.position -> Center\n# Answer:\nm.04fw72v"], "ground_truth": ["Orlando Magic", "Cleveland Cavaliers", "Los Angeles Lakers", "LSU Tigers men's basketball", "Miami Heat", "Phoenix Suns", "Boston Celtics"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16216216216216217, "path_precision": 0.42857142857142855, "path_recall": 0.1, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno\n# Answer:\nJay Leno", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand\n# Answer:\nThailand", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language"], "ground_truth": ["Mon Language", "Akha Language", "Saek language", "Phu Thai language", "Malay, Pattani Language", "Hmong language", "Khmer language", "Vietnamese Language", "Lao Language", "Nyaw Language", "Cham language", "Thai Language", "Mlabri Language"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.25806451612903225, "ans_precission": 0.8, "ans_recall": 0.15384615384615385, "path_f1": 0.2448979591836735, "path_precision": 0.6, "path_recall": 0.15384615384615385, "path_ans_f1": 0.25806451612903225, "path_ans_precision": 0.8, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-106", "prediction": [], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson\n# Answer:\nThomas Jefferson"], "ground_truth": ["Lawyer", "Inventor", "Philosopher", "Farmer", "Architect", "Writer", "Archaeologist", "Teacher", "Author", "Statesman"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin\n# Answer:\nCharles Darwin"], "ground_truth": ["The Origin of Species (Variorum Reprint)", "The voyage of the Beagle.", "The Correspondence of Charles Darwin, Volume 5", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The structure and distribution of coral reefs.", "Volcanic Islands", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Die fundamente zur entstehung der arten", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Voyage of the Beagle (Dover Value Editions)", "To the members of the Down Friendly Club", "The Formation of Vegetable Mould through the Action of Worms", "The Structure And Distribution of Coral Reefs", "Geological Observations on South America", "Het uitdrukken van emoties bij mens en dier", "On the Movements and Habits of Climbing Plants", "Origin of Species", "The Correspondence of Charles Darwin, Volume 9", "The Voyage of the Beagle (Unabridged Classics)", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 2", "The Correspondence of Charles Darwin, Volume 1", "The Expression of the Emotions in Man and Animals", "Resa kring jorden", "The Descent of Man, and Selection in Relation to Sex", "The descent of man, and selection in relation to sex", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "La facult\u00e9 motrice dans les plantes", "The expression of the emotions in man and animals", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Correspondence of Charles Darwin, Volume 11", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Darwin Compendium", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Autobiography of Charles Darwin (Large Print)", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "On evolution", "The Essential Darwin", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Autobiography of Charles Darwin", "Charles Darwin", "Evolution and natural selection", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Autobiography of Charles Darwin, and selected letters", "Darwin's insects", "Rejse om jorden", "The Autobiography of Charles Darwin (Great Minds Series)", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 15", "The Correspondence of Charles Darwin, Volume 13: 1865", "Les mouvements et les habitudes des plantes grimpantes", "The Darwin Reader First Edition", "A Darwin Selection", "The Correspondence of Charles Darwin, Volume 13", "Notebooks on transmutation of species", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "On a remarkable bar of sandstone off Pernambuco", "Voyage of the Beagle (Harvard Classics, Part 29)", "The Expression Of The Emotions In Man And Animals", "The Origin of Species (Great Minds Series)", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 17: 1869", "The origin of species : complete and fully illustrated", "Voyage of the Beagle", "Voyage d'un naturaliste autour du monde", "Kleinere geologische Abhandlungen", "The Correspondence of Charles Darwin, Volume 4", "Darwin en Patagonia", "On the origin of species by means of natural selection", "The Autobiography Of Charles Darwin", "Darwinism stated by Darwin himself", "H.M.S. Beagle in South America", "Beagle letters", "On the tendency of species to form varieties", "The descent of man, and selection in relation to sex.", "Fertilisation of Orchids", "Origin of Species (Everyman's University Paperbacks)", "The autobiography of Charles Darwin", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Structure and Distribution of Coral Reefs", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "Memorias y epistolario i\u0301ntimo", "Del Plata a Tierra del Fuego", "The Origin of Species (Oxford World's Classics)", "The origin of species", "Gesammelte kleinere Schriften", "On Natural Selection", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Different Forms of Flowers on Plants of the Same Species", "The Power of Movement in Plants", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Motsa ha-minim", "Darwin on humus and the earthworm", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Voyage of the Beagle (NG Adventure Classics)", "The Expression of the Emotions in Man And Animals", "The structure and distribution of coral reefs", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Reise eines Naturforschers um die Welt", "The Autobiography of Charles Darwin (Dodo Press)", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Darwin's Ornithological notes", "Charles Darwin on the routes of male humble bees", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Correspondence of Charles Darwin, Volume 3", "The action of carbonate of ammonia on the roots of certain plants", "Charles Darwin's natural selection", "The Origin of Species (Enriched Classics)", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 12", "The Variation of Animals and Plants under Domestication", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Origin of Species (Collector's Library)", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "red notebook of Charles Darwin", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "From So Simple a Beginning", "The Correspondence of Charles Darwin, Volume 6", "The foundations of the Origin of species", "The Origin of Species (Mentor)", "Reise um die Welt 1831 - 36", "Insectivorous Plants", "Wu zhong qi yuan", "Charles Darwin's letters", "Diario del Viaje de Un Naturalista Alrededor", "The principal works", "The Origin of Species", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The collected papers of Charles Darwin", "Works", "The autobiography of Charles Darwin, 1809-1882", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "From Darwin's unpublished notebooks", "Evolution", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 9: 1861", "genese\u014ds t\u014dn eid\u014dn", "The Voyage of the Beagle (Everyman Paperbacks)", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Voyage of the Beagle (Great Minds Series)", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Origin of Species (Harvard Classics, Part 11)", "The\u0301orie de l'e\u0301volution", "Origins", "vari\u00eberen der huisdieren en cultuurplanten", "Darwin and Henslow", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Voyage of the Beagle", "Evolution by natural selection", "The expression of the emotions in man and animals.", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 10", "Tesakneri tsagume\u030c", "The Voyage of the Beagle (Mentor)", "Notes on the fertilization of orchids", "Darwin", "Questions about the breeding of animals", "Darwin Darwin", "Proiskhozhdenie vidov", "La vie et la correspondance de Charles Darwin", "Les moyens d'expression chez les animaux", "The Origin Of Species", "Leben und Briefe von Charles Darwin", "El Origin De Las Especies", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Correspondence of Charles Darwin, Volume 14", "The Correspondence of Charles Darwin, Volume 12: 1864", "The living thoughts of Darwin", "Autobiography of Charles Darwin", "Die geschlechtliche Zuchtwahl", "Charles Darwin's marginalia", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The descent of man and selection in relation to sex.", "The Orgin of Species", "Diary of the voyage of H.M.S. Beagle", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Human nature, Darwin's view", "The portable Darwin", "The voyage of Charles Darwin", "The Origin of Species (World's Classics)", "The Origin of Species (Great Books : Learning Channel)", "Voyage Of The Beagle", "ontstaan der soorten door natuurlijke teeltkeus", "The Voyage of the Beagle (Adventure Classics)", "The Descent of Man and Selection in Relation to Sex", "Darwin's notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 8", "More Letters of Charles Darwin", "monograph on the sub-class Cirripedia", "Darwin for Today", "The Correspondence of Charles Darwin, Volume 7", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The geology of the voyage of H.M.S. Beagle", "From so simple a beginning", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Correspondence of Charles Darwin, Volume 14: 1866", "Metaphysics, Materialism, & the evolution of mind", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Opsht\u0323amung fun menshen", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)"], "ans_acc": 0.009345794392523364, "ans_hit": 1, "ans_f1": 0.018518518518518517, "ans_precission": 1.0, "ans_recall": 0.009345794392523364, "path_f1": 0.06666666666666667, "path_precision": 1.0, "path_recall": 0.034482758620689655, "path_ans_f1": 0.018518518518518517, "path_ans_precision": 1.0, "path_ans_recall": 0.009345794392523364}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.institution -> University of Florida\n# Answer:\nm.05v4mnf", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnl -> education.education.institution -> Allen D. Nease High School\n# Answer:\nm.05v4mnl", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.major_field_of_study -> Family, Youth and Community Sciences\n# Answer:\nm.05v4mnf", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nm.0vxk_df", "# Reasoning Path:\nTim Tebow\n# Answer:\nTim Tebow", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0vxk_df"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning\n# Answer:\nPeyton Manning", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nm.0j4z5bh", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j4z5bh", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0j5d2kv"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.16666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.geolocation -> m.02_lmf5\n# Answer:\nm.02_lmf5", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> France\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> Spain\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> France\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> Kingdom of the Netherlands\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains\n# Answer:\nCarpathian Mountains", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns\n# Answer:\nRobert Burns", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> The Passion-Driven Writer and the Digital-Age Literary Marketplace\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Contemporary Authors: A Bio-Bibliographical Guide to Current Writers in Fiction, General Nonfiction, Poetry, Journalism, Drama, Motion Pictures, Television. Vol. 249\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter"], "ground_truth": ["Poet", "Bard", "Author", "Writer"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8076923076923077, "ans_precission": 0.875, "ans_recall": 0.75, "path_f1": 0.8076923076923077, "path_precision": 0.875, "path_recall": 0.75, "path_ans_f1": 0.8076923076923077, "path_ans_precision": 0.875, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": [], "ground_truth": ["Hayden Christensen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning\n# Answer:\nPeyton Manning"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": [], "ground_truth": ["Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War\n# Answer:\nGulf War", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Invasion of Kuwait -> military.military_conflict.combatants -> m.0bhdrfq\n# Answer:\nInvasion of Kuwait", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.04fvd6y -> military.military_combatant_group.combatants -> Saudi Arabia\n# Answer:\nm.04fvd6y", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Invasion of Kuwait -> military.military_conflict.combatants -> m.04yxs82\n# Answer:\nInvasion of Kuwait", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Gulf War air campaign -> military.military_conflict.combatants -> m.0dh5lqx\n# Answer:\nGulf War air campaign"], "ground_truth": ["Argentina", "Iraq", "Saudi Arabia", "France", "Australia", "United States of America", "United Kingdom"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.09090909090909091, "path_precision": 0.3333333333333333, "path_recall": 0.05263157894736842, "path_ans_f1": 0.6250000000000001, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-119", "prediction": [], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-12", "prediction": [], "ground_truth": ["John Kasich", "Return J. Meigs, Jr.", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham\n# Answer:\nDavid Beckham", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0bvddcw -> sports.sports_team_roster.team -> Real Madrid C.F.\n# Answer:\nm.0bvddcw", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0pxgr8r -> sports.sports_team_roster.team -> England national under-21 football team\n# Answer:\nm.0pxgr8r", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0pxgqz5 -> sports.sports_team_roster.team -> England national football team\n# Answer:\nm.0pxgqz5", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0bvddcw -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.0bvddcw", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0pxgr8r -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.0pxgr8r", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0pxgqz5 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0pxgqz5"], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado\n# Answer:\nFrancisco V\u00e1zquez de Coronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Timothy Olague\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Ed Soph\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> m.0hk9n_c\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Armando Favela\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> common.topic.article -> m.0r4xz\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> m.0hk9n_4\n# Answer:\nCoronado"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy\n# Answer:\nJohn F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant\n# Answer:\nFukushima Daiichi Nuclear Power Plant", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> common.topic.image -> The Fukushima 1 NPP -> common.image.size -> m.0k127w\n# Answer:\nThe Fukushima 1 NPP", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> periodicals.newspaper_circulation_area.newspapers -> Daily Express\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> periodicals.newspaper_circulation_area.newspapers -> SecEd Newspaper\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> periodicals.newspaper_circulation_area.newspapers -> The Sun\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Scotland -> base.aareas.schema.administrative_area.administrative_children -> Edinburgh\n# Answer:\nScotland"], "ground_truth": ["Northern Ireland", "Scotland", "England", "Wales"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7999999999999999, "ans_precission": 0.8571428571428571, "ans_recall": 0.75, "path_f1": 0.26666666666666666, "path_precision": 0.2857142857142857, "path_recall": 0.25, "path_ans_f1": 0.7999999999999999, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.75}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana\n# Answer:\nLouisiana", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo\n# Answer:\nAugustine of Hippo", "# Reasoning Path:\nAugustine of Hippo -> religion.religious_leader.religious_leadership -> m.0dgvxmz -> religion.religious_organization_leadership.role -> Doctor of the Church\n# Answer:\nm.0dgvxmz", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Boethius -> influence.influence_node.influenced -> Dante Alighieri\n# Answer:\nBoethius", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Boethius -> influence.influence_node.influenced -> Johannes Scotus Eriugena\n# Answer:\nBoethius", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> influence.influence_node.influenced -> g.12307f46\n# Answer:\nBlaise Pascal", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Boethius -> book.author.works_written -> The Consolation of Philosophy\n# Answer:\nBoethius", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Boethius -> influence.influence_node.influenced -> Albertus Magnus\n# Answer:\nBoethius", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Boethius -> common.topic.notable_for -> g.1258k3wm1\n# Answer:\nBoethius", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> influence.influence_node.influenced -> Friedrich Nietzsche\n# Answer:\nBlaise Pascal"], "ground_truth": ["Philosopher", "Physician", "Writer"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-21", "prediction": [], "ground_truth": ["Sal Gibson", "Francine Lons", "Leon Cole"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt\n# Answer:\nEgypt", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government -> Government of Egypt -> government.government.agency -> Central Agency for Public Mobilization and Statistics\n# Answer:\nGovernment of Egypt", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> House of Representatives\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> Shura Council\n# Answer:\nParliament of Egypt"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": [], "ground_truth": ["Memphis"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe\n# Answer:\nEdgar Allan Poe", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta\n# Answer:\nAtlanta", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Atlanta History Center -> exhibitions.exhibition_venue.exhibitions_at_this_venue -> m.05by1nb\n# Answer:\nAtlanta History Center", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> Georgia\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> Douglas County\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Atlanta History Center -> location.location.containedby -> Fulton County\n# Answer:\nAtlanta History Center"], "ground_truth": ["Four Seasons Hotel Atlanta", "Atlanta Symphony Orchestra", "Arbor Place Mall", "CNN Center", "Fernbank Science Center", "Masquerade", "The Tabernacle", "Fernbank Museum of Natural History", "World of Coca-Cola", "Atlanta Jewish Film Festival", "Atlanta Marriott Marquis", "Georgia World Congress Center", "Centennial Olympic Park", "Six Flags White Water", "Georgia State Capitol", "Hyatt Regency Atlanta", "Atlanta Cyclorama & Civil War Museum", "Center for Puppetry Arts", "Georgia Dome", "Peachtree Road Race", "Turner Field", "Philips Arena", "Atlanta Ballet", "Omni Coliseum", "Margaret Mitchell House & Museum", "Atlanta History Center", "Variety Playhouse", "Underground Atlanta", "Cobb Energy Performing Arts Centre", "Fox Theatre", "Six Flags Over Georgia", "Martin Luther King, Jr. National Historic Site", "Jimmy Carter Library and Museum", "Zoo Atlanta", "Georgia Aquarium", "Woodruff Arts Center"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.1518987341772152, "ans_precission": 0.8571428571428571, "ans_recall": 0.08333333333333333, "path_f1": 0.1518987341772152, "path_precision": 0.8571428571428571, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19672131147540986, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh\n# Answer:\nAnna Bligh", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.basic_title -> Premier\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street\n# Answer:\nCoronation Street"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray\n# Answer:\nAndy Murray", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.first_level_division_of -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#range -> Location\n# Answer:\nPlace of birth"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar\n# Answer:\nAustralian dollar"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": [], "ground_truth": ["Central European Time Zone"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton\n# Answer:\nCam Newton", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04nb7yn"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick\n# Answer:\nFrederick", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee\n# Answer:\nHarper Lee", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nm.0lwxmy9", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah\n# Answer:\nUtah", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush\n# Answer:\nGeorge W. Bush", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> common.topic.notable_for -> g.1257w3www\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.party -> Republican Party\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> common.topic.notable_types -> Election campaign\n# Answer:\nGeorge W. Bush presidential campaign, 2000"], "ground_truth": ["Gene Amondson", "Ralph Nader", "John Kerry", "Michael Peroutka"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson\n# Answer:\nNiall Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4jq57"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.22222222222222224, "path_precision": 0.14285714285714285, "path_recall": 0.5, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands\n# Answer:\nGal\u00e1pagos Islands", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.administrative_division.country -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.geolocation -> m.05l1d9y\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.notable_for -> g.1255fs0l4\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Gal\u00e1pagos National Park -> protected_sites.protected_site.iucn_category -> National park\n# Answer:\nGal\u00e1pagos National Park"], "ground_truth": ["Ecuador", "Gal\u00e1pagos Province", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> All Around The World -> music.composition.recordings -> All Around The World (featuring Ludacris)\n# Answer:\nAll Around The World", "# Reasoning Path:\nJustin Bieber\n# Answer:\nJustin Bieber", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> Never Let You Go -> music.composition.lyricist -> Johnt\u00e1 Austin\n# Answer:\nNever Let You Go", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> All Around The World -> music.composition.recordings -> All Around the World\n# Answer:\nAll Around The World", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> All Around The World -> music.composition.composer -> Ludacris\n# Answer:\nAll Around The World", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> All Around The World -> music.composition.composer -> Ludacris\n# Answer:\nAll Around The World", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> All Around The World -> music.composition.recordings -> All Around the World (acoustic version)\n# Answer:\nAll Around The World", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> All Around The World -> common.topic.article -> m.09c4dl\n# Answer:\nAll Around The World", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> Never Let You Go -> music.composition.lyricist -> Bryan-Michael Cox\n# Answer:\nNever Let You Go", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Baby -> music.composition.composer -> Terius Nash\n# Answer:\nBaby"], "ground_truth": ["Wait for a Minute", "Roller Coaster", "Confident", "Right Here", "Die in Your Arms", "Somebody to Love", "Pray", "Heartbreaker", "Thought Of You", "Lolly", "Never Say Never", "As Long as You Love Me", "Never Let You Go", "Baby", "Home to Mama", "Beauty And A Beat", "Eenie Meenie", "All That Matters", "Hold Tight", "All Around The World", "All Bad", "First Dance", "Change Me", "#thatPower", "Live My Life", "Turn to You (Mother's Day Dedication)", "Boyfriend", "Bigger", "PYD", "Bad Day", "Recovery"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.17475728155339806, "ans_precission": 0.9, "ans_recall": 0.0967741935483871, "path_f1": 0.15584415584415584, "path_precision": 0.4, "path_recall": 0.0967741935483871, "path_ans_f1": 0.17475728155339806, "path_ans_precision": 0.9, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau\n# Answer:\nGeorges Cl\u00e9menceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> book.author.works_written -> Le silence de M. Clemenceau -> common.topic.notable_for -> g.125bm54_b\n# Answer:\nLe silence de M. Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> book.author.works_written -> La France devant L'Allemagne -> common.topic.notable_for -> g.125bd954b\n# Answer:\nLa France devant L'Allemagne", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> book.author.works_written -> Le silence de M. Clemenceau -> common.topic.notable_types -> Book\n# Answer:\nLe silence de M. Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> book.author.works_written -> La France devant L'Allemagne -> common.topic.notable_types -> Book\n# Answer:\nLa France devant L'Allemagne"], "ground_truth": ["Publisher", "Physician", "Journalist", "Writer", "Statesman"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-44", "prediction": [], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.places_lived -> m.0h30zxl -> people.place_lived.location -> Barbados\n# Answer:\nm.0h30zxl"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison\n# Answer:\nWilliam Henry Harrison", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez\n# Answer:\nGeorge Lopez", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094j13b -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.094j13b", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094j13b -> common.webpage.resource -> The George Lopez Show\n# Answer:\nm.094j13b", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094k4jf -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.094k4jf", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094k4jf -> common.webpage.resource -> George Lopez will host Latin Grammys\n# Answer:\nm.094k4jf"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group\n# Answer:\nSamsung Group", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nm.03l9ynf"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam\n# Answer:\nIslam", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Jehovah's Witnesses\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Christianity\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Moses and Monotheism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Conservative Judaism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfc1g\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Beside Still Waters\n# Answer:\nMonotheism"], "ground_truth": ["Islamic holy books", "God in Islam", "Sharia", "\u1e6c\u016bb\u0101", "Islamic view of angels", "Masih ad-Dajjal", "Prophets in Islam", "Predestination in Islam", "Monotheism", "Mahdi", "Tawhid", "Entering Heaven alive", "Qiyamah"], "ans_acc": 0.07692307692307693, "ans_hit": 1, "ans_f1": 0.1411764705882353, "ans_precission": 0.8571428571428571, "ans_recall": 0.07692307692307693, "path_f1": 0.1411764705882353, "path_precision": 0.8571428571428571, "path_recall": 0.07692307692307693, "path_ans_f1": 0.1411764705882353, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.07692307692307693}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey\n# Answer:\nChristian Grey", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell\n# Answer:\nGeorge Orwell", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler\n# Answer:\nAdolf Hitler", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> Austria -> location.location.containedby -> Europe\n# Answer:\nAustria", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> German Reich -> location.location.containedby -> Europe\n# Answer:\nGerman Reich", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> Austria -> location.country.first_level_divisions -> Vienna\n# Answer:\nAustria", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> Austria -> location.country.capital -> Vienna\n# Answer:\nAustria", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> German Reich -> location.location.events -> The Holocaust\n# Answer:\nGerman Reich"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.09999999999999999, "path_precision": 0.1111111111111111, "path_recall": 0.09090909090909091, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9\n# Answer:\nMichael Bubl\u00e9", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nm.03lpqjt", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.resource -> Indicates the official home page for an artist.\n# Answer:\nm.03lpqjt"], "ground_truth": ["Singer", "Songwriter", "Actor"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson\n# Answer:\nJackie Robinson", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> sports.sports_team_roster.team -> Kansas City Monarchs\n# Answer:\nm.0ncxlxp", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nm.0ncxm4r", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0ncxlxp", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0ncxlxp", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0ncxm4r", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.position -> Second baseman\n# Answer:\nm.0ncxm4r"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie\n# Answer:\nAnnie", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Date Opened\n# Answer:\nDate Opened", "# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0j56jnl\n# Answer:\nm.0j56jnl", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Date Opened\n# Answer:\nDate Opened", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Date Closed\n# Answer:\nDate Closed"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell\n# Answer:\nJaMarcus Russell", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09grg7l -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.09grg7l", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09tckd5 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.09tckd5", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09grg7l -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\nm.09grg7l", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09tckd5 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\nm.09tckd5"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt\n# Answer:\nEleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia\n# Answer:\nIndonesia", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nm.064szk2", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w"], "ground_truth": ["Protestantism", "Catholicism", "Islam", "Hinduism"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.6666666666666666, "path_recall": 0.4, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.5}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James\n# Answer:\nJesse James", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens\n# Answer:\nCharles Dickens", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Adelaide Anne Procter\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Elizabeth Gaskell\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.notable_for -> g.1255hfpzv\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Wilkie Collins\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.article -> m.0273w68\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> \\\"The Life and Adventures of Nicolas Nickleby\\\" followed, then \\\"The Old Curiosity Shop\\\", which featured one of Dickens's  best known characters, Little Nell. -> base.kwebbase.kwsentence.next_sentence -> Then came \\\"Barnaby Rudge\\\", a historical novel.\n# Answer:\n\\\"The Life and Adventures of Nicolas Nickleby\\\" followed, then \\\"The Old Curiosity Shop\\\", which featured one of Dickens's  best known characters, Little Nell.", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> \\\"The Life and Adventures of Nicolas Nickleby\\\" followed, then \\\"The Old Curiosity Shop\\\", which featured one of Dickens's  best known characters, Little Nell. -> base.kwebbase.kwsentence.previous_sentence -> The first instalment of \\\"Oliver Twist\\\" appeared in 1837 in \\\"Bentley's Miscellany\\\", where Dickens worked as an editor.\n# Answer:\n\\\"The Life and Adventures of Nicolas Nickleby\\\" followed, then \\\"The Old Curiosity Shop\\\", which featured one of Dickens's  best known characters, Little Nell."], "ground_truth": ["The life and adventures of Nicholas Nickleby", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Bantam Classic)", "The cricket on the hearth", "A Tale of Two Cities (Dramascripts S.)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Ladybird Classics)", "Little Dorrit", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Masterworks)", "Dombey and Son.", "A Tale of Two Cities (Penguin Classics)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Adopted Classic)", "Martin Chuzzlewit", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Children's Classics)", "Great expectations.", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Soundings)", "The Old Curiosity Shop", "A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Acting Edition)", "David Copperfield", "A Tale of Two Cities (Konemann Classics)", "Our mutual friend", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Dramatized)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (BBC Audio Series)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (The Classic Collection)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Illustrated Classics)", "A Christmas Carol (R)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Signet Classics)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale of Two Cities (The Greatest Historical Novels)", "Bleak House", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Large Print)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (Take Part)", "A Christmas Carol (Pacemaker Classic)", "Great expectations", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Tor Classics)", "A CHRISTMAS CAROL", "Great Expectations", "A Tale of Two Cities (Cassette (1 Hr).)", "Our mutual friend.", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Watermill Classics)", "Oliver Twist", "A Christmas Carol (Gollancz Children's Classics)", "A Christmas Carol (The Kennett Library)", "The mystery of Edwin Drood", "David Copperfield.", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "Bleak house", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Classic Fiction)", "The old curiosity shop", "A Christmas Carol (Watermill Classic)", "Bleak House.", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Classic Collection)", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Compact English Classics)", "The Pickwick papers", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Longman Fiction)", "Hard times", "The Pickwick Papers", "A Tale of Two Cities (Oxford Playscripts)", "A Christmas Carol (Nelson Graded Readers)", "A Tale of Two Cities (Naxos AudioBooks)", "Dombey and Son", "A Christmas Carol (Whole Story)", "A Christmas Carol (Audio Editions)", "The old curiosity shop.", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (New Longman Literature)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities", "A Tale of Two Cities (Everyman's Library Classics)", "Dombey and son", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "Great Expectations.", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (40th Anniversary Edition)", "Sketches by Boz", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Enriched Classic)", "A TALE OF TWO CITIES", "A Christmas Carol (Dramascripts)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol", "A Christmas Carol (Puffin Choice)", "A Tale of Two Cities (Progressive English)"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0.03314917127071824, "ans_precission": 0.25, "ans_recall": 0.01775147928994083, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.04324324324324324, "path_ans_precision": 0.25, "path_ans_recall": 0.023668639053254437}
{"id": "WebQTest-65", "prediction": [], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico\n# Answer:\nPuerto Rico", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson\n# Answer:\nCarl Wilson", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": [], "ground_truth": ["William Daniels"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood\n# Answer:\nBrentwood", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver\n# Answer:\nGeorge Washington Carver"], "ground_truth": ["Diamond"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox\n# Answer:\nMichael J. Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> common.topic.webpage -> m.094kfwk -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.094kfwk", "# Reasoning Path:\nMichael J. Fox -> common.topic.webpage -> m.094kfwk -> common.webpage.resource -> Michael J. Fox will appear on ''Scrubs''\n# Answer:\nm.094kfwk"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.16666666666666666, "path_recall": 0.125, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Chancellorsville -> military.military_conflict.military_personnel_involved -> Henry Heth\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.military_personnel_involved -> John Brown Gordon\n# Answer:\nBattle of Antietam", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Chancellorsville -> military.military_conflict.military_personnel_involved -> Edward Porter Alexander\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Chancellorsville -> base.culturalevent.event.entity_involved -> Joseph Hooker\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.combatants -> m.049y38h\n# Answer:\nBattle of Antietam", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9mr -> military.military_command.military_conflict -> Battle of Cedar Mountain\n# Answer:\nm.04fv9mr", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Chancellorsville -> military.military_conflict.military_personnel_involved -> Adolph von Steinwehr\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Chancellorsville -> base.americancivilwar.battle.result_of_conflict -> m.0gv1sq6\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.combatants -> m.03z965k\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> military.military_conflict.military_personnel_involved -> Richard S. Ewell\n# Answer:\nJackson's Valley Campaign"], "ground_truth": ["First Battle of Rappahannock Station", "Battle of Chancellorsville", "First Battle of Kernstown", "Battle of Port Republic", "Battle of White Oak Swamp", "Battle of Cedar Mountain", "First Battle of Winchester", "Battle of Hancock", "Battle of Harpers Ferry", "Manassas Station Operations", "Battle of McDowell", "American Civil War", "Second Battle of Bull Run", "Battle of Front Royal", "Jackson's Valley Campaign", "How Few Remain", "Battle of Chantilly", "Romney Expedition", "Battle of Hoke's Run"], "ans_acc": 0.21052631578947367, "ans_hit": 1, "ans_f1": 0.2576687116564417, "ans_precission": 0.7, "ans_recall": 0.15789473684210525, "path_f1": 0.13186813186813187, "path_precision": 0.6, "path_recall": 0.07407407407407407, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.8, "path_ans_recall": 0.21052631578947367}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people\n# Answer:\nMaasai people", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nm.0j4kb46"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.16666666666666666, "path_recall": 0.25, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": [], "ground_truth": ["Pancreatic cancer"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci\n# Answer:\nLeonardo da Vinci", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Mona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci\n# Answer:\nMona Lisa", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Painter -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainter", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation -> common.topic.notable_types -> Artwork\n# Answer:\nAnnunciation", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Painter -> common.topic.subject_of -> Fabrice de Villeneuve\n# Answer:\nPainter", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Painter -> people.profession.corresponding_type -> Visual Artist\n# Answer:\nPainter"], "ground_truth": ["The Virgin and Child with St Anne and St John the Baptist", "g.1219sb0g", "Vitruvian Man", "Leda and the Swan", "The Virgin and Child with St. Anne", "La belle ferronni\u00e8re", "Sala delle Asse", "Madonna of the Yarnwinder", "Virgin of the Rocks", "The Battle of Anghiari", "Leonardo's horse", "Ginevra de' Benci", "Madonna of Laroque", "g.120vt1gz", "Adoration of the Magi", "The Last Supper", "g.1224tf0c", "Benois Madonna", "Portrait of a man in red chalk", "g.121wt37c", "Head of a Woman", "Bacchus", "g.1213jb_b", "The Holy Infants Embracing", "Salvator Mundi", "Annunciation", "St. John the Baptist", "Drapery for a Seated Figure", "Portrait of a Young Fianc\u00e9e", "Mona Lisa", "g.121yh91r", "Portrait of Isabella d'Este", "Madonna Litta", "g.12215rxg", "g.12314dm1", "The Baptism of Christ", "St. Jerome in the Wilderness", "g.1239jd9p", "Medusa", "Portrait of a Musician", "Horse and Rider", "Lucan portrait of Leonardo da Vinci", "Madonna of the Carnation", "Madonna and Child with St Joseph", "Lady with an Ermine"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.11764705882352941, "ans_precission": 0.5, "ans_recall": 0.06666666666666667, "path_f1": 0.12000000000000001, "path_precision": 0.5, "path_recall": 0.06818181818181818, "path_ans_f1": 0.11764705882352941, "path_ans_precision": 0.5, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria\n# Answer:\nAustria", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n\n# Answer:\nEva Per\u00f3n", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.symptoms -> cerebella tumor\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.symptoms -> Desmoplasia\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjftz\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.symptoms -> Medulloblastoma\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> The Cancer Journals\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfyd\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfrp\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha\n# Answer:\nGautama Buddha", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.containedby -> Nepal\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.notable_for -> g.125fblhtl\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> An Armonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.0f888dp\n# Answer:\nGlass harmonica"], "ground_truth": ["Lightning rod", "Franklin stove", "Bifocals", "Glass harmonica"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8076923076923077, "ans_precission": 0.875, "ans_recall": 0.75, "path_f1": 0.8076923076923077, "path_precision": 0.875, "path_recall": 0.75, "path_ans_f1": 0.8076923076923077, "path_ans_precision": 0.875, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley\n# Answer:\nGreeley", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky\n# Answer:\nPyotr Ilyich Tchaikovsky", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.012ggq1s\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Eugene Onegin -> common.topic.image -> Yevgeny Onegin by Repin\n# Answer:\nEugene Onegin", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.012ggp0l\n# Answer:\nComposer"], "ground_truth": ["Musician", "Composer", "Librettist"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.75, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.first_level_divisions -> Berlin\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium -> location.country.languages_spoken -> French\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium -> location.country.official_language -> French\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Germany\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Saxon, Upper Language\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.first_level_divisions -> Hamburg\n# Answer:\nGermany"], "ground_truth": ["Switzerland", "East Germany", "Luxembourg", "Belgium", "Liechtenstein", "Austria", "Germany"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5526315789473685, "ans_precission": 0.7777777777777778, "ans_recall": 0.42857142857142855, "path_f1": 0.30769230769230765, "path_precision": 0.3333333333333333, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5783132530120482, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon\n# Answer:\nJohn Lennon", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Psychedelic rock\n# Answer:\nPsychedelic rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock\n# Answer:\nArt rock"], "ground_truth": ["Pop rock", "Experimental music", "Soft rock", "Blues rock", "Pop music", "Experimental rock", "Psychedelic rock", "Rock music", "Art rock"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.75, "ans_recall": 0.3333333333333333, "path_f1": 0.46153846153846156, "path_precision": 0.75, "path_recall": 0.3333333333333333, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.75, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-85", "prediction": [], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-86", "prediction": [], "ground_truth": ["Denmark"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-87", "prediction": [], "ground_truth": ["98108", "98119-4114", "98103", "98178", "98118", "98175", "98144", "98139", "98132", "98168", "98117", "98109", "98188", "98115", "98177", "98121", "98138", "98195", "98114", "98171", "98190", "98166", "98141", "98112", "98160", "98174", "98194", "98145", "98191", "98185", "98161", "98127", "98113", "98148", "98104", "98170", "98126", "98101", "98107", "98136", "98155", "98165", "98146", "98164", "98181", "98199", "98116", "98158", "98134", "98154", "98129", "98111", "98184", "98125", "98198", "98133", "98106", "98124", "98131", "98122", "98119", "98102", "98105"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-88", "prediction": [], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language\n# Answer:\nChinese language", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters"], "ground_truth": ["'Phags-pa script", "Traditional Chinese characters", "Simplified Chinese character", "Chinese characters", "N\u00fcshu script"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.5, "ans_recall": 0.2, "path_f1": 0.28571428571428575, "path_precision": 0.5, "path_recall": 0.2, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.5, "path_ans_recall": 0.2}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nm.02h98gq"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.25, "path_recall": 0.14285714285714285, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons\n# Answer:\nThe Jeffersons", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.0l5j2hv -> tv.regular_tv_appearance.actor -> Jay Hammer\n# Answer:\nm.0l5j2hv", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdh -> tv.regular_tv_appearance.actor -> Marla Gibbs\n# Answer:\nm.03lkkdh", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdn -> tv.regular_tv_appearance.actor -> Franklin Cover\n# Answer:\nm.03lkkdn", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> tv.tv_series_season.episodes -> A Friend in Need\n# Answer:\nThe Jeffersons - Season 1", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> tv.tv_series_season.episodes -> Former Neighbors\n# Answer:\nThe Jeffersons - Season 1", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> common.topic.notable_for -> g.1259dkfrc\n# Answer:\nThe Jeffersons - Season 1", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> tv.tv_series_season.episodes -> Entertainment Tonight: Whatever Happened To...\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Roxie Roker", "Isabel Sanford", "Jay Hammer", "Damon Evans", "Berlinda Tolbert", "Marla Gibbs", "Zara Cully", "Franklin Cover", "Paul Benedict", "Sherman Hemsley", "Mike Evans"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08955223880597016, "path_precision": 0.375, "path_recall": 0.05084745762711865, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.375, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-91", "prediction": [], "ground_truth": ["Sing Tao Daily", "Dock of the Bay", "Synapse", "California Star", "The San Francisco Examiner", "Bay Area Reporter", "San Francisco News-Call Bulletin Newspaper", "San Francisco Foghorn", "The Golden Era", "San Francisco Bay View", "AsianWeek", "San Francisco Bay Times", "San Francisco Chronicle", "San Francisco Call", "San Francisco Bay Guardian", "Street Sheet", "The Daily Alta California", "San Francisco Daily", "Free Society", "San Francisco Business Times"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia\n# Answer:\nArmenia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage\n# Answer:\nRandy Savage", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin\n# Answer:\nCharles Darwin", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Charles Darwin: Voyaging\n# Answer:\nCharles Darwin: Voyaging"], "ground_truth": ["Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Volcanic Islands", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Die fundamente zur entstehung der arten", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "To the members of the Down Friendly Club", "The Formation of Vegetable Mould through the Action of Worms", "Geological Observations on South America", "Het uitdrukken van emoties bij mens en dier", "On the Movements and Habits of Climbing Plants", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Expression of the Emotions in Man and Animals", "Resa kring jorden", "The Descent of Man, and Selection in Relation to Sex", "La facult\u00e9 motrice dans les plantes", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Darwin Compendium", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "On evolution", "The Essential Darwin", "The Autobiography of Charles Darwin", "Charles Darwin", "Evolution and natural selection", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin's insects", "Rejse om jorden", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Life of Erasmus Darwin", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The Correspondence of Charles Darwin, Volume 13: 1865", "Les mouvements et les habitudes des plantes grimpantes", "The Darwin Reader First Edition", "A Darwin Selection", "Notebooks on transmutation of species", "On a remarkable bar of sandstone off Pernambuco", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 17: 1869", "Voyage d'un naturaliste autour du monde", "Kleinere geologische Abhandlungen", "On the origin of species by means of natural selection", "Darwin en Patagonia", "Darwin from Insectivorous Plants to Worms", "Darwinism stated by Darwin himself", "Beagle letters", "H.M.S. Beagle in South America", "On the tendency of species to form varieties", "Fertilisation of Orchids", "The Life and Letters of Charles Darwin Volume 1", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Structure and Distribution of Coral Reefs", "Memorias y epistolario i\u0301ntimo", "Del Plata a Tierra del Fuego", "Evolutionary Writings: Including the Autobiographies", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 11: 1863", "On Natural Selection", "The Different Forms of Flowers on Plants of the Same Species", "The Power of Movement in Plants", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Motsa ha-minim", "Darwin on humus and the earthworm", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Reise eines Naturforschers um die Welt", "Darwin's Ornithological notes", "Charles Darwin on the routes of male humble bees", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The action of carbonate of ammonia on the roots of certain plants", "South American Geology", "Charles Darwin's natural selection", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin's journal", "The Variation of Animals and Plants under Domestication", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 18: 1870", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The foundations of the Origin of species", "Reise um die Welt 1831 - 36", "Insectivorous Plants", "Wu zhong qi yuan", "Charles Darwin's letters", "Diario del Viaje de Un Naturalista Alrededor", "The principal works", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The collected papers of Charles Darwin", "Works", "Evolution", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "From Darwin's unpublished notebooks", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 9: 1861", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The\u0301orie de l'e\u0301volution", "Origins", "vari\u00eberen der huisdieren en cultuurplanten", "Darwin and Henslow", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Voyage of the Beagle", "Evolution by natural selection", "The Correspondence of Charles Darwin, Volume 10: 1862", "Geological Observations on the Volcanic Islands", "Tesakneri tsagume\u030c", "Notes on the fertilization of orchids", "Darwin", "Questions about the breeding of animals", "Darwin Darwin", "Proiskhozhdenie vidov", "La vie et la correspondance de Charles Darwin", "Les moyens d'expression chez les animaux", "Leben und Briefe von Charles Darwin", "El Origin De Las Especies", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Correspondence of Charles Darwin, Volume 12: 1864", "The living thoughts of Darwin", "Die geschlechtliche Zuchtwahl", "Charles Darwin's marginalia", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The Orgin of Species", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Diary of the voyage of H.M.S. Beagle", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Human nature, Darwin's view", "The portable Darwin", "The voyage of Charles Darwin", "ontstaan der soorten door natuurlijke teeltkeus", "Darwin's notebooks on transmutation of species", "More Letters of Charles Darwin", "monograph on the sub-class Cirripedia", "Darwin for Today", "The Life and Letters of Charles Darwin Volume 2", "The geology of the voyage of H.M.S. Beagle", "From so simple a beginning", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Correspondence of Charles Darwin, Volume 14: 1866", "Metaphysics, Materialism, & the evolution of mind", "Opsht\u0323amung fun menshen", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct"], "ans_acc": 0.0196078431372549, "ans_hit": 1, "ans_f1": 0.02580645161290323, "ans_precission": 1.0, "ans_recall": 0.013071895424836602, "path_f1": 0.07142857142857142, "path_precision": 1.0, "path_recall": 0.037037037037037035, "path_ans_f1": 0.038461538461538464, "path_ans_precision": 1.0, "path_ans_recall": 0.0196078431372549}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You've Really Got a Hold on Me -> music.recording.artist -> Engelbert Humperdinck\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You Really Got a Hold on Me -> music.recording.artist -> Jools Holland & His Rhythm & Blues Orchestra\n# Answer:\nYou Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You've Really Got a Hold on Me -> music.recording.artist -> Michael Jackson\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You've Really Got a Hold on Me -> music.composition.recordings -> You've Really Got A Hold On Me\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson\n# Answer:\nSmokey Robinson", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You've Really Got a Hold on Me -> music.recording.artist -> Rod Stewart\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You Really Got a Hold on Me -> music.recording.song -> You've Really Got a Hold on Me\n# Answer:\nYou Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You Really Got a Hold on Me -> common.topic.notable_types -> Musical Recording\n# Answer:\nYou Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger"], "ground_truth": ["Love Bath", "You've Really Got a Hold on Me", "And I Don't Love You", "Food For Thought", "I Want You Back", "Come by Here (Kum Ba Ya)", "When A Woman Cries", "Never My Love / Never Can Say Goodbye", "Tell Me Tomorrow, Part 1", "Let Me Be The Clock", "Tears Of A Clown", "Don't Play Another Love Song", "I Can't Get Enough", "Really Gonna Miss You", "Christmas Everyday", "Coincidentally", "Theme From the Big Time", "Don't Wanna Be Just Physical", "Ooo Baby Baby (live)", "Heavy On Pride (Light On Love)", "Just Like You", "The Way You Do (The Things You Do)", "Tea for Two", "Unless You Do It Again", "In My Corner", "Ooo Baby Baby", "The Tracks of My Tears (live)", "Happy (Love Theme From Lady Sings the Blues)", "Please Come Home for Christmas", "Crusin", "I'm in the Mood for Love", "My World", "No Time to Stop Believing", "It's Christmas Time", "The Christmas Song", "I've Made Love To You A Thousand Times", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "With Your Love Came", "Virgin Man", "He Can Fix Anything", "Hanging on by a Thread", "I Know You by Heart", "I Love The Nearness Of You", "Going to a Go Go", "Nearness of You", "Jesus Told Me To Love You", "The Agony and the Ecstasy", "Let Your Light Shine On Me", "Why Do Happy Memories Hurt So Bad", "I Can't Find", "Going to a Gogo", "Deck the Halls", "Going to a Go-Go", "Some People Will Do Anything for Love", "My Girl", "Same Old Love", "Wedding Song", "Who's Sad", "Santa Claus is Coming to Town", "One Heartbeat", "Easy", "And I Love Her", "I Care About Detroit", "Shoe Soul", "Love Brought Us Here", "Did You Know (Berry's Theme)", "Night and Day", "Ain't That Peculiar", "It's a Good Feeling", "Girl I'm Standing There", "Love Don't Give No Reason", "Love Letters", "Aqui Con Tigo (Being With You)", "It's Time to Stop Shoppin' Around", "You Cannot Laugh Alone", "Being With You", "So Bad", "Holly", "Be Who You Are", "The Tracks of My Tears", "Sweet Harmony", "I Second That Emotions", "Why Are You Running From My Love", "You're the One for Me (feat. Joss Stone)", "Time Flies", "Tears of a Sweet Free Clown", "Pops, We Love You", "Fallin'", "There Will Come a Day (I'm Gonna Happen to You)", "Tell Me Tomorrow (12\\\" extended mix)", "Be Careful What You Wish For (instrumental)", "The Hurt's On You", "Ooh Baby Baby", "If You Wanna Make Love", "I Second That Emotion", "Skid Row", "Cruisin", "Walk on By", "If You Want My Love", "Gang Bangin'", "I Am, I Am", "Blame It On Love (Duet with Barbara Mitchell)", "Satisfy You", "Open", "You Are Forever", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Don't Know Why", "Asleep on My Love", "That Place", "I'm Glad There Is You", "We've Saved the Best for Last", "Because of You It's the Best It's Ever Been", "Season's Greetings from Smokey Robinson", "You're Just My Life (feat. India.Arie)", "Sleepless Nights", "The Tears Of A Clown", "Our Love Is Here to Stay", "You Made Me Feel Love", "We Are The Warriors", "She's Only a Baby Herself", "Will You Still Love Me Tomorrow", "Quiet Storm", "I\u2019ve Got You Under My Skin", "The Tears of a Clown", "Little Girl Little Girl", "Mother's Son", "Shop Around", "A Child Is Waiting", "Tell Me Tomorrow", "You Are So Beautiful (feat. Dave Koz)", "I Hear The Children Singing", "It's Her Turn to Live", "Standing On Jesus", "Just To See Her Again", "You've Really Go a Hold on Me", "Fly Me to the Moon (In Other Words)", "Everything for Christmas", "I Like Your Face", "Driving Thru Life in the Fast Lane", "Medley: Never My Love / Never Can Say Goodbye", "Wanna Know My Mind", "Double Good Everything", "Be Kind To The Growing Mind (with The Temptations)", "Hold on to Your Love", "What's Too Much", "Daylight & Darkness", "No\u00ebl", "Noel", "Quiet Storm (single version)", "Wishful Thinking", "Rack Me Back", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Speak Low", "I'll Keep My Light In My Window", "Jingle Bells", "Be Kind to the Growing Mind", "I Can't Give You Anything but Love", "Whatcha Gonna Do", "Winter Wonderland", "Yester Love", "I've Got You Under My Skin", "Train of Thought", "You Don't Know What It's Like", "A Silent Partner in a Three-Way Love Affair", "If You Wanna Make Love (Come 'round Here)", "Melody Man", "Will You Love Me Tomorrow?", "Love So Fine", "Everything You Touch", "Mickey's Monkey", "Baby That's Backatcha", "As You Do", "Keep Me", "You Really Got a Hold on Me", "I've Made Love to You a Thousand Times", "Photograph in My Mind", "Take Me Through The Night", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Be Careful What You Wish For", "The Tracks Of My Tears", "Quiet Storm (Groove Boutique remix)", "More Than You Know", "Jasmin", "Quiet Storm (Groove Boutique Chill Jazz mix)", "It's Fantastic", "Time After Time", "Just My Soul Responding", "Love Don' Give No Reason (12 Inch Club Mix)", "We\u2019ve Come Too Far to End It Now", "Just Another Kiss", "Little Girl, Little Girl", "I Am I Am", "Save Me", "Share It", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "You Go to My Head", "You Take Me Away", "There Will Come A Day ( I'm Gonna Happen To You )", "I Praise & Worship You Father", "Cruisin'", "I Have Prayed On It", "When Smokey Sings Tears Of A Clown", "(It's The) Same Old Love", "Pops, We Love You (disco)", "Yes It's You Lady", "Vitamin U", "Tracks of my Tears", "Why", "Will You Love Me Tomorrow", "And I Don't Love You (Larry Levan instrumental dub)", "It's A Good Night", "The Family Song", "If You Can Want", "Baby Come Close", "My Guy", "The Agony And The Ecstasy", "Fulfill Your Need", "Close Encounters of the First Kind", "Te Quiero Como Si No Hubiera Un Manana", "One Time", "The Tracks of My Heart", "Rewind", "I Love Your Face", "Please Don't Take Your Love (feat. Carlos Santana)", "Christmas Every Day", "The Road to Damascus", "Love' n Life", "A Tattoo", "Tracks Of My Tears (Live)", "Get Ready", "Blame It on Love", "Ever Had A Dream", "Ebony Eyes (Duet with Rick James)", "Christmas Greeting", "Just to See Her", "Bad Girl", "Just a Touch Away", "More Love", "Love Is The Light", "Crusin'", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "I Can\u2019t Stand to See You Cry (Commercial version)", "Come to Me Soon", "Girlfriend", "Just Passing Through", "Ebony Eyes", "The Track of My Tears", "Tracks of My Tears", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Let Me Be the Clock", "God Rest Ye Merry Gentlemen", "Away in the Manger / Coventry Carol", "The Love Between Me and My Kids", "Tears of a Clown", "Can't Fight Love", "Gone Forever"], "ans_acc": 0.0078125, "ans_hit": 1, "ans_f1": 0.01545253863134658, "ans_precission": 0.7, "ans_recall": 0.0078125, "path_f1": 0.02443991853360489, "path_precision": 0.3, "path_recall": 0.012738853503184714, "path_ans_f1": 0.01545253863134658, "path_ans_precision": 0.7, "path_ans_recall": 0.0078125}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy\n# Answer:\nJohn F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia\n# Answer:\nSerbia", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> language.language_dialect.language -> Serbo-Croatian Language\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Balkan dialects belic 1914\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak dialects map\n# Answer:\nTorlakian dialect"], "ground_truth": ["Serbian language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}

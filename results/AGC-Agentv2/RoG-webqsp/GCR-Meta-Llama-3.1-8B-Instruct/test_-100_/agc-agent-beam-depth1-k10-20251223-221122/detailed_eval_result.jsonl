{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Hurricane Dean\n# Answer:\nHurricane Dean", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane\n# Answer:\n1900 Galveston hurricane"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8\n# Answer:\nm.04j5sk8", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer\n# Answer:\nLawyer"], "ground_truth": ["Speaker of the United States House of Representatives", "United States Representative", "Governor of Tennessee"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_3vpv\n# Answer:\ng.1245_3vpv", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_4dk3\n# Answer:\ng.1245_4dk3"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2\n# Answer:\nm.03ljbs2", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy\n# Answer:\nm.041w0vy", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn\n# Answer:\nm.0c04kpn", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0y54_x4\n# Answer:\nm.0y54_x4", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr\n# Answer:\nm.04qcbdr", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58\n# Answer:\nm.02kbc58", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc5c\n# Answer:\nm.02kbc5c", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> (I Know I Got) Skillz\n# Answer:\n(I Know I Got) Skillz", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> (I Know I Got) Skillz (feat. Def Jef)\n# Answer:\n(I Know I Got) Skillz (feat. Def Jef)"], "ground_truth": ["LSU Tigers men's basketball", "Orlando Magic", "Los Angeles Lakers", "Boston Celtics", "Phoenix Suns", "Cleveland Cavaliers", "Miami Heat"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0cqc3ql\n# Answer:\nm.0cqc3ql", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk\n# Answer:\nm.0n2qyfk"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\ng.12tb6gh2z", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\ng.1hhc37pvk"], "ground_truth": ["Malay, Pattani Language", "Mon Language", "Lao Language", "Phu Thai language", "Akha Language", "Cham language", "Khmer language", "Vietnamese Language", "Thai Language", "Hmong language", "Saek language", "Mlabri Language", "Nyaw Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5714285714285714, "ans_recall": 0.3076923076923077, "path_f1": 0.3, "path_precision": 0.42857142857142855, "path_recall": 0.23076923076923078, "path_ans_f1": 0.4, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07\n# Answer:\nm.0_rfq07", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkghb\n# Answer:\nm.0fpkghb", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0dlskcl\n# Answer:\nm.0dlskcl", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpnkz_\n# Answer:\nm.0fpnkz_", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkgmk\n# Answer:\nm.0fpkgmk", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkgn_\n# Answer:\nm.0fpkgn_"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson"], "ground_truth": ["Statesman", "Teacher", "Archaeologist", "Writer", "Philosopher", "Inventor", "Farmer", "Architect", "Lawyer", "Author"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.5714285714285714, "ans_recall": 0.4, "path_f1": 0.3529411764705882, "path_precision": 0.42857142857142855, "path_recall": 0.3, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Celebrating Evolution the Web Way\n# Answer:\nCelebrating Evolution the Web Way", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Charles Darwin in Cyberspace\n# Answer:\nCharles Darwin in Cyberspace"], "ground_truth": ["Les moyens d'expression chez les animaux", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Formation of Vegetable Mould through the Action of Worms", "ontstaan der soorten door natuurlijke teeltkeus", "Voyage Of The Beagle", "Leben und Briefe von Charles Darwin", "El Origin De Las Especies", "Die fundamente zur entstehung der arten", "The Correspondence of Charles Darwin, Volume 1", "The Darwin Reader Second Edition", "The Variation of Animals and Plants under Domestication", "Opsht\u0323amung fun menshen", "The origin of species", "The Origin of Species (World's Classics)", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Voyage of the Beagle (Great Minds Series)", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 8", "The Autobiography of Charles Darwin, and selected letters", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Correspondence of Charles Darwin, Volume 14", "red notebook of Charles Darwin", "Charles Darwin's marginalia", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 17: 1869", "genese\u014ds t\u014dn eid\u014dn", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 7", "The Correspondence of Charles Darwin, Volume 2", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Origin of Species (Oxford World's Classics)", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The foundations of the Origin of species", "Les mouvements et les habitudes des plantes grimpantes", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Autobiography Of Charles Darwin", "On the tendency of species to form varieties", "The Autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "Del Plata a Tierra del Fuego", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 10", "Notes on the fertilization of orchids", "Part I: Contributions to the Theory of Natural Selection / Part II", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 11", "The autobiography of Charles Darwin", "Questions about the breeding of animals", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "On evolution", "The Structure And Distribution of Coral Reefs", "The Expression Of The Emotions In Man And Animals", "Darwin's notebooks on transmutation of species", "Origin of Species", "Charles Darwin's natural selection", "The principal works", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The Correspondence of Charles Darwin, Volume 16: 1868", "Het uitdrukken van emoties bij mens en dier", "Voyage of the Beagle (NG Adventure Classics)", "A Darwin Selection", "The Structure and Distribution of Coral Reefs", "The Voyage of the Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "From So Simple a Beginning", "La vie et la correspondance de Charles Darwin", "The Origin of Species (Mentor)", "The living thoughts of Darwin", "The Origin of Species (Great Minds Series)", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Voyage d'un naturaliste autour du monde", "Origins", "The Orgin of Species", "Beagle letters", "Volcanic Islands", "The Autobiography of Charles Darwin (Large Print)", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The voyage of Charles Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The voyage of the Beagle.", "The descent of man, and selection in relation to sex.", "Darwin on humus and the earthworm", "Darwin and Henslow", "The Life of Erasmus Darwin", "Cartas de Darwin 18251859", "Charles Darwin", "Memorias y epistolario i\u0301ntimo", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 3", "Gesammelte kleinere Schriften", "The Darwin Reader First Edition", "Diario del Viaje de Un Naturalista Alrededor", "Rejse om jorden", "The structure and distribution of coral reefs", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Correspondence of Charles Darwin, Volume 15: 1867", "The education of Darwin", "The Origin of Species", "Origin of Species (Everyman's University Paperbacks)", "The Expression of the Emotions in Man and Animals", "Darwin for Today", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "To the members of the Down Friendly Club", "Darwinism stated by Darwin himself", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Kleinere geologische Abhandlungen", "La facult\u00e9 motrice dans les plantes", "A student's introduction to Charles Darwin", "The collected papers of Charles Darwin", "Darwin's Ornithological notes", "Voyage of the Beagle (Dover Value Editions)", "The Origin of Species (Collector's Library)", "Charles Darwin on the routes of male humble bees", "The Origin of Species (Enriched Classics)", "The Origin of Species (Variorum Reprint)", "On the origin of species by means of natural selection", "vari\u00eberen der huisdieren en cultuurplanten", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin's insects", "More Letters of Charles Darwin", "From Darwin's unpublished notebooks", "Geological Observations on South America", "Proiskhozhdenie vidov", "Resa kring jorden", "Fertilisation of Orchids", "On a remarkable bar of sandstone off Pernambuco", "The Voyage of the Beagle (Unabridged Classics)", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Voyage of the Beagle", "The Expression of the Emotions in Man And Animals", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 13", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Correspondence of Charles Darwin, Volume 15", "H.M.S. Beagle in South America", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The action of carbonate of ammonia on the roots of certain plants", "The descent of man and selection in relation to sex.", "The expression of the emotions in man and animals", "The Correspondence of Charles Darwin, Volume 5", "monograph on the sub-class Cirripedia", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The structure and distribution of coral reefs.", "The Correspondence of Charles Darwin, Volume 12", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The expression of the emotions in man and animals.", "Evolution", "The Essential Darwin", "The Autobiography of Charles Darwin [EasyRead Edition]", "Autobiography of Charles Darwin", "The Descent of Man and Selection in Relation to Sex", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Evolution by natural selection", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The\u0301orie de l'e\u0301volution", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Autobiography of Charles Darwin (Dodo Press)", "Darwin Darwin", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Works", "Human nature, Darwin's view", "Tesakneri tsagume\u030c", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 12: 1864", "The autobiography of Charles Darwin, 1809-1882", "The Voyage of the Beagle (Adventure Classics)", "Evolution and natural selection", "Darwin's journal", "The Different Forms of Flowers on Plants of the Same Species", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 9", "Origin of Species (Harvard Classics, Part 11)", "On the Movements and Habits of Climbing Plants", "Die geschlechtliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Metaphysics, Materialism, & the evolution of mind", "Voyage of the Beagle (Harvard Classics, Part 29)", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Origin of Species (Great Books : Learning Channel)", "From so simple a beginning", "The descent of man, and selection in relation to sex", "On Natural Selection", "The Correspondence of Charles Darwin, Volume 6", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Voyage of the Beagle (Everyman Paperbacks)", "Diary of the voyage of H.M.S. Beagle", "Darwin Compendium", "The Origin Of Species", "The Voyage of the Beagle (Mentor)", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 10: 1862", "Notebooks on transmutation of species", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Autobiography of Charles Darwin (Great Minds Series)", "Darwin", "Charles Darwin's letters", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 4", "Darwin en Patagonia", "The geology of the voyage of H.M.S. Beagle", "The origin of species : complete and fully illustrated"], "ans_acc": 0.018691588785046728, "ans_hit": 1, "ans_f1": 0.027366020524515394, "ans_precission": 0.5714285714285714, "ans_recall": 0.014018691588785047, "path_f1": 0.06666666666666667, "path_precision": 1.0, "path_recall": 0.034482758620689655, "path_ans_f1": 0.03669724770642201, "path_ans_precision": 1.0, "path_ans_recall": 0.018691588785046728}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w\n# Answer:\nm.07mmh5w", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773\n# Answer:\nm.0791773", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07sgy3b\n# Answer:\nm.07sgy3b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender"], "ground_truth": ["Denver Broncos"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Serbia\n# Answer:\nSerbia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Brebeneskul\n# Answer:\nBrebeneskul", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103\n# Answer:\nBucura Dumbrav\u0103"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Bob Dylan\n# Answer:\nBob Dylan", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Charlotte Bront\u00eb\n# Answer:\nCharlotte Bront\u00eb"], "ground_truth": ["Writer", "Bard", "Poet", "Author"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.42857142857142855, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.42857142857142855, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard\n# Answer:\nThe Changing of the Guard", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Dangerous Games\n# Answer:\nThe Dangerous Games"], "ground_truth": ["Hayden Christensen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk\n# Answer:\nm.0115qhzk"], "ground_truth": ["Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl\n# Answer:\nm.043wphl", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphv\n# Answer:\nm.043wphv", "# Reasoning Path:\nGulf War -> time.event.locations -> Israel\n# Answer:\nIsrael", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpj2\n# Answer:\nm.043wpj2", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula\n# Answer:\nArabian Peninsula"], "ground_truth": ["United Kingdom", "Argentina", "Saudi Arabia", "Australia", "Iraq", "United States of America", "France"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.125, "ans_precission": 0.1111111111111111, "ans_recall": 0.14285714285714285, "path_f1": 0.030303030303030304, "path_precision": 0.1111111111111111, "path_recall": 0.017543859649122806, "path_ans_f1": 0.125, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs\n# Answer:\nm.0sgkyvs", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86\n# Answer:\nm.0sgkz86", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol\n# Answer:\nA London Carol", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> Ala-ka-scram!\n# Answer:\nAla-ka-scram!", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> Any Given Fantasy\n# Answer:\nAny Given Fantasy"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Ethel\n# Answer:\nHurricane Ethel", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Frances\n# Answer:\nHurricane Frances"], "ground_truth": ["Return J. Meigs, Jr.", "Ted Strickland", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f\n# Answer:\nm.0_z851f", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0bvv9g7\n# Answer:\nm.0bvv9g7", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0bvvp8v\n# Answer:\nm.0bvvp8v"], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> book.author.works_written -> Coronado's letter to Mendoza, August 3, 1540\n# Answer:\nCoronado's letter to Mendoza, August 3, 1540", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site\n# Answer:\nCoronado Historic Site", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado National Forest\n# Answer:\nCoronado National Forest"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.crime.crime_victim.crime -> Assassination of John F. Kennedy\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2\n# Answer:\nFukushima I \u2013 2"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7999999999999999, "ans_precission": 0.8571428571428571, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.42857142857142855, "path_recall": 0.75, "path_ans_f1": 0.7999999999999999, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.75}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__5\n# Answer:\nm.0wg8__5", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8___\n# Answer:\nm.0wg8___"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently.\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> By faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity.\n# Answer:\nBy faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity.", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> Charity is no substitute for justice withheld.\n# Answer:\nCharity is no substitute for justice withheld."], "ground_truth": ["Writer", "Physician", "Philosopher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0\n# Answer:\nm.05cqdz0", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> (When You Gonna) Give It Up To Me\n# Answer:\n(When You Gonna) Give It Up To Me", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> Bad Bad Bad\n# Answer:\nBad Bad Bad", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05csddw\n# Answer:\nm.05csddw", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.0_srnrh\n# Answer:\nm.0_srnrh"], "ground_truth": ["Leon Cole", "Sal Gibson", "Francine Lons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\ng.1hhc3f_h6"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Academic Middle School\n# Answer:\nDr. Martin Luther King, Jr. Academic Middle School", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library\n# Answer:\nDr. Martin Luther King, Jr. Library", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2\n# Answer:\nm.0_714v2", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_\n# Answer:\nm.0gbz10_"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore\n# Answer:\nAlan Moore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.\n# Answer:\nAfter reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41\n# Answer:\nm.03ldb41", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.0542hwn\n# Answer:\nm.0542hwn", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09w0bqz\n# Answer:\nm.09w0bqz", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Atlanta Regional Airport\n# Answer:\nAtlanta Regional Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Cobb County Airport\n# Answer:\nCobb County Airport"], "ground_truth": ["Georgia State Capitol", "Turner Field", "Fernbank Science Center", "Atlanta Marriott Marquis", "Fernbank Museum of Natural History", "Atlanta History Center", "Georgia Aquarium", "The Tabernacle", "Georgia Dome", "Atlanta Ballet", "Fox Theatre", "Margaret Mitchell House & Museum", "Four Seasons Hotel Atlanta", "Martin Luther King, Jr. National Historic Site", "CNN Center", "Omni Coliseum", "Six Flags Over Georgia", "Jimmy Carter Library and Museum", "Zoo Atlanta", "Peachtree Road Race", "Cobb Energy Performing Arts Centre", "Woodruff Arts Center", "Center for Puppetry Arts", "Atlanta Symphony Orchestra", "Variety Playhouse", "Georgia World Congress Center", "World of Coca-Cola", "Masquerade", "Underground Atlanta", "Six Flags White Water", "Hyatt Regency Atlanta", "Atlanta Jewish Film Festival", "Centennial Olympic Park", "Philips Arena", "Arbor Place Mall", "Atlanta Cyclorama & Civil War Museum"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.13333333333333333, "ans_precission": 0.3333333333333333, "ans_recall": 0.08333333333333333, "path_f1": 0.13333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.08333333333333333, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h\n# Answer:\nm.0cr301h"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy\n# Answer:\nComedy", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy-drama\n# Answer:\nComedy-drama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010\n# Answer:\nThursday 10th June 2010"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010pyd6z\n# Answer:\nm.010pyd6z", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010q5kls\n# Answer:\nm.010q5kls", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010qb6kd\n# Answer:\nm.010qb6kd"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gl\n# Answer:\nm.04lt3gl", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gr\n# Answer:\nm.04lt3gr"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr.\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton\n# Answer:\nJackie Newton"], "ground_truth": ["Carolina Panthers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer\n# Answer:\nAlfred G. Mayer", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bazabeel Norman\n# Answer:\nBazabeel Norman", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bert Myers\n# Answer:\nBert Myers"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9\n# Answer:\nm.0lwxmy9", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor (to Kill a Mockingbird)\n# Answer:\nMatar Un Ruisenor (to Kill a Mockingbird)"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010l3l5d\n# Answer:\nm.010l3l5d", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.04kcmn6\n# Answer:\nm.04kcmn6"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx\n# Answer:\nm.079q3lx", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv\n# Answer:\nm.010l29pv", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.0115sdhb\n# Answer:\nm.0115sdhb"], "ground_truth": ["Ralph Nader", "Michael Peroutka", "John Kerry", "Gene Amondson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7\n# Answer:\nm.0h4nmc7", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> Civilization: The West and the Rest\n# Answer:\nCivilization: The West and the Rest", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f\n# Answer:\nm.0h4nq4f", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn\n# Answer:\nm.0pdthbn", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> Coloso/ Colossus\n# Answer:\nColoso/ Colossus", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> Colossus: The Rise and Fall of the American Empire\n# Answer:\nColossus: The Rise and Fall of the American Empire"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Floreana Island\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Floreana Island\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Gal\u00e1pagos National Park\n# Answer:\nGal\u00e1pagos National Park", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Bartolom\u00e9 Island\n# Answer:\nBartolom\u00e9 Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Baltra Island\n# Answer:\nBaltra Island"], "ground_truth": ["Ecuador", "Pacific Ocean", "Gal\u00e1pagos Province"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk\n# Answer:\nm.0115qhzk", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber: Never Say Never\n# Answer:\nJustin Bieber: Never Say Never"], "ground_truth": ["All That Matters", "Beauty And A Beat", "Hold Tight", "Thought Of You", "#thatPower", "All Bad", "Bigger", "Boyfriend", "Turn to You (Mother's Day Dedication)", "Home to Mama", "Never Say Never", "Bad Day", "Change Me", "As Long as You Love Me", "Recovery", "Heartbreaker", "PYD", "Pray", "Somebody to Love", "Live My Life", "Confident", "Baby", "Die in Your Arms", "Never Let You Go", "Right Here", "Lolly", "Eenie Meenie", "First Dance", "Roller Coaster", "All Around The World", "Wait for a Minute"], "ans_acc": 0.03225806451612903, "ans_hit": 1, "ans_f1": 0.05405405405405405, "ans_precission": 0.16666666666666666, "ans_recall": 0.03225806451612903, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.05405405405405405, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 0.03225806451612903}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau"], "ground_truth": ["Publisher", "Statesman", "Journalist", "Writer", "Physician"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz\n# Answer:\nm.04st6lz", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s\n# Answer:\nm.09w1l1s", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w2hwc\n# Answer:\nm.09w2hwc", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403hm\n# Answer:\nm.04403hm", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403wb\n# Answer:\nm.04403wb"], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv\n# Answer:\nm.010_ydxv", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw\n# Answer:\nm.064_ltw"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w\n# Answer:\nm.0cs3c3w", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd\n# Answer:\nm.0948qtd", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094cr65\n# Answer:\nm.094cr65", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs8ydd\n# Answer:\nm.0cs8ydd", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0djcy_v\n# Answer:\nm.0djcy_v"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Allah\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> God\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat\n# Answer:\nDay of Arafat", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura\n# Answer:\nDay of Ashura"], "ground_truth": ["Islamic holy books", "Islamic view of angels", "Entering Heaven alive", "Mahdi", "\u1e6c\u016bb\u0101", "Sharia", "Qiyamah", "Monotheism", "Tawhid", "Predestination in Islam", "God in Islam", "Masih ad-Dajjal", "Prophets in Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion.\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> \u00c9mile Zola\n# Answer:\n\u00c9mile Zola", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A liberal is a power worshipper without the power.\n# Answer:\nA liberal is a power worshipper without the power.", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A tragic situation exists precisely when virtue does not triumph but when it is still felt that man is nobler than the forces which destroy him.\n# Answer:\nA tragic situation exists precisely when virtue does not triumph but when it is still felt that man is nobler than the forces which destroy him.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley\n# Answer:\nAldous Huxley", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler\n# Answer:\nArthur Koestler"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler"], "ground_truth": ["Nazi Germany"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\ng.11b7_lvdf2", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt\n# Answer:\nm.03lpqjt", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t\n# Answer:\nm.09w699t", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> m.01362dph\n# Answer:\nm.01362dph", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> m.0drf0x3\n# Answer:\nm.0drf0x3"], "ground_truth": ["Songwriter", "Actor", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2001 Protection One 400\n# Answer:\n2001 Protection One 400", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66101\n# Answer:\n66101", "# Reasoning Path:\nKansas Speedway -> base.nascar.nascar_venue.nascar_races_held_here -> Hollywood Casino 400\n# Answer:\nHollywood Casino 400", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2002 Protection One 400\n# Answer:\n2002 Protection One 400"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player\n# Answer:\nBaseball Player", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Film character\n# Answer:\nFilm character"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln\n# Answer:\nm.07919ln", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4\n# Answer:\nm.07mmjx4", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07sg_z8\n# Answer:\nm.07sg_z8"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> This I remember\n# Answer:\nThis I remember", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr.\n# Answer:\nFranklin D. Roosevelt, Jr."], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\ng.1245_67l9", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\ng.1hhc378kt", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\ng.1hhc38d0l"], "ground_truth": ["Catholicism", "Islam", "Protestantism", "Hinduism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_\n# Answer:\nm.03pn4x_", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss\n# Answer:\nm.04hdfss"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846)\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet\n# Answer:\nAlphonse Daudet", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Anne Rice\n# Answer:\nAnne Rice", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> Ada Lovelace asked to see him when she was dying in 1847.\n# Answer:\nAda Lovelace asked to see him when she was dying in 1847.", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> After 80 readings, his health was clearly affected, but he kept up the pace.\n# Answer:\nAfter 80 readings, his health was clearly affected, but he kept up the pace."], "ground_truth": ["Dombey and son", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Classic Fiction)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Everyman's Library Children's Classics)", "Our mutual friend.", "The cricket on the hearth", "Great expectations", "A Tale of Two Cities (Cyber Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Clear Print)", "The old curiosity shop.", "Great Expectations", "Bleak House", "The Pickwick papers", "A Christmas Carol (R)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (Puffin Choice)", "Our mutual friend", "A Tale of Two Cities (Cover to Cover Classics)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "Sketches by Boz", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "The old curiosity shop", "A Tale of Two Cities (Collected Works of Charles Dickens)", "Bleak house", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol (Penguin Readers, Level 2)", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Christmas Carol (Take Part)", "David Copperfield", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Progressive English)", "Little Dorrit", "A Christmas Carol (Scholastic Classics)", "The life and adventures of Nicholas Nickleby", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Radio Theatre)", "The Mystery of Edwin Drood", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Piccolo Books)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Paperback Classics)", "David Copperfield.", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Cassette (1 Hr).)", "The Old Curiosity Shop", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol. (Lernmaterialien)", "A Christmas Carol (Illustrated Classics)", "A CHRISTMAS CAROL", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Tor Classics)", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Soundings)", "Oliver Twist", "The mystery of Edwin Drood", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "Martin Chuzzlewit", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "The Pickwick Papers", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Nelson Graded Readers)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Watermill Classic)", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Family Classics)", "Hard times", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (10 Cassettes)", "Dombey and Son", "Great Expectations.", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol", "Bleak House.", "A Christmas Carol (Apple Classics)", "A Christmas Carol (Great Stories)", "Great expectations.", "A Christmas Carol (Whole Story)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Adopted Classic)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Penguin Student Editions)", "Dombey and Son.", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Tor Classics)"], "ans_acc": 0.005917159763313609, "ans_hit": 1, "ans_f1": 0.011235955056179775, "ans_precission": 0.1111111111111111, "ans_recall": 0.005917159763313609, "path_f1": 0.07317073170731707, "path_precision": 0.1111111111111111, "path_recall": 0.05454545454545454, "path_ans_f1": 0.011235955056179775, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 0.005917159763313609}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf\n# Answer:\nm.049y3kf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya\n# Answer:\n1940\u201344 insurgency in Chechnya", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings\n# Answer:\n1977 Moscow bombings", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1991 Soviet coup d'\u00e9tat attempt\n# Answer:\n1991 Soviet coup d'\u00e9tat attempt"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp.\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions\n# Answer:\nFrontpoint Security Solutions", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Hair Club\n# Answer:\nHair Club"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> A Scottish Soldier\n# Answer:\nA Scottish Soldier", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne\n# Answer:\nAul Lang Syne", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Ballard of Glencoe\n# Answer:\nBallard of Glencoe"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk\n# Answer:\nm.03lj4lk", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1\n# Answer:\nKnight Rider - Season 1", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0m_1\n# Answer:\nm.09w0m_1", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0t0g\n# Answer:\nm.09w0t0g"], "ground_truth": ["William Daniels"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> location.location.contains -> Alexander Smith House\n# Answer:\nAlexander Smith House", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\ng.11x1ddsd6", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1fr8dg\n# Answer:\ng.11x1fr8dg", "# Reasoning Path:\nBrentwood -> location.location.contains -> Andrew Crockett House\n# Answer:\nAndrew Crockett House", "# Reasoning Path:\nBrentwood -> location.location.contains -> Constantine Sneed House\n# Answer:\nConstantine Sneed House"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist\n# Answer:\nBotanist", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy\n# Answer:\nCarver Academy", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Chemist\n# Answer:\nChemist", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Inventor\n# Answer:\nInventor"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg\n# Answer:\nm.07nvttg", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt\n# Answer:\nm.07nvtvt", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtwp\n# Answer:\nm.07nvtwp"], "ground_truth": ["Tracy Pollan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd\n# Answer:\nm.04fv9nd", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam\n# Answer:\nBattle of Antietam", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry"], "ground_truth": ["Battle of Harpers Ferry", "Battle of Chantilly", "First Battle of Kernstown", "Battle of Hancock", "Second Battle of Bull Run", "How Few Remain", "Romney Expedition", "Battle of Port Republic", "First Battle of Winchester", "Manassas Station Operations", "Battle of Hoke's Run", "Battle of Front Royal", "Battle of Chancellorsville", "Jackson's Valley Campaign", "First Battle of Rappahannock Station", "Battle of White Oak Swamp", "American Civil War", "Battle of McDowell", "Battle of Cedar Mountain"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.22222222222222218, "ans_precission": 0.375, "ans_recall": 0.15789473684210525, "path_f1": 0.11428571428571428, "path_precision": 0.25, "path_recall": 0.07407407407407407, "path_ans_f1": 0.22222222222222218, "path_ans_precision": 0.375, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5\n# Answer:\nm.012zbkk5"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006\n# Answer:\nPatrick Swayze 2006", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2\n# Answer:\nSwayze2"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bob Kane\n# Answer:\nBob Kane"], "ground_truth": ["Leda and the Swan", "Vitruvian Man", "g.121yh91r", "Portrait of Isabella d'Este", "Sala delle Asse", "Madonna of the Yarnwinder", "g.12314dm1", "Lucan portrait of Leonardo da Vinci", "Bacchus", "Annunciation", "Drapery for a Seated Figure", "La belle ferronni\u00e8re", "g.1213jb_b", "Benois Madonna", "Leonardo's horse", "The Baptism of Christ", "g.120vt1gz", "g.12215rxg", "Horse and Rider", "g.121wt37c", "Virgin of the Rocks", "The Battle of Anghiari", "g.1224tf0c", "Madonna of the Carnation", "St. John the Baptist", "The Last Supper", "St. Jerome in the Wilderness", "Adoration of the Magi", "Head of a Woman", "Mona Lisa", "Madonna of Laroque", "Madonna and Child with St Joseph", "Portrait of a Musician", "The Virgin and Child with St. Anne", "The Virgin and Child with St Anne and St John the Baptist", "Portrait of a man in red chalk", "The Holy Infants Embracing", "g.1219sb0g", "Lady with an Ermine", "Portrait of a Young Fianc\u00e9e", "Medusa", "Salvator Mundi", "Madonna Litta", "g.1239jd9p", "Ginevra de' Benci"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1111111111111111, "ans_precission": 0.3333333333333333, "ans_recall": 0.06666666666666667, "path_f1": 0.11320754716981131, "path_precision": 0.3333333333333333, "path_recall": 0.06818181818181818, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia\n# Answer:\nCarinthia"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Discursos completos\n# Answer:\nDiscursos completos", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Essays\n# Answer:\nEssays", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Eva Per\u00f3n habla a las mujeres\n# Answer:\nEva Per\u00f3n habla a las mujeres"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism\n# Answer:\nBuddhism"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Bifocals", "Glass harmonica", "Lightning rod", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.5, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.location.contains -> Aims Community College\n# Answer:\nAims Community College", "# Reasoning Path:\nGreeley -> location.location.contains -> Bank of Colorado Arena\n# Answer:\nBank of Colorado Arena"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Charode\u01d0ka\n# Answer:\nCharode\u01d0ka", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Classic Bits & Pieces\n# Answer:\nClassic Bits & Pieces"], "ground_truth": ["Librettist", "Musician", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7\n# Answer:\nm.0102xvg7", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0103v73t\n# Answer:\nm.0103v73t", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0127rg0f\n# Answer:\nm.0127rg0f"], "ground_truth": ["Liechtenstein", "Switzerland", "Austria", "Luxembourg", "East Germany", "Belgium", "Germany"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.42857142857142855, "ans_recall": 0.42857142857142855, "path_f1": 0.2857142857142857, "path_precision": 0.2857142857142857, "path_recall": 0.2857142857142857, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> CRIK FM - The Lynx Classic Rock\n# Answer:\nCRIK FM - The Lynx Classic Rock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> CRIK FM - The Lynx Classic Hits\n# Answer:\nCRIK FM - The Lynx Classic Hits", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Clock\n# Answer:\nClock", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Erection\n# Answer:\nErection"], "ground_truth": ["Rock music", "Pop rock", "Soft rock", "Experimental music", "Art rock", "Experimental rock", "Blues rock", "Psychedelic rock", "Pop music"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.3333333333333333, "ans_recall": 0.3333333333333333, "path_f1": 0.3333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2010\n# Answer:\nUnited States Senate election in Colorado, 2010", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> Denver City Home Guard\n# Answer:\nDenver City Home Guard", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment\n# Answer:\n1st Colorado Infantry Regiment"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc38qmq\n# Answer:\ng.1hhc38qmq", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc390kr\n# Answer:\ng.1hhc390kr"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nSeattle -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights\n# Answer:\nArbor Heights", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Atlantic\n# Answer:\nAtlantic"], "ground_truth": ["98194", "98119", "98109", "98104", "98114", "98138", "98107", "98116", "98129", "98119-4114", "98184", "98126", "98118", "98132", "98113", "98158", "98199", "98170", "98134", "98185", "98141", "98101", "98165", "98171", "98133", "98122", "98131", "98174", "98117", "98103", "98121", "98102", "98124", "98195", "98188", "98127", "98155", "98166", "98191", "98108", "98139", "98168", "98164", "98175", "98111", "98198", "98154", "98161", "98115", "98181", "98112", "98148", "98144", "98146", "98190", "98145", "98125", "98178", "98177", "98160", "98106", "98136", "98105"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08450704225352113, "ans_precission": 0.375, "ans_recall": 0.047619047619047616, "path_f1": 0.08450704225352113, "path_precision": 0.375, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08450704225352113, "path_ans_precision": 0.375, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> common.topic.webpage -> m.0cq8blb\n# Answer:\nm.0cq8blb", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance music\n# Answer:\nDance music", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance-pop\n# Answer:\nDance-pop", "# Reasoning Path:\nWillow Smith -> common.topic.webpage -> m.0d_tv_c\n# Answer:\nm.0d_tv_c", "# Reasoning Path:\nWillow Smith -> common.topic.webpage -> m.0dff2s8\n# Answer:\nm.0dff2s8"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1\n# Answer:\nm.0104b7h1", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02t0c71\n# Answer:\nm.02t0c71", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02wn10k\n# Answer:\nm.02wn10k"], "ground_truth": ["Chinese characters", "Simplified Chinese character", "N\u00fcshu script", "Traditional Chinese characters", "'Phags-pa script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.42857142857142855, "ans_recall": 0.6, "path_f1": 0.5, "path_precision": 0.42857142857142855, "path_recall": 0.6, "path_ans_f1": 0.5, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_\n# Answer:\nm.0_0cs2_"], "ground_truth": ["Pat Nixon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx\n# Answer:\nm.0_yczwx", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1\n# Answer:\nThe Jeffersons - Season 1", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 10\n# Answer:\nThe Jeffersons - Season 10", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.07shryn\n# Answer:\nm.07shryn", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_tlrff\n# Answer:\nm.0_tlrff"], "ground_truth": ["Roxie Roker", "Zara Cully", "Mike Evans", "Damon Evans", "Franklin Cover", "Isabel Sanford", "Marla Gibbs", "Berlinda Tolbert", "Paul Benedict", "Jay Hammer", "Sherman Hemsley"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Dock of the Bay\n# Answer:\nDock of the Bay", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die\n# Answer:\n1st to Die", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 2nd Chance\n# Answer:\n2nd Chance", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94102\n# Answer:\n94102", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94102-4733\n# Answer:\n94102-4733"], "ground_truth": ["San Francisco Bay Times", "Sing Tao Daily", "Synapse", "Free Society", "San Francisco Foghorn", "San Francisco Bay View", "The San Francisco Examiner", "San Francisco Bay Guardian", "San Francisco Chronicle", "San Francisco News-Call Bulletin Newspaper", "The Golden Era", "San Francisco Call", "San Francisco Daily", "AsianWeek", "California Star", "The Daily Alta California", "Dock of the Bay", "Street Sheet", "Bay Area Reporter", "San Francisco Business Times"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.20689655172413793, "ans_precission": 0.3333333333333333, "ans_recall": 0.15, "path_f1": 0.20689655172413793, "path_precision": 0.3333333333333333, "path_recall": 0.15, "path_ans_f1": 0.20689655172413793, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales\n# Answer:\nRoman Catholic Archdiocese of Manizales"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Perfect Friend\n# Answer:\nPerfect Friend", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Be a Man\n# Answer:\nBe a Man", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Feel the Madness\n# Answer:\nFeel the Madness"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin\n# Answer:\nCharles Waring Darwin"], "ground_truth": ["Les moyens d'expression chez les animaux", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Formation of Vegetable Mould through the Action of Worms", "ontstaan der soorten door natuurlijke teeltkeus", "Leben und Briefe von Charles Darwin", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "El Origin De Las Especies", "Die fundamente zur entstehung der arten", "The Darwin Reader Second Edition", "The Variation of Animals and Plants under Domestication", "Opsht\u0323amung fun menshen", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Darwin-Wallace", "red notebook of Charles Darwin", "Charles Darwin's marginalia", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 17: 1869", "genese\u014ds t\u014dn eid\u014dn", "Reise um die Welt 1831 - 36", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The foundations of the Origin of species", "Les mouvements et les habitudes des plantes grimpantes", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "On the tendency of species to form varieties", "The Autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Notes on the fertilization of orchids", "Part I: Contributions to the Theory of Natural Selection / Part II", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Insectivorous Plants", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Questions about the breeding of animals", "On evolution", "Darwin's notebooks on transmutation of species", "Charles Darwin's natural selection", "The Life and Letters of Charles Darwin Volume 2", "The principal works", "The Correspondence of Charles Darwin, Volume 16: 1868", "Het uitdrukken van emoties bij mens en dier", "A Darwin Selection", "The Structure and Distribution of Coral Reefs", "The Voyage of the Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "La vie et la correspondance de Charles Darwin", "The living thoughts of Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Voyage d'un naturaliste autour du monde", "Origins", "The Orgin of Species", "Beagle letters", "Volcanic Islands", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The voyage of Charles Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin on humus and the earthworm", "Darwin and Henslow", "The Life of Erasmus Darwin", "Cartas de Darwin 18251859", "Charles Darwin", "Memorias y epistolario i\u0301ntimo", "The Correspondence of Charles Darwin, Volume 13: 1865", "Gesammelte kleinere Schriften", "The Darwin Reader First Edition", "Diario del Viaje de Un Naturalista Alrededor", "Rejse om jorden", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Correspondence of Charles Darwin, Volume 15: 1867", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The education of Darwin", "The Life and Letters of Charles Darwin Volume 1", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Darwin for Today", "The Expression of the Emotions in Man and Animals", "To the members of the Down Friendly Club", "Darwinism stated by Darwin himself", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Kleinere geologische Abhandlungen", "La facult\u00e9 motrice dans les plantes", "A student's introduction to Charles Darwin", "The collected papers of Charles Darwin", "Darwin's Ornithological notes", "Charles Darwin on the routes of male humble bees", "On the origin of species by means of natural selection", "vari\u00eberen der huisdieren en cultuurplanten", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin's insects", "More Letters of Charles Darwin", "From Darwin's unpublished notebooks", "Geological Observations on South America", "Proiskhozhdenie vidov", "Resa kring jorden", "Fertilisation of Orchids", "On a remarkable bar of sandstone off Pernambuco", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Evolutionary Writings: Including the Autobiographies", "Geological Observations on the Volcanic Islands", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 11: 1863", "H.M.S. Beagle in South America", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The action of carbonate of ammonia on the roots of certain plants", "monograph on the sub-class Cirripedia", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "South American Geology", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Evolution", "The Essential Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 9: 1861", "Evolution by natural selection", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The\u0301orie de l'e\u0301volution", "Darwin Darwin", "Works", "Human nature, Darwin's view", "Tesakneri tsagume\u030c", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 12: 1864", "Evolution and natural selection", "Darwin's journal", "Darwin from Insectivorous Plants to Worms", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Different Forms of Flowers on Plants of the Same Species", "On the Movements and Habits of Climbing Plants", "Die geschlechtliche Zuchtwahl", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Metaphysics, Materialism, & the evolution of mind", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "From so simple a beginning", "On Natural Selection", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Diary of the voyage of H.M.S. Beagle", "Darwin Compendium", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 10: 1862", "Notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Darwin", "Charles Darwin's letters", "The portable Darwin", "Darwin en Patagonia", "The geology of the voyage of H.M.S. Beagle"], "ans_acc": 0.0196078431372549, "ans_hit": 1, "ans_f1": 0.025559105431309907, "ans_precission": 0.5714285714285714, "ans_recall": 0.013071895424836602, "path_f1": 0.07142857142857142, "path_precision": 1.0, "path_recall": 0.037037037037037035, "path_ans_f1": 0.038461538461538464, "path_ans_precision": 1.0, "path_ans_recall": 0.0196078431372549}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5\n# Answer:\nm.09nsgl5", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_\n# Answer:\nm.0_0cs2_", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb\n# Answer:\nm.09nsglb", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh\n# Answer:\nm.09nsglh"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Come Spy with Me\n# Answer:\nCome Spy with Me"], "ground_truth": ["Just Passing Through", "Tears Of A Clown", "We Are The Warriors", "It's a Good Feeling", "Tracks of my Tears", "Tell Me Tomorrow, Part 1", "You Are Forever", "Fallin'", "Save Me", "Ever Had A Dream", "Melody Man", "Just a Touch Away", "Season's Greetings from Smokey Robinson", "Heavy On Pride (Light On Love)", "Just My Soul Responding", "Some People Will Do Anything for Love", "Tell Me Tomorrow", "Let Me Be The Clock", "Going to a Go Go", "Blame It On Love (Duet with Barbara Mitchell)", "Love Don't Give No Reason", "I'm in the Mood for Love", "Wedding Song", "Will You Love Me Tomorrow?", "The Agony and the Ecstasy", "Pops, We Love You (disco)", "Same Old Love", "The Tracks of My Heart", "And I Don't Love You (Larry Levan instrumental dub)", "Nearness of You", "Share It", "I Know You by Heart", "Come by Here (Kum Ba Ya)", "Mickey's Monkey", "God Rest Ye Merry Gentlemen", "Just Like You", "Tracks Of My Tears (Live)", "There Will Come a Day (I'm Gonna Happen to You)", "Jingle Bells", "Be Careful What You Wish For", "I've Got You Under My Skin", "Can't Fight Love", "I've Made Love to You a Thousand Times", "Unless You Do It Again", "It's A Good Night", "Theme From the Big Time", "More Love", "Quiet Storm (single version)", "I Love The Nearness Of You", "Rewind", "Te Quiero Como Si No Hubiera Un Manana", "Time Flies", "Sweet Harmony", "Sleepless Nights", "The Track of My Tears", "The Tracks of My Tears (live)", "Food For Thought", "Get Ready", "Santa Claus is Coming to Town", "Open", "Skid Row", "Girl I'm Standing There", "Standing On Jesus", "Bad Girl", "My World", "Yester Love", "I Hear The Children Singing", "I Second That Emotion", "She's Only a Baby Herself", "Girlfriend", "I Want You Back", "Just Another Kiss", "You Made Me Feel Love", "Quiet Storm (Groove Boutique Chill Jazz mix)", "My Girl", "Away in the Manger / Coventry Carol", "I Like Your Face", "Jesus Told Me To Love You", "The Family Song", "You Are So Beautiful (feat. Dave Koz)", "Let Me Be the Clock", "Be Kind to the Growing Mind", "Christmas Everyday", "Crusin'", "Coincidentally", "Medley: Never My Love / Never Can Say Goodbye", "Never My Love / Never Can Say Goodbye", "Please Don't Take Your Love (feat. Carlos Santana)", "You Go to My Head", "Gang Bangin'", "Virgin Man", "Holly", "Our Love Is Here to Stay", "Just to See Her", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Love' n Life", "Don't Know Why", "You're Just My Life (feat. India.Arie)", "I Can\u2019t Stand to See You Cry (Commercial version)", "Speak Low", "The Love Between Me and My Kids", "Baby Come Close", "The Hurt's On You", "Love Is The Light", "Aqui Con Tigo (Being With You)", "Gone Forever", "When A Woman Cries", "Come to Me Soon", "No\u00ebl", "Driving Thru Life in the Fast Lane", "Happy (Love Theme From Lady Sings the Blues)", "Going to a Go-Go", "Going to a Gogo", "Rack Me Back", "Ooh Baby Baby", "I'll Keep My Light In My Window", "I Can't Give You Anything but Love", "Being With You", "Who's Sad", "Will You Love Me Tomorrow", "Let Your Light Shine On Me", "What's Too Much", "One Time", "I Care About Detroit", "Wanna Know My Mind", "Quiet Storm", "Winter Wonderland", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "It's Fantastic", "The Road to Damascus", "Love So Fine", "Tears of a Sweet Free Clown", "Ebony Eyes", "My Guy", "Be Kind To The Growing Mind (with The Temptations)", "Just To See Her Again", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Wishful Thinking", "I Am I Am", "I'm Glad There Is You", "You're the One for Me (feat. Joss Stone)", "As You Do", "I Have Prayed On It", "There Will Come A Day ( I'm Gonna Happen To You )", "Ooo Baby Baby (live)", "I Can't Find", "I\u2019ve Got You Under My Skin", "Ain't That Peculiar", "If You Wanna Make Love (Come 'round Here)", "So Bad", "I Love Your Face", "I Praise & Worship You Father", "When Smokey Sings Tears Of A Clown", "It's Time to Stop Shoppin' Around", "Love Bath", "You've Really Got a Hold on Me", "I've Made Love To You A Thousand Times", "Tracks of My Tears", "A Child Is Waiting", "Jasmin", "Deck the Halls", "Yes It's You Lady", "Whatcha Gonna Do", "The Way You Do (The Things You Do)", "Hanging on by a Thread", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Be Who You Are", "Close Encounters of the First Kind", "You've Really Go a Hold on Me", "You Take Me Away", "Blame It on Love", "Pops, We Love You", "Night and Day", "Shop Around", "Love Letters", "Quiet Storm (Groove Boutique remix)", "In My Corner", "Ooo Baby Baby", "Did You Know (Berry's Theme)", "Noel", "You Don't Know What It's Like", "Time After Time", "No Time to Stop Believing", "Asleep on My Love", "With Your Love Came", "The Tracks Of My Tears", "Tell Me Tomorrow (12\\\" extended mix)", "And I Love Her", "We've Saved the Best for Last", "It's Her Turn to Live", "Cruisin'", "Christmas Greeting", "Hold on to Your Love", "The Tears Of A Clown", "Shoe Soul", "If You Wanna Make Love", "It's Christmas Time", "I Can't Get Enough", "(It's The) Same Old Love", "Don't Play Another Love Song", "Love Don' Give No Reason (12 Inch Club Mix)", "If You Want My Love", "Vitamin U", "Will You Still Love Me Tomorrow", "Keep Me", "Why Are You Running From My Love", "Walk on By", "Christmas Every Day", "Tears of a Clown", "Fulfill Your Need", "Ebony Eyes (Duet with Rick James)", "If You Can Want", "Fly Me to the Moon (In Other Words)", "One Heartbeat", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "The Christmas Song", "Tea for Two", "Really Gonna Miss You", "Mother's Son", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Baby That's Backatcha", "Why", "Everything You Touch", "More Than You Know", "The Tears of a Clown", "Please Come Home for Christmas", "Be Careful What You Wish For (instrumental)", "He Can Fix Anything", "Train of Thought", "We\u2019ve Come Too Far to End It Now", "A Silent Partner in a Three-Way Love Affair", "The Agony And The Ecstasy", "That Place", "Little Girl, Little Girl", "Crusin", "And I Don't Love You", "Don't Wanna Be Just Physical", "You Really Got a Hold on Me", "Double Good Everything", "Satisfy You", "Photograph in My Mind", "Little Girl Little Girl", "Take Me Through The Night", "I Am, I Am", "Easy", "Because of You It's the Best It's Ever Been", "Why Do Happy Memories Hurt So Bad", "A Tattoo", "Daylight & Darkness", "I Second That Emotions", "Love Brought Us Here", "Everything for Christmas", "Cruisin", "You Cannot Laugh Alone", "The Tracks of My Tears", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.02wj9fq\n# Answer:\nm.02wj9fq", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.03wytyc\n# Answer:\nm.03wytyc", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.046cspc\n# Answer:\nm.046cspc"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}

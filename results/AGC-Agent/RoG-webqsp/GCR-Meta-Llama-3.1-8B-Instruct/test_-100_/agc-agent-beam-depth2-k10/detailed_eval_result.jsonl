{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Atlantic Canada\n# Answer:\n1900 Galveston hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nm.04j5sk8", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04j5sk8"], "ground_truth": ["United States Representative", "Governor of Tennessee", "Speaker of the United States House of Representatives"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.region -> Americas\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2 -> tv.regular_tv_appearance.actor -> Angie Harmon\n# Answer:\nm.03ljbs2", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Entertainment Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pk4l\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective"], "ground_truth": ["Ilyssa Fradin", "Hannah Gunn", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.season -> 1994\u201395 NBA season\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nm.02kbc58", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["Orlando Magic", "Cleveland Cavaliers", "Boston Celtics", "Miami Heat", "Phoenix Suns", "LSU Tigers men's basketball", "Los Angeles Lakers"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.5, "path_recall": 0.16666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.5, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> Mulberry Street NYC c1900 LOC 3g04637u edit\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Chandler Muriel Bing\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nm.0n2qyfk"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.countries_spoken_in -> Laos\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Nyaw Language -> language.human_language.region -> Asia\n# Answer:\nNyaw Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.region -> Asia\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> common.topic.notable_types -> Human Language\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Cham language", "Lao Language", "Saek language", "Phu Thai language", "Vietnamese Language", "Malay, Pattani Language", "Mlabri Language", "Khmer language", "Akha Language", "Thai Language", "Nyaw Language", "Hmong language", "Mon Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.45859872611464964, "ans_precission": 0.9, "ans_recall": 0.3076923076923077, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.45859872611464964, "path_ans_precision": 0.9, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> film.film_subject.films -> Internet Rising\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Karen Kempner\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social networking service\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> andyforsberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social Network (Consumer)\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer -> common.topic.notable_types -> Profession\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Philosopher -> common.topic.notable_types -> Profession\n# Answer:\nPhilosopher", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Jyotirao Phule -> influence.influence_node.influenced_by -> Abraham Lincoln\n# Answer:\nJyotirao Phule", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Jyotirao Phule -> influence.influence_node.influenced_by -> Benjamin Franklin\n# Answer:\nJyotirao Phule", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Jyotirao Phule -> common.topic.notable_types -> Author\n# Answer:\nJyotirao Phule"], "ground_truth": ["Architect", "Philosopher", "Author", "Inventor", "Statesman", "Teacher", "Farmer", "Writer", "Archaeologist", "Lawyer"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.5, "ans_recall": 0.3, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.gender -> Female\n# Answer:\nAnne Darwin"], "ground_truth": ["Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Orgin of Species", "The Structure and Distribution of Coral Reefs", "Evolution by natural selection", "The Origin of Species (World's Classics)", "The Correspondence of Charles Darwin, Volume 10", "Darwin Darwin", "The Voyage of the Beagle (Adventure Classics)", "The autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 5", "Voyage of the Beagle (NG Adventure Classics)", "A Darwin Selection", "The Correspondence of Charles Darwin, Volume 8", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Origins", "The Origin of Species (Mentor)", "Questions about the breeding of animals", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Autobiography of Charles Darwin (Dodo Press)", "Les moyens d'expression chez les animaux", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The structure and distribution of coral reefs", "The education of Darwin", "The living thoughts of Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "On a remarkable bar of sandstone off Pernambuco", "The Variation of Animals and Plants under Domestication", "The Darwin Reader Second Edition", "Darwin's Ornithological notes", "The structure and distribution of coral reefs.", "The Correspondence of Charles Darwin, Volume 15", "Opsht\u0323amung fun menshen", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Darwinism stated by Darwin himself", "Evolution and natural selection", "Autobiography of Charles Darwin", "The expression of the emotions in man and animals.", "Darwin for Today", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The collected papers of Charles Darwin", "The Autobiography of Charles Darwin (Large Print)", "On the origin of species by means of natural selection", "More Letters of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "On the Movements and Habits of Climbing Plants", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Correspondence of Charles Darwin, Volume 1", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Darwin Reader First Edition", "The Correspondence of Charles Darwin, Volume 11", "The Voyage of the Beagle (Unabridged Classics)", "On Natural Selection", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Memorias y epistolario i\u0301ntimo", "Volcanic Islands", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 13", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Correspondence of Charles Darwin, Volume 4", "Del Plata a Tierra del Fuego", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 12", "Works", "Notebooks on transmutation of species", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Het uitdrukken van emoties bij mens en dier", "The Origin of Species (Enriched Classics)", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Origin of Species (Great Minds Series)", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 16: 1868", "Darwin's notebooks on transmutation of species", "Charles Darwin's letters", "vari\u00eberen der huisdieren en cultuurplanten", "Leben und Briefe von Charles Darwin", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Essential Darwin", "The voyage of Charles Darwin", "The Origin Of Species", "The Correspondence of Charles Darwin, Volume 10: 1862", "The origin of species", "Darwin's journal", "H.M.S. Beagle in South America", "The Autobiography of Charles Darwin", "Voyage of the Beagle (Harvard Classics, Part 29)", "Les mouvements et les habitudes des plantes grimpantes", "Charles Darwin on the routes of male humble bees", "The origin of species : complete and fully illustrated", "The autobiography of Charles Darwin, 1809-1882", "Metaphysics, Materialism, & the evolution of mind", "Evolution", "The Correspondence of Charles Darwin, Volume 12: 1864", "Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Autobiography of Charles Darwin [EasyRead Edition]", "Reise eines Naturforschers um die Welt", "The Origin of Species (Oxford World's Classics)", "The geology of the voyage of H.M.S. Beagle", "The Formation of Vegetable Mould through the Action of Worms", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The action of carbonate of ammonia on the roots of certain plants", "Wu zhong qi yuan", "The Origin of Species", "Charles Darwin's natural selection", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Part I: Contributions to the Theory of Natural Selection / Part II", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 3", "Darwin Compendium", "Diary of the voyage of H.M.S. Beagle", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 6", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Insectivorous Plants", "Origin of Species (Harvard Classics, Part 11)", "To the members of the Down Friendly Club", "The\u0301orie de l'e\u0301volution", "The Voyage of the Beagle (Mentor)", "red notebook of Charles Darwin", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Expression of the Emotions in Man And Animals", "La facult\u00e9 motrice dans les plantes", "El Origin De Las Especies", "La vie et la correspondance de Charles Darwin", "The voyage of the Beagle.", "The Voyage of the Beagle (Everyman Paperbacks)", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Reise um die Welt 1831 - 36", "The Origin of Species (Collector's Library)", "The Correspondence of Charles Darwin, Volume 17: 1869", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Structure And Distribution of Coral Reefs", "Tesakneri tsagume\u030c", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Beagle letters", "Voyage d'un naturaliste autour du monde", "The Descent of Man, and Selection in Relation to Sex", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "A student's introduction to Charles Darwin", "Charles Darwin", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Expression Of The Emotions In Man And Animals", "genese\u014ds t\u014dn eid\u014dn", "On the tendency of species to form varieties", "The Autobiography of Charles Darwin (Great Minds Series)", "Darwin en Patagonia", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Die geschlechtliche Zuchtwahl", "Rejse om jorden", "Kleinere geologische Abhandlungen", "Darwin and Henslow", "The descent of man, and selection in relation to sex", "The descent of man, and selection in relation to sex.", "monograph on the sub-class Cirripedia", "The descent of man and selection in relation to sex.", "Geological Observations on South America", "On evolution", "Origin of Species (Everyman's University Paperbacks)", "Notes on the fertilization of orchids", "The Correspondence of Charles Darwin, Volume 9", "From Darwin's unpublished notebooks", "Fertilisation of Orchids", "Die fundamente zur entstehung der arten", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Origin of Species (Great Books : Learning Channel)", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 14", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The principal works", "Voyage of the Beagle", "The Voyage of the Beagle (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 13: 1865", "The portable Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 8: 1860", "Proiskhozhdenie vidov", "The Correspondence of Charles Darwin, Volume 2", "ontstaan der soorten door natuurlijke teeltkeus", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Voyage of the Beagle", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Resa kring jorden", "The Correspondence of Charles Darwin, Volume 15: 1867", "Darwin-Wallace", "From So Simple a Beginning", "The Correspondence of Charles Darwin, Volume 7", "The Power of Movement in Plants", "The Expression of the Emotions in Man and Animals", "Voyage of the Beagle (Dover Value Editions)", "The Descent of Man and Selection in Relation to Sex", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Autobiography of Charles Darwin, and selected letters", "Charles Darwin's marginalia", "Darwin on humus and the earthworm", "The expression of the emotions in man and animals", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The foundations of the Origin of species", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Origin of Species", "The Autobiography Of Charles Darwin", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Voyage Of The Beagle"], "ans_acc": 0.06542056074766354, "ans_hit": 1, "ans_f1": 0.01840490797546012, "ans_precission": 0.6, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.12280701754385964, "path_ans_precision": 1.0, "path_ans_recall": 0.06542056074766354}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.institution -> University of Florida\n# Answer:\nm.05v4mnf", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.major_field_of_study -> Family, Youth and Community Sciences\n# Answer:\nm.05v4mnf"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nm.0j4z5bh", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0j5d2kv"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.containedby -> Europe\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Czech Republic\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> France\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> location.location.containedby -> Europe\n# Answer:\nBucura Dumbrav\u0103", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.passes -> Dukla Pass -> location.location.containedby -> Poland\n# Answer:\nDukla Pass"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> J. D. Salinger -> people.person.profession -> Writer\n# Answer:\nJ. D. Salinger", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet -> media_common.quotation_subject.quotations_about_this_subject -> As imagination bodies forth The forms of things unknown, the poet's pen Turns them to shapes\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> Hope-coventina01a\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet -> media_common.quotation_subject.quotations_about_this_subject -> \u201cA poet is a verb that blossoms light in gardens of dawn, or sometimes midnight.\u201d\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet -> fictional_universe.character_occupation.characters_with_this_occupation -> Arthur Rimbaud\n# Answer:\nPoet"], "ground_truth": ["Author", "Writer", "Poet", "Bard"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.spouse -> Ashley Thompson\n# Answer:\nm.0hpd4nj"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Jeremy Bieber\n# Answer:\nStratford"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05ckldy\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.locations -> Iraqi Kurdistan\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05cklf2\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.04y_t_s\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05cklf7\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> common.topic.notable_for -> g.125cxty6s\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.09vl6dk\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.0bhfk_c\n# Answer:\n1991 uprisings in Iraq"], "ground_truth": ["United States of America", "Australia", "France", "Argentina", "Iraq", "Saudi Arabia", "United Kingdom"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.24242424242424243, "ans_precission": 0.8, "ans_recall": 0.14285714285714285, "path_f1": 0.05970149253731343, "path_precision": 0.2, "path_recall": 0.03508771929824561, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.character -> London Tipton\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.actor -> Erin Cardillo\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.character -> Mr. Moseby\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Actor\n# Answer:\nm.0sgkyvs", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.character -> Emma Tutweiller\n# Answer:\nm.05v3ngr"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhl0 -> government.government_position_held.office_holder -> Mike DeWine\n# Answer:\nm.0bfmhl0", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Frances -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nHurricane Frances"], "ground_truth": ["Ted Strickland", "John Kasich", "Return J. Meigs, Jr."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nm.02nr829", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.02nr829", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.05ckhjp -> sports.sports_team_roster.team -> A.C. Milan\n# Answer:\nm.05ckhjp", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0bvddcl -> sports.sports_team_roster.team -> Preston North End F.C.\n# Answer:\nm.0bvddcl", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Jorge Alonso\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> San Diego County\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Salamanca 2008\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.second_level_divisions -> Asturias\n# Answer:\nSpain"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pt_4 -> base.popstra.dated.participant -> Betty Grable\n# Answer:\nm.065pt_4", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pzp2 -> base.popstra.dated.participant -> Arlene Dahl\n# Answer:\nm.065pzp2", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.family_member.family -> Kennedy family\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.nationality -> United States of America\n# Answer:\nArabella Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Akita Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Fukushima Prefecture\n# Answer:\nJapan"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> Anguilla\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories -> location.country.first_level_divisions -> Anguilla\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories -> location.location.partially_containedby -> Western Europe\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Blaenau Gwent\n# Answer:\nWales"], "ground_truth": ["Northern Ireland", "Wales", "England", "Scotland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.22222222222222224, "path_precision": 0.2, "path_recall": 0.25, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> common.topic.notable_types -> River\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> common.topic.notable_types -> River\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon -> common.topic.notable_types -> River\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Livingston Parish\n# Answer:\nAmite River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> people.person.profession -> Philosopher\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Physician\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> people.person.profession -> Writer\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter"], "ground_truth": ["Philosopher", "Physician", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson -> common.topic.notable_types -> Deceased Person\n# Answer:\nSal Gibson"], "ground_truth": ["Sal Gibson", "Leon Cole", "Francine Lons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> common.topic.subject_of -> As Egypt Howls and History Tweets -> book.written_work.subjects -> Hosni Mubarak\n# Answer:\nAs Egypt Howls and History Tweets", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> common.topic.notable_types -> Governmental Body\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> common.topic.subject_of -> As Egypt Howls and History Tweets -> common.topic.subjects -> Hosni Mubarak\n# Answer:\nAs Egypt Howls and History Tweets", "# Reasoning Path:\nEgypt -> common.topic.subject_of -> As Egypt Howls and History Tweets -> common.resource.annotations -> m.0g98xbf\n# Answer:\nAs Egypt Howls and History Tweets", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.06_41f8\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nm.0_714v2"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Fernando Pessoa -> common.topic.notable_types -> Author\n# Answer:\nFernando Pessoa", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Fernando Pessoa -> influence.influence_node.influenced -> Jos\u00e9 Saramago\n# Answer:\nFernando Pessoa", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Fumiko Enchi -> common.topic.notable_types -> Author\n# Answer:\nFumiko Enchi", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Fumiko Enchi -> influence.influence_node.influenced_by -> Jun'ichir\u014d Tanizaki\n# Answer:\nFumiko Enchi", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Fumiko Enchi -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nFumiko Enchi", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Fernando Pessoa -> people.person.languages -> English Language\n# Answer:\nFernando Pessoa", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Novelist\n# Answer:\nAgatha Christie"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History -> base.movietheatres.movie_theatre.type_of_movietheatre -> IMAX\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Science Center -> location.location.geolocation -> m.0cltjm2\n# Answer:\nFernbank Science Center", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History -> common.topic.article -> m.04jnyg\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History -> location.location.containedby -> Georgia\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Science Center -> common.topic.notable_types -> Museum\n# Answer:\nFernbank Science Center", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History -> location.location.containedby -> 30307\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History -> location.location.containedby -> DeKalb County\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome"], "ground_truth": ["Peachtree Road Race", "Cobb Energy Performing Arts Centre", "Atlanta Ballet", "Jimmy Carter Library and Museum", "Philips Arena", "Underground Atlanta", "Six Flags White Water", "Atlanta History Center", "Margaret Mitchell House & Museum", "Hyatt Regency Atlanta", "Georgia State Capitol", "Variety Playhouse", "Atlanta Jewish Film Festival", "Georgia Dome", "Zoo Atlanta", "Georgia Aquarium", "Masquerade", "Fernbank Museum of Natural History", "Center for Puppetry Arts", "Centennial Olympic Park", "The Tabernacle", "Four Seasons Hotel Atlanta", "Woodruff Arts Center", "Martin Luther King, Jr. National Historic Site", "Omni Coliseum", "Six Flags Over Georgia", "Fernbank Science Center", "Arbor Place Mall", "Georgia World Congress Center", "Atlanta Cyclorama & Civil War Museum", "Atlanta Symphony Orchestra", "Turner Field", "World of Coca-Cola", "Atlanta Marriott Marquis", "CNN Center", "Fox Theatre"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15254237288135591, "ans_precission": 0.9, "ans_recall": 0.08333333333333333, "path_f1": 0.15254237288135591, "path_precision": 0.9, "path_recall": 0.08333333333333333, "path_ans_f1": 0.15254237288135591, "path_ans_precision": 0.9, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.basic_title -> Member of Parliament\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alf Roberts\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> Alakdana\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> media_common.literary_genre.books_in_this_genre -> Amis and Amiloun\n# Answer:\nChivalric romance"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> common.topic.article -> m.0dg0pnd\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> freebase.type_hints.included_types -> Person\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> base.descriptive_names.names.descriptive_name -> m.0104hg70\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> base.descriptive_names.names.descriptive_name -> m.0104hggx\n# Answer:\nTennis Player"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> common.topic.notable_types -> Currency -> type.type.expected_by -> currency\n# Answer:\nCurrency", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_types -> Currency -> freebase.type_hints.included_types -> Topic\n# Answer:\nCurrency", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.location.time_zones -> Cocos Islands\u00a0Time Zone\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_types -> Currency -> type.type.expected_by -> Adjustment currency\n# Answer:\nCurrency", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_types -> Currency -> type.type.properties -> Countries Formerly Used\n# Answer:\nCurrency"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgnsj"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11x1cfntl\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nm.0lwxmy9", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.book_edition.book -> To Kill a Mockingbird\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.publication.contents -> m.0j1v7r4\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> book.written_work.subjects -> History\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> common.topic.notable_types -> Book\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010l3l5d -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nm.010l3l5d"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.09k8753 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nm.09k8753", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.010g1fx8 -> government.government_position_held.office_holder -> Michael Mukasey\n# Answer:\nm.010g1fx8", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.09k8753 -> government.government_position_held.governmental_body -> Executive Office of the President of the United States\n# Answer:\nm.09k8753", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.010g1fx8 -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nm.010g1fx8", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.011qhxxc -> government.government_position_held.office_holder -> Sean Stackley\n# Answer:\nm.011qhxxc", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nm.079q3lx"], "ground_truth": ["Gene Amondson", "John Kerry", "Michael Peroutka", "Ralph Nader"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0pdthbn", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> Civilization: The West and the Rest -> common.topic.notable_types -> Book\n# Answer:\nCivilization: The West and the Rest", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7 -> film.personal_film_appearance.film -> The Ascent of Money\n# Answer:\nm.0h4nmc7"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> base.aareas.schema.administrative_area.administrative_parent -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> base.aareas.schema.administrative_area.administrative_children -> Gal\u00e1pagos Province\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.adjoin_s -> m.02_4t96\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nm.0115qhzk", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fs_z\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.ceremony -> 2014 Billboard Music Awards\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.award -> Billboard Music Award for Top Social Artist\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fsyr\n# Answer:\nJustin Bieber's Believe"], "ground_truth": ["Change Me", "Somebody to Love", "Thought Of You", "Roller Coaster", "Home to Mama", "Die in Your Arms", "Right Here", "Eenie Meenie", "Baby", "Never Say Never", "Turn to You (Mother's Day Dedication)", "Pray", "All That Matters", "Heartbreaker", "As Long as You Love Me", "Bigger", "Beauty And A Beat", "Recovery", "Wait for a Minute", "Never Let You Go", "First Dance", "Confident", "#thatPower", "Live My Life", "Bad Day", "All Around The World", "Hold Tight", "PYD", "Lolly", "Boyfriend", "All Bad"], "ans_acc": 0.03225806451612903, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.04878048780487805, "path_ans_precision": 0.1, "path_ans_recall": 0.03225806451612903}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> fictional_universe.character_occupation.characters_with_this_occupation -> Arthur Wellesley, 1st Duke of Wellington\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Ali G\n# Answer:\nJournalist"], "ground_truth": ["Journalist", "Statesman", "Physician", "Writer", "Publisher"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nm.04st6lz", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1l1s", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nm.04st6lz"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Iowa\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.program_creator -> Bruce Helford -> people.person.place_of_birth -> United States of America\n# Answer:\nBruce Helford", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.program_creator -> Robert Borden\n# Answer:\nRobert Borden", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles County\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Chris Snail\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Deborah Lurie\n# Answer:\nMission Hills"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.geolocation -> m.0239ks3\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tcbqv4\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tm_xvh\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tm_xwq\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_khq\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> common.topic.article -> m.0h6dr\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_for -> g.1q6hmhsk5\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive"], "ground_truth": ["Qiyamah", "Islamic view of angels", "Mahdi", "Islamic holy books", "Prophets in Islam", "Tawhid", "\u1e6c\u016bb\u0101", "Sharia", "God in Islam", "Monotheism", "Predestination in Islam", "Entering Heaven alive", "Masih ad-Dajjal"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.17391304347826086, "ans_precission": 0.2, "ans_recall": 0.15384615384615385, "path_f1": 0.17391304347826086, "path_precision": 0.2, "path_recall": 0.15384615384615385, "path_ans_f1": 0.17391304347826086, "path_ans_precision": 0.2, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> kp_lw.philosophy_influencer.influencee -> Mookie Tenembaum -> kp_lw.philosopher.era -> Contemporary philosophy\n# Answer:\nMookie Tenembaum", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> kp_lw.philosophy_influencer.influencee -> Mookie Tenembaum -> common.topic.notable_types -> Person\n# Answer:\nMookie Tenembaum", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> kp_lw.philosophy_influencer.influencee -> Mookie Tenembaum -> kp_lw.philosopher.main_interests -> Biology\n# Answer:\nMookie Tenembaum"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.geographic_scope -> Germany\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\ng.11b7_lvdf2", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nm.03lpqjt"], "ground_truth": ["Actor", "Songwriter", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z -> sports.sports_team_roster.team -> Los Angeles Bulldogs\n# Answer:\nm.0hpgj2z", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Position\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\nm.06sbpz2"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.nationality -> United States of America\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> people.person.nationality -> United States of America\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> comic_strips.comic_strip_creator.comic_strips_written -> m.0gwbjrm\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0jsmxbk -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0jsmxbk", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010by0k1\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> film.film_story_contributor.film_story_credits -> Little Orphan Annie\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> freebase.type_hints.included_types -> Topic\n# Answer:\nComposer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.07919ln", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\nm.07919ln"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> It Seems To Me -> common.topic.notable_types -> Book Edition\n# Answer:\nIt Seems To Me", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt -> people.deceased_person.place_of_death -> Scottsdale\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_for -> g.125b58fny\n# Answer:\nAutobiography of Eleanor Roosevelt"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\ng.1245_67l9", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\ng.1hhc378kt", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\ng.1hhc38d0l"], "ground_truth": ["Islam", "Protestantism", "Catholicism", "Hinduism"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.containedby -> Missouri\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Anna Lindh assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Rifle\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.children -> Charles Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nm.03pn4x_", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nm.04hdfss"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> theater.theater_character.plays_appears_in -> Oliver!\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.010p_33b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> common.topic.image -> \\\"Please, sir, I want some more.\\\" Illustration by George Cruikshank.\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> common.topic.image -> Scrooges third visitor-John Leech,1843\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.dates -> m.0c0z7kp\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.011cd25b\n# Answer:\nGhost of Christmas Present"], "ground_truth": ["A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Chrysalis Children's Classics Series)", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Reissue)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "Great Expectations.", "A Christmas Carol (Apple Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Soundings)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Classic Fiction)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Read & Listen Books)", "The Pickwick papers", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (Signet Classics)", "A CHRISTMAS CAROL", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Illustrated Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Tor Classics)", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Acting Edition)", "Our mutual friend", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Prentice Hall Science)", "Oliver Twist", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Whole Story)", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Nelson Graded Readers)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "Hard times", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "Bleak house", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Large Print)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Dramascripts S.)", "Bleak House", "The Pickwick Papers", "A Tale of Two Cities (Penguin Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "The mystery of Edwin Drood", "Sketches by Boz", "A Christmas Carol (Soundings)", "A Christmas Carol", "A Christmas Carol (Family Classics)", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Paperback Classics)", "The old curiosity shop", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "Martin Chuzzlewit", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Enriched Classic)", "Little Dorrit", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (R)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Illustrated Junior Library)", "Dombey and Son.", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Cover to Cover Classics)", "The old curiosity shop.", "Great expectations", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Everyman's Library (Paper))", "Dombey and Son", "A Christmas Carol (Radio Theatre)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Value Books)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale Of Two Cities (Adult Classics in Audio)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Enriched Classics)", "Great expectations.", "Dombey and son", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Clear Print)", "Our mutual friend.", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "The cricket on the hearth", "A Tale of Two Cities (Collector's Library)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Acting Edition)", "The Old Curiosity Shop", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Silver Classics)", "David Copperfield", "A Christmas Carol (Classic Books on Cassettes Collection)", "Bleak House.", "A TALE OF TWO CITIES", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Penguin Readers, Level 5)", "Great Expectations", "David Copperfield.", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Collected Works of Charles Dickens)"], "ans_acc": 0.01775147928994083, "ans_hit": 1, "ans_f1": 0.011494252873563218, "ans_precission": 0.2, "ans_recall": 0.005917159763313609, "path_f1": 0.23225806451612901, "path_precision": 0.4, "path_recall": 0.16363636363636364, "path_ans_f1": 0.0339943342776204, "path_ans_precision": 0.4, "path_ans_recall": 0.01775147928994083}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nm.049y3kf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_conflict -> World War II\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_conflict -> Battle of Berlin\n# Answer:\nm.049y3kf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nm.049x6_k"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States Virgin Islands\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.04kq24y\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nCoamo", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.us_county.hud_section_8_area -> Puerto Rico, PR HUD Nonmetro Area\n# Answer:\nCoamo", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Adjuntas -> location.location.containedby -> United States, with Territories\n# Answer:\nAdjuntas", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Aguada -> location.location.containedby -> Aguadilla-Isabela-San Sebasti\u00e1n PR Metropolitan Statistical Area\n# Answer:\nAguada", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Aguada -> location.administrative_division.second_level_division_of -> United States, with Territories\n# Answer:\nAguada"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Adjuvant\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Brachytherapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Carboplatin\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> common.topic.webpage -> m.09v_zjv\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Head pressing\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Blumer's shelf\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Neurology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Bone pain\n# Answer:\nLung cancer"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Behind the Scenes\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> common.topic.notable_for -> g.125fby47j\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.languages -> English Language -> organization.organization_sector.organizations_in_this_sector -> dragon-class.com\n# Answer:\nEnglish Language", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Blueprints Gallery Feature\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Knight Moves Feature\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1 -> tv.tv_series_season.episodes -> A Nice, Indecent Little Town\n# Answer:\nKnight Rider - Season 1", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1 -> common.topic.notable_for -> g.1259rk9g7\n# Answer:\nKnight Rider - Season 1", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.resource -> NBC Sked Scoop: 'Medium' returns, 'ER' ends, 'Knight Rider' trimmed, and more!\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0m_1 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w0m_1"], "ground_truth": ["William Daniels"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.us_county.county_seat -> Franklin\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Newton County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1ct3wq\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> organization.organization.headquarters -> m.0dhcz20\n# Answer:\nCarver Center for Arts and Technology"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\ng.11bcf3yybd", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.military_personnel_involved -> Albion P. Howe\n# Answer:\nBattle of Antietam"], "ground_truth": ["American Civil War", "Battle of White Oak Swamp", "Second Battle of Bull Run", "First Battle of Rappahannock Station", "First Battle of Kernstown", "Battle of Front Royal", "Battle of Chancellorsville", "Battle of Harpers Ferry", "First Battle of Winchester", "Manassas Station Operations", "Battle of McDowell", "Battle of Hoke's Run", "Battle of Chantilly", "How Few Remain", "Battle of Cedar Mountain", "Jackson's Valley Campaign", "Battle of Port Republic", "Romney Expedition", "Battle of Hancock"], "ans_acc": 0.21052631578947367, "ans_hit": 1, "ans_f1": 0.17391304347826086, "ans_precission": 0.5, "ans_recall": 0.10526315789473684, "path_f1": 0.18750000000000003, "path_precision": 0.6, "path_recall": 0.1111111111111111, "path_ans_f1": 0.3236994219653179, "path_ans_precision": 0.7, "path_ans_recall": 0.21052631578947367}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Peter Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.deceased_person.place_of_burial -> Granary Burying Ground\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.symptoms -> Abdominal mass\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Pancreatectomy\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subjects -> Fabrice de Villeneuve\n# Answer:\nPainting"], "ground_truth": ["Madonna and Child with St Joseph", "g.121yh91r", "Madonna Litta", "Head of a Woman", "Adoration of the Magi", "g.12314dm1", "Portrait of a man in red chalk", "g.12215rxg", "The Battle of Anghiari", "g.1224tf0c", "The Baptism of Christ", "Portrait of Isabella d'Este", "Ginevra de' Benci", "The Virgin and Child with St Anne and St John the Baptist", "Portrait of a Musician", "The Last Supper", "g.1219sb0g", "g.121wt37c", "Lady with an Ermine", "Madonna of Laroque", "Mona Lisa", "Horse and Rider", "Drapery for a Seated Figure", "Vitruvian Man", "g.120vt1gz", "g.1213jb_b", "Leda and the Swan", "Portrait of a Young Fianc\u00e9e", "The Virgin and Child with St. Anne", "La belle ferronni\u00e8re", "Salvator Mundi", "Benois Madonna", "Lucan portrait of Leonardo da Vinci", "Medusa", "Bacchus", "Sala delle Asse", "Virgin of the Rocks", "Madonna of the Carnation", "St. John the Baptist", "The Holy Infants Embracing", "Leonardo's horse", "Madonna of the Yarnwinder", "St. Jerome in the Wilderness", "Annunciation", "g.1239jd9p"], "ans_acc": 0.044444444444444446, "ans_hit": 1, "ans_f1": 0.07272727272727272, "ans_precission": 0.2, "ans_recall": 0.044444444444444446, "path_f1": 0.07407407407407407, "path_precision": 0.2, "path_recall": 0.045454545454545456, "path_ans_f1": 0.07272727272727272, "path_ans_precision": 0.2, "path_ans_recall": 0.044444444444444446}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> common.topic.notable_types -> Administrative Division\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> Anticancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.notable_types -> Museum\n# Answer:\nBuddha Tooth Relic Temple and Museum"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> An Armonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Glass harmonica", "Franklin stove", "Lightning rod", "Bifocals"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpt45\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Ault\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_types -> Profession\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer"], "ground_truth": ["Musician", "Composer", "Librettist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Arkadii Dragomoshchenko\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.administrative_divisions -> Dresden\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq8sf\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein"], "ground_truth": ["Germany", "Liechtenstein", "Belgium", "Switzerland", "East Germany", "Austria", "Luxembourg"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.4210526315789473, "path_precision": 0.8, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental rock -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental rock -> music.genre.parent_genre -> Rock music\n# Answer:\nExperimental rock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Rock music\n# Answer:\n.997 Radiostorm Oldies"], "ground_truth": ["Psychedelic rock", "Rock music", "Experimental music", "Pop rock", "Blues rock", "Soft rock", "Art rock", "Experimental rock", "Pop music"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.8, "ans_recall": 0.3333333333333333, "path_f1": 0.47058823529411764, "path_precision": 0.8, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6870229007633587, "path_ans_precision": 0.9, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 112th United States Congress\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.article -> m.027phsx\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 113th United States Congress\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.05kh0nn"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> base.aareas.schema.administrative_area.administrative_parent -> Kingdom of Denmark -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> base.aareas.schema.administrative_area.administrative_parent -> Kingdom of Denmark -> location.location.contains -> Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Faroe Islands\n# Answer:\nNordic countries"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.postal_code.country -> United States of America\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98104 -> location.postal_code.country -> United States of America\n# Answer:\n98104", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98104 -> location.location.geometry -> m.055f3vp\n# Answer:\n98104", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98104 -> location.location.contains -> Alaskan Way Viaduct\n# Answer:\n98104", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.postal_code.country -> United States of America\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11btt54h7d\n# Answer:\ng.11btt54h7d"], "ground_truth": ["98125", "98190", "98114", "98161", "98155", "98122", "98136", "98170", "98107", "98105", "98124", "98134", "98166", "98109", "98199", "98178", "98158", "98146", "98198", "98116", "98132", "98144", "98185", "98112", "98165", "98138", "98119-4114", "98127", "98106", "98195", "98154", "98141", "98133", "98126", "98181", "98113", "98139", "98164", "98129", "98188", "98145", "98108", "98121", "98174", "98168", "98104", "98119", "98117", "98115", "98103", "98148", "98118", "98131", "98171", "98177", "98102", "98160", "98184", "98175", "98191", "98194", "98111", "98101"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08917197452229297, "ans_precission": 0.7, "ans_recall": 0.047619047619047616, "path_f1": 0.08917197452229297, "path_precision": 0.7, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08917197452229297, "path_ans_precision": 0.7, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> JellyRadio.com\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> common.topic.image -> Profile of a man in a crowd of people wearing a blue-colored suit\n# Answer:\nWill Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> language.language_writing_system.languages -> Chinese, Xiang Language\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.image -> Nu shu\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.form_of_government -> Socialist state\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Fuzhou dialect\n# Answer:\nChinese characters"], "ground_truth": ["Chinese characters", "'Phags-pa script", "N\u00fcshu script", "Traditional Chinese characters", "Simplified Chinese character"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Donald Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> tv.tv_series_season.episodes -> Entertainment Tonight: Whatever Happened To...\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.ceremony -> 35th Primetime Emmy Awards\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nm.0_yczwx"], "ground_truth": ["Berlinda Tolbert", "Paul Benedict", "Marla Gibbs", "Roxie Roker", "Zara Cully", "Franklin Cover", "Jay Hammer", "Mike Evans", "Damon Evans", "Sherman Hemsley", "Isabel Sanford"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08695652173913043, "path_precision": 0.3, "path_recall": 0.05084745762711865, "path_ans_f1": 0.13953488372093023, "path_ans_precision": 0.3, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Dock of the Bay -> common.topic.notable_types -> Newspaper\n# Answer:\nDock of the Bay", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Dock of the Bay -> book.periodical.first_issue_date -> m.0_pbpv1\n# Answer:\nDock of the Bay", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco News-Call Bulletin Newspaper -> book.newspaper.headquarters -> m.0j1grx7\n# Answer:\nSan Francisco News-Call Bulletin Newspaper", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco News-Call Bulletin Newspaper -> common.topic.notable_types -> Newspaper\n# Answer:\nSan Francisco News-Call Bulletin Newspaper", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.newspaper.issues -> San Francisco Bay Guardian, 24 Nov 1999\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906"], "ground_truth": ["San Francisco Foghorn", "The Golden Era", "California Star", "San Francisco News-Call Bulletin Newspaper", "Synapse", "The San Francisco Examiner", "San Francisco Chronicle", "Sing Tao Daily", "San Francisco Bay Guardian", "Bay Area Reporter", "Street Sheet", "The Daily Alta California", "San Francisco Daily", "San Francisco Bay View", "San Francisco Bay Times", "Dock of the Bay", "San Francisco Call", "AsianWeek", "San Francisco Business Times", "Free Society"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24705882352941178, "ans_precission": 0.7, "ans_recall": 0.15, "path_f1": 0.24705882352941178, "path_precision": 0.7, "path_recall": 0.15, "path_ans_f1": 0.24705882352941178, "path_ans_precision": 0.7, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Cocaine\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Amphetamine\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Elizabeth Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nElizabeth Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland"], "ground_truth": ["Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Orgin of Species", "The Structure and Distribution of Coral Reefs", "Evolution by natural selection", "Darwin Darwin", "Origins", "A Darwin Selection", "Questions about the breeding of animals", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "On a remarkable bar of sandstone off Pernambuco", "Les moyens d'expression chez les animaux", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The education of Darwin", "The living thoughts of Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Variation of Animals and Plants under Domestication", "The Darwin Reader Second Edition", "Darwin's Ornithological notes", "Evolutionary Writings: Including the Autobiographies", "Opsht\u0323amung fun menshen", "Darwinism stated by Darwin himself", "Evolution and natural selection", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Darwin for Today", "On the origin of species by means of natural selection", "The collected papers of Charles Darwin", "More Letters of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "On the Movements and Habits of Climbing Plants", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Darwin Reader First Edition", "On Natural Selection", "Memorias y epistolario i\u0301ntimo", "Volcanic Islands", "From so simple a beginning", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 9: 1861", "Del Plata a Tierra del Fuego", "Human nature, Darwin's view", "The Life and Letters of Charles Darwin Volume 2", "Works", "Notebooks on transmutation of species", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Het uitdrukken van emoties bij mens en dier", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 16: 1868", "Darwin's notebooks on transmutation of species", "Charles Darwin's letters", "vari\u00eberen der huisdieren en cultuurplanten", "Leben und Briefe von Charles Darwin", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Essential Darwin", "The voyage of Charles Darwin", "The Correspondence of Charles Darwin, Volume 10: 1862", "Darwin's journal", "H.M.S. Beagle in South America", "The Autobiography of Charles Darwin", "Charles Darwin on the routes of male humble bees", "Les mouvements et les habitudes des plantes grimpantes", "Metaphysics, Materialism, & the evolution of mind", "Evolution", "The Correspondence of Charles Darwin, Volume 12: 1864", "Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Reise eines Naturforschers um die Welt", "The geology of the voyage of H.M.S. Beagle", "The Formation of Vegetable Mould through the Action of Worms", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The action of carbonate of ammonia on the roots of certain plants", "Wu zhong qi yuan", "Charles Darwin's natural selection", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin from Insectivorous Plants to Worms", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Darwin Compendium", "Diary of the voyage of H.M.S. Beagle", "The Different Forms of Flowers on Plants of the Same Species", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Insectivorous Plants", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "To the members of the Down Friendly Club", "The\u0301orie de l'e\u0301volution", "red notebook of Charles Darwin", "La facult\u00e9 motrice dans les plantes", "La vie et la correspondance de Charles Darwin", "El Origin De Las Especies", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Geological Observations on the Volcanic Islands", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 17: 1869", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Tesakneri tsagume\u030c", "Beagle letters", "Voyage d'un naturaliste autour du monde", "The Descent of Man, and Selection in Relation to Sex", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "A student's introduction to Charles Darwin", "Charles Darwin", "genese\u014ds t\u014dn eid\u014dn", "On the tendency of species to form varieties", "Motsa ha-minim", "Darwin en Patagonia", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Die geschlechtliche Zuchtwahl", "Rejse om jorden", "Kleinere geologische Abhandlungen", "Darwin and Henslow", "monograph on the sub-class Cirripedia", "Geological Observations on South America", "On evolution", "Notes on the fertilization of orchids", "From Darwin's unpublished notebooks", "Fertilisation of Orchids", "Die fundamente zur entstehung der arten", "South American Geology", "Darwin's insects", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The principal works", "The Correspondence of Charles Darwin, Volume 13: 1865", "The portable Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Correspondence of Charles Darwin, Volume 8: 1860", "Proiskhozhdenie vidov", "ontstaan der soorten door natuurlijke teeltkeus", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Voyage of the Beagle", "Resa kring jorden", "The Correspondence of Charles Darwin, Volume 15: 1867", "Darwin-Wallace", "The Power of Movement in Plants", "The Expression of the Emotions in Man and Animals", "Charles Darwin's marginalia", "Darwin on humus and the earthworm", "The foundations of the Origin of species", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Life and Letters of Charles Darwin Volume 1", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl"], "ans_acc": 0.0457516339869281, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.3125, "path_precision": 1.0, "path_recall": 0.18518518518518517, "path_ans_f1": 0.0875, "path_ans_precision": 1.0, "path_ans_recall": 0.0457516339869281}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nm.0_0cs2_", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nm.09nsgl5", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nm.09nsglb", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh -> tv.tv_guest_role.episodes_appeared_in -> Johnny Grant\n# Answer:\nm.09nsglh"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness -> music.recording.releases -> Gettin' Ready\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All -> music.composition.recordings -> After All (stereo mix)\n# Answer:\nAfter All"], "ground_truth": ["I Am, I Am", "Virgin Man", "Just to See Her", "The Agony And The Ecstasy", "The Road to Damascus", "So Bad", "Hold on to Your Love", "Driving Thru Life in the Fast Lane", "Time After Time", "Don't Wanna Be Just Physical", "Tears of a Clown", "You Don't Know What It's Like", "Vitamin U", "Our Love Is Here to Stay", "Will You Still Love Me Tomorrow", "Walk on By", "Just To See Her Again", "Wanna Know My Mind", "Really Gonna Miss You", "Holly", "Be Who You Are", "Happy (Love Theme From Lady Sings the Blues)", "Be Careful What You Wish For", "It's Time to Stop Shoppin' Around", "Crusin", "Gang Bangin'", "A Tattoo", "She's Only a Baby Herself", "Keep Me", "Some People Will Do Anything for Love", "I'm in the Mood for Love", "Ain't That Peculiar", "Te Quiero Como Si No Hubiera Un Manana", "You Really Got a Hold on Me", "I Can't Get Enough", "Nearness of You", "Shoe Soul", "Who's Sad", "Just a Touch Away", "Aqui Con Tigo (Being With You)", "I Love The Nearness Of You", "You Take Me Away", "Because of You It's the Best It's Ever Been", "More Than You Know", "Christmas Everyday", "Love' n Life", "Same Old Love", "What's Too Much", "If You Want My Love", "I Love Your Face", "No Time to Stop Believing", "Pops, We Love You (disco)", "Skid Row", "He Can Fix Anything", "Noel", "Food For Thought", "Fly Me to the Moon (In Other Words)", "Photograph in My Mind", "Mother's Son", "Ooo Baby Baby (live)", "I've Got You Under My Skin", "Close Encounters of the First Kind", "Wishful Thinking", "Tears of a Sweet Free Clown", "Going to a Go Go", "Tracks of My Tears", "More Love", "Love Brought Us Here", "Please Come Home for Christmas", "My Guy", "Cruisin'", "Come by Here (Kum Ba Ya)", "Tea for Two", "Tell Me Tomorrow", "I've Made Love To You A Thousand Times", "You're Just My Life (feat. India.Arie)", "When A Woman Cries", "Girl I'm Standing There", "Going to a Gogo", "Be Kind To The Growing Mind (with The Temptations)", "Jesus Told Me To Love You", "You're the One for Me (feat. Joss Stone)", "I Can\u2019t Stand to See You Cry (Commercial version)", "Everything for Christmas", "The Tracks of My Heart", "I Can't Give You Anything but Love", "Shop Around", "It's Her Turn to Live", "Fulfill Your Need", "One Heartbeat", "Theme From the Big Time", "I Like Your Face", "Tell Me Tomorrow (12\\\" extended mix)", "Sweet Harmony", "Share It", "If You Wanna Make Love (Come 'round Here)", "Being With You", "Wedding Song", "Don't Play Another Love Song", "Season's Greetings from Smokey Robinson", "Just Passing Through", "The Christmas Song", "And I Don't Love You", "I Hear The Children Singing", "Daylight & Darkness", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Bad Girl", "The Hurt's On You", "Unless You Do It Again", "Be Kind to the Growing Mind", "Away in the Manger / Coventry Carol", "Rewind", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Train of Thought", "Crusin'", "Get Ready", "Tell Me Tomorrow, Part 1", "Whatcha Gonna Do", "It's A Good Night", "The Track of My Tears", "(It's The) Same Old Love", "As You Do", "Just Another Kiss", "My World", "Come to Me Soon", "It's Fantastic", "Never My Love / Never Can Say Goodbye", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "I've Made Love to You a Thousand Times", "Take Me Through The Night", "The Love Between Me and My Kids", "Santa Claus is Coming to Town", "Christmas Greeting", "Just Like You", "I\u2019ve Got You Under My Skin", "If You Wanna Make Love", "Fallin'", "Open", "The Agony and the Ecstasy", "When Smokey Sings Tears Of A Clown", "Jingle Bells", "God Rest Ye Merry Gentlemen", "Don't Know Why", "Gone Forever", "The Family Song", "With Your Love Came", "I Care About Detroit", "It's a Good Feeling", "Baby That's Backatcha", "Can't Fight Love", "Hanging on by a Thread", "Medley: Never My Love / Never Can Say Goodbye", "Quiet Storm (Groove Boutique remix)", "Love Is The Light", "I Can't Find", "You've Really Go a Hold on Me", "I Know You by Heart", "It's Christmas Time", "Ooo Baby Baby", "Heavy On Pride (Light On Love)", "Love Letters", "Quiet Storm", "Why Are You Running From My Love", "Tears Of A Clown", "Little Girl Little Girl", "Night and Day", "Why", "Ooh Baby Baby", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Please Don't Take Your Love (feat. Carlos Santana)", "I Second That Emotions", "We've Saved the Best for Last", "You Are Forever", "Quiet Storm (Groove Boutique Chill Jazz mix)", "A Silent Partner in a Three-Way Love Affair", "In My Corner", "Satisfy You", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Speak Low", "And I Love Her", "Save Me", "Let Your Light Shine On Me", "Will You Love Me Tomorrow", "Jasmin", "A Child Is Waiting", "The Way You Do (The Things You Do)", "You've Really Got a Hold on Me", "I Am I Am", "Quiet Storm (single version)", "Be Careful What You Wish For (instrumental)", "Double Good Everything", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "The Tears of a Clown", "Rack Me Back", "Let Me Be The Clock", "I Have Prayed On It", "Everything You Touch", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "You Made Me Feel Love", "If You Can Want", "Ebony Eyes (Duet with Rick James)", "Yester Love", "Going to a Go-Go", "Time Flies", "You Are So Beautiful (feat. Dave Koz)", "Blame It on Love", "The Tears Of A Clown", "Ebony Eyes", "Melody Man", "I Praise & Worship You Father", "Love So Fine", "You Cannot Laugh Alone", "The Tracks of My Tears", "Will You Love Me Tomorrow?", "Tracks of my Tears", "There Will Come a Day (I'm Gonna Happen to You)", "My Girl", "Ever Had A Dream", "Tracks Of My Tears (Live)", "Love Bath", "Yes It's You Lady", "We Are The Warriors", "No\u00ebl", "Winter Wonderland", "Standing On Jesus", "Little Girl, Little Girl", "Blame It On Love (Duet with Barbara Mitchell)", "The Tracks of My Tears (live)", "One Time", "Love Don't Give No Reason", "Did You Know (Berry's Theme)", "Mickey's Monkey", "Girlfriend", "Christmas Every Day", "Deck the Halls", "Easy", "Cruisin", "You Go to My Head", "Coincidentally", "We\u2019ve Come Too Far to End It Now", "Love Don' Give No Reason (12 Inch Club Mix)", "The Tracks Of My Tears", "And I Don't Love You (Larry Levan instrumental dub)", "Why Do Happy Memories Hurt So Bad", "Let Me Be the Clock", "Baby Come Close", "There Will Come A Day ( I'm Gonna Happen To You )", "I Second That Emotion", "Just My Soul Responding", "I'll Keep My Light In My Window", "Pops, We Love You", "I'm Glad There Is You", "I Want You Back", "Sleepless Nights", "Asleep on My Love", "That Place"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.nationality -> United States of America\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> freebase.valuenotation.has_no_value -> Profession\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.children -> John Schlossberg\n# Answer:\nCaroline Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> time.event.locations -> Czech Republic\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}

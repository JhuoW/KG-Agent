{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> fictional_universe.character_occupation.characters_with_this_occupation -> Alice Dingle\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Arkansas\n# Answer:\nUnited States of America"], "ground_truth": ["United States Representative", "Governor of Tennessee", "Speaker of the United States House of Representatives"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9090909090909091, "ans_precission": 0.8333333333333334, "ans_recall": 1.0, "path_f1": 0.9090909090909091, "path_precision": 0.8333333333333334, "path_recall": 1.0, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0110s7g9\n# Answer:\nDetective"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.season -> 1992\u201393 NBA season\n# Answer:\nm.02kbc3w"], "ground_truth": ["Orlando Magic", "LSU Tigers men's basketball", "Los Angeles Lakers", "Boston Celtics", "Phoenix Suns", "Miami Heat", "Cleveland Cavaliers"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1142857142857143, "path_precision": 0.4, "path_recall": 0.06666666666666667, "path_ans_f1": 0.21052631578947364, "path_ans_precision": 0.4, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Akha Language", "Saek language", "Khmer language", "Cham language", "Nyaw Language", "Phu Thai language", "Thai Language", "Malay, Pattani Language", "Lao Language", "Mlabri Language", "Mon Language", "Hmong language", "Vietnamese Language"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.25974025974025977, "ans_precission": 0.8333333333333334, "ans_recall": 0.15384615384615385, "path_f1": 0.125, "path_precision": 0.3333333333333333, "path_recall": 0.07692307692307693, "path_ans_f1": 0.25974025974025977, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07 -> award.award_nomination.award -> NAACP Image Award for Outstanding Supporting Actor in a Motion Picture\n# Answer:\nm.0_rfq07", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07 -> award.award_nomination.award_nominee -> Justin Timberlake\n# Answer:\nm.0_rfq07"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Ayn Rand -> people.person.profession -> Writer\n# Answer:\nAyn Rand", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Ayn Rand -> common.topic.notable_types -> Author\n# Answer:\nAyn Rand"], "ground_truth": ["Archaeologist", "Teacher", "Author", "Statesman", "Writer", "Architect", "Inventor", "Lawyer", "Farmer", "Philosopher"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.5, "ans_recall": 0.1, "path_f1": 0.16666666666666669, "path_precision": 0.5, "path_recall": 0.1, "path_ans_f1": 0.4411764705882353, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 0.3}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The origin of species", "Opsht\u0323amung fun menshen", "The expression of the emotions in man and animals", "On the Movements and Habits of Climbing Plants", "Voyage of the Beagle (NG Adventure Classics)", "The voyage of Charles Darwin", "Die fundamente zur entstehung der arten", "The Correspondence of Charles Darwin, Volume 11", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Autobiography Of Charles Darwin", "The Voyage of the Beagle (Unabridged Classics)", "Darwin", "Tesakneri tsagume\u030c", "The Descent of Man and Selection in Relation to Sex", "Evolution by natural selection", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Origin of Species", "The Correspondence of Charles Darwin, Volume 9", "Notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 6", "The Origin of Species (Enriched Classics)", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Wu zhong qi yuan", "The Origin of Species (Great Minds Series)", "Het uitdrukken van emoties bij mens en dier", "Memorias y epistolario i\u0301ntimo", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Darwin for Today", "Diary of the voyage of H.M.S. Beagle", "The Descent of Man, and Selection in Relation to Sex", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 12: 1864", "The foundations of the Origin of species", "On a remarkable bar of sandstone off Pernambuco", "The Expression Of The Emotions In Man And Animals", "Voyage of the Beagle", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Voyage d'un naturaliste autour du monde", "Origin of Species (Harvard Classics, Part 11)", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Correspondence of Charles Darwin, Volume 5", "The Correspondence of Charles Darwin, Volume 8", "Works", "The principal works", "red notebook of Charles Darwin", "Voyage Of The Beagle", "Darwin's insects", "The Origin of Species (Collector's Library)", "Evolution and natural selection", "Darwin's notebooks on transmutation of species", "The Origin Of Species", "Volcanic Islands", "A student's introduction to Charles Darwin", "The autobiography of Charles Darwin", "Reise um die Welt 1831 - 36", "monograph on the sub-class Cirripedia", "The Expression of the Emotions in Man And Animals", "The Correspondence of Charles Darwin, Volume 18: 1870", "From so simple a beginning", "Kleinere geologische Abhandlungen", "Rejse om jorden", "Darwin Darwin", "On Natural Selection", "Darwin en Patagonia", "The Origin of Species", "Notes on the fertilization of orchids", "The action of carbonate of ammonia on the roots of certain plants", "Beagle letters", "Charles Darwin's letters", "Diario del Viaje de Un Naturalista Alrededor", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Charles Darwin's natural selection", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 9: 1861", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 4", "The Correspondence of Charles Darwin, Volume 14", "A Darwin Selection", "Reise eines Naturforschers um die Welt", "The Autobiography of Charles Darwin (Large Print)", "The Variation of Animals and Plants under Domestication", "The voyage of the Beagle.", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 7", "Origins", "Les mouvements et les habitudes des plantes grimpantes", "Leben und Briefe von Charles Darwin", "The Origin of Species (Great Books : Learning Channel)", "The Darwin Reader Second Edition", "The Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 11: 1863", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 2", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Darwin's journal", "The Voyage of the Beagle (Everyman Paperbacks)", "Darwin on humus and the earthworm", "The Correspondence of Charles Darwin, Volume 3", "The descent of man, and selection in relation to sex.", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Fertilisation of Orchids", "On the tendency of species to form varieties", "El Origin De Las Especies", "The\u0301orie de l'e\u0301volution", "The Correspondence of Charles Darwin, Volume 13: 1865", "The education of Darwin", "The Origin of Species (Oxford World's Classics)", "Charles Darwin", "La facult\u00e9 motrice dans les plantes", "The Structure And Distribution of Coral Reefs", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The expression of the emotions in man and animals.", "The Formation of Vegetable Mould through the Action of Worms", "From So Simple a Beginning", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "On the origin of species by means of natural selection", "On evolution", "The living thoughts of Darwin", "Proiskhozhdenie vidov", "The Life of Erasmus Darwin", "Insectivorous Plants", "La vie et la correspondance de Charles Darwin", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Les moyens d'expression chez les animaux", "The Autobiography of Charles Darwin (Dodo Press)", "ontstaan der soorten door natuurlijke teeltkeus", "Charles Darwin's marginalia", "Human nature, Darwin's view", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Darwinism stated by Darwin himself", "The descent of man, and selection in relation to sex", "The Autobiography of Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 16: 1868", "The origin of species : complete and fully illustrated", "The Darwin Reader First Edition", "The Essential Darwin", "The Voyage of the Beagle (Adventure Classics)", "Die geschlechtliche Zuchtwahl", "Voyage of the Beagle (Dover Value Editions)", "Voyage of the Beagle (Harvard Classics, Part 29)", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Autobiography of Charles Darwin [EasyRead Edition]", "The descent of man and selection in relation to sex.", "The Autobiography of Charles Darwin (Great Minds Series)", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 14: 1866", "More Letters of Charles Darwin", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 10", "The structure and distribution of coral reefs", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Evolution", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "H.M.S. Beagle in South America", "The Correspondence of Charles Darwin, Volume 17: 1869", "The geology of the voyage of H.M.S. Beagle", "The Voyage of the Beagle (Mentor)", "Resa kring jorden", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Correspondence of Charles Darwin, Volume 13", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 1", "The portable Darwin", "The collected papers of Charles Darwin", "The structure and distribution of coral reefs.", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 15", "The Origin of Species (World's Classics)", "Origin of Species (Everyman's University Paperbacks)", "To the members of the Down Friendly Club", "Metaphysics, Materialism, & the evolution of mind", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin's Ornithological notes", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Questions about the breeding of animals", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Origin of Species (Variorum Reprint)", "The Orgin of Species", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin-Wallace", "vari\u00eberen der huisdieren en cultuurplanten", "The Different Forms of Flowers on Plants of the Same Species", "The Origin of Species (Mentor)", "The Voyage of the Beagle (Great Minds Series)", "The Autobiography of Charles Darwin, and selected letters", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 12", "Charles Darwin on the routes of male humble bees", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Structure and Distribution of Coral Reefs", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Autobiography of Charles Darwin", "Darwin and Henslow", "The autobiography of Charles Darwin, 1809-1882"], "ans_acc": 0.04205607476635514, "ans_hit": 1, "ans_f1": 0.01840490797546012, "ans_precission": 0.6, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.08071748878923767, "path_ans_precision": 1.0, "path_ans_recall": 0.04205607476635514}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.institution -> University of Florida\n# Answer:\nm.05v4mnf", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0j5d2kv"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> base.locations.countries.continent -> Europe\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> location.location.containedby -> Europe\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Governess\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Bard", "Author", "Poet", "Writer"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.35294117647058826, "ans_precission": 0.6, "ans_recall": 0.25, "path_f1": 0.35294117647058826, "path_precision": 0.6, "path_recall": 0.25, "path_ans_f1": 0.35294117647058826, "path_ans_precision": 0.6, "path_ans_recall": 0.25}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hpd4nj"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.includes_event -> Bombing of Iraq\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.locations -> Iraqi Kurdistan\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> common.topic.image -> Iraq NO FLY ZONES\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nm.043wphl"], "ground_truth": ["Australia", "Iraq", "Saudi Arabia", "France", "Argentina", "United Kingdom", "United States of America"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.23076923076923073, "ans_precission": 0.6, "ans_recall": 0.14285714285714285, "path_f1": 0.06451612903225806, "path_precision": 0.4, "path_recall": 0.03508771929824561, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.character -> London Tipton\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> freebase.valuenotation.has_value -> Award Nominee\n# Answer:\nm.0sgkxsb"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob"], "ground_truth": ["Return J. Meigs, Jr.", "John Kasich", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nm.02nr829", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.02nr829", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.4, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.second_level_divisions -> Zaragoza\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.administrative_divisions -> Andalusia\n# Answer:\nSpain"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.nationality -> United States of America\n# Answer:\nArabella Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Wales", "Northern Ireland", "Scotland"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.3076923076923077, "path_precision": 0.4, "path_recall": 0.25, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nm.0wg8__0"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently. -> common.topic.notable_types -> Quotation\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.nationality -> United Kingdom\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently. -> media_common.quotation.subjects -> Truth\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> book.author.works_written -> A short history of ethics\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Physician", "Writer", "Philosopher"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.16666666666666666, "ans_recall": 0.3333333333333333, "path_f1": 0.2222222222222222, "path_precision": 0.16666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4444444444444444, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0 -> award.award_nomination.nominated_for -> Heaven Sent\n# Answer:\nm.05cqdz0"], "ground_truth": ["Leon Cole", "Francine Lons", "Sal Gibson"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.6, "ans_recall": 0.3333333333333333, "path_f1": 0.42857142857142855, "path_precision": 0.6, "path_recall": 0.3333333333333333, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.6, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nEgypt -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nEgypt -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Africa\n# Answer:\nEarth", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.2, "ans_recall": 0.5, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> book.written_work.author -> Alfred Russel Wallace -> people.person.gender -> Male\n# Answer:\nAlfred Russel Wallace", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Novelist\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.gender -> Female\n# Answer:\nAgatha Christie"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41 -> common.webpage.resource -> m.0blf53m\n# Answer:\nm.03ldb41", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> common.topic.notable_types -> Airport\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport"], "ground_truth": ["The Tabernacle", "Georgia Aquarium", "Zoo Atlanta", "Masquerade", "Philips Arena", "CNN Center", "Hyatt Regency Atlanta", "Fernbank Science Center", "Omni Coliseum", "Atlanta Ballet", "Georgia World Congress Center", "Six Flags Over Georgia", "Jimmy Carter Library and Museum", "World of Coca-Cola", "Cobb Energy Performing Arts Centre", "Atlanta History Center", "Centennial Olympic Park", "Turner Field", "Woodruff Arts Center", "Four Seasons Hotel Atlanta", "Peachtree Road Race", "Six Flags White Water", "Atlanta Marriott Marquis", "Martin Luther King, Jr. National Historic Site", "Georgia Dome", "Fernbank Museum of Natural History", "Margaret Mitchell House & Museum", "Variety Playhouse", "Arbor Place Mall", "Georgia State Capitol", "Fox Theatre", "Center for Puppetry Arts", "Atlanta Cyclorama & Civil War Museum", "Atlanta Jewish Film Festival", "Atlanta Symphony Orchestra", "Underground Atlanta"], "ans_acc": 0.027777777777777776, "ans_hit": 1, "ans_f1": 0.051948051948051945, "ans_precission": 0.4, "ans_recall": 0.027777777777777776, "path_f1": 0.051948051948051945, "path_precision": 0.4, "path_recall": 0.027777777777777776, "path_ans_f1": 0.051948051948051945, "path_ans_precision": 0.4, "path_ans_recall": 0.027777777777777776}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.office_position_or_title -> Member of the Queensland Legislative Assembly\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Elsie Tanner\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.resource -> m.0blsygc\n# Answer:\nm.04lt3gf"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr."], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.16666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.010g1fx8 -> government.government_position_held.office_holder -> Michael Mukasey\n# Answer:\nm.010g1fx8", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> common.topic.image -> BUSHLAURA\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.party -> Republican Party\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.010g1fx8 -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nm.010g1fx8", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2"], "ground_truth": ["John Kerry", "Ralph Nader", "Michael Peroutka", "Gene Amondson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4jq57"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> base.aareas.schema.administrative_area.administrative_parent -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island"], "ground_truth": ["Ecuador", "Gal\u00e1pagos Province", "Pacific Ocean"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.6, "ans_recall": 0.3333333333333333, "path_f1": 0.42857142857142855, "path_precision": 0.6, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.album -> Eenie Meenie -> music.recording.artist -> Sean Kingston\n# Answer:\nEenie Meenie", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx"], "ground_truth": ["First Dance", "Die in Your Arms", "Hold Tight", "Right Here", "Recovery", "Beauty And A Beat", "Bigger", "As Long as You Love Me", "PYD", "All Bad", "Never Say Never", "Pray", "Somebody to Love", "Change Me", "Turn to You (Mother's Day Dedication)", "All Around The World", "Never Let You Go", "#thatPower", "Home to Mama", "Baby", "Roller Coaster", "All That Matters", "Thought Of You", "Boyfriend", "Heartbreaker", "Bad Day", "Eenie Meenie", "Live My Life", "Wait for a Minute", "Lolly", "Confident"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.05555555555555555, "ans_precission": 0.2, "ans_recall": 0.03225806451612903, "path_f1": 0.05555555555555555, "path_precision": 0.2, "path_recall": 0.03225806451612903, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 0.4, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> base.descriptive_names.names.descriptive_name -> m.0105cq_d\n# Answer:\nJournalist"], "ground_truth": ["Publisher", "Journalist", "Statesman", "Writer", "Physician"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.3, "ans_precission": 0.6, "ans_recall": 0.2, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.3, "path_ans_precision": 0.6, "path_ans_recall": 0.2}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> base.aareas.schema.administrative_area.administrative_children -> Apache County -> location.location.containedby -> United States of America\n# Answer:\nApache County", "# Reasoning Path:\nArizona -> base.aareas.schema.administrative_area.administrative_children -> Apache County -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nApache County", "# Reasoning Path:\nArizona -> base.aareas.schema.administrative_area.administrative_children -> Apache County -> symbols.namesake.named_after -> Apache\n# Answer:\nApache County"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9090909090909091, "ans_precission": 0.8333333333333334, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Illinois\n# Answer:\nUnited States of America"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.program_creator -> Bruce Helford -> people.person.place_of_birth -> United States of America\n# Answer:\nBruce Helford", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\nm.03lppm1"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tcbqv4\n# Answer:\nDaegu"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.article -> m.02wvcg8\n# Answer:\nEntering Heaven alive"], "ground_truth": ["Tawhid", "Islamic view of angels", "Monotheism", "Islamic holy books", "Entering Heaven alive", "Mahdi", "Sharia", "Predestination in Islam", "Qiyamah", "Masih ad-Dajjal", "God in Islam", "Prophets in Islam", "\u1e6c\u016bb\u0101"], "ans_acc": 0.07692307692307693, "ans_hit": 1, "ans_f1": 0.12903225806451613, "ans_precission": 0.4, "ans_recall": 0.07692307692307693, "path_f1": 0.12903225806451613, "path_precision": 0.4, "path_recall": 0.07692307692307693, "path_ans_f1": 0.12903225806451613, "path_ans_precision": 0.4, "path_ans_recall": 0.07692307692307693}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> book.book_subject.works -> Shooting an Elephant -> book.published_work.published_in_published_as -> m.0q32l7t\n# Answer:\nShooting an Elephant", "# Reasoning Path:\nGeorge Orwell -> fictional_universe.fictional_character_creator.fictional_characters_created -> Big Brother -> common.topic.image -> YourCountryNeedsYou\n# Answer:\nBig Brother"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.geographic_scope -> Germany\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352942, "path_precision": 0.16666666666666666, "path_recall": 0.09090909090909091, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\ng.11b7_lvdf2", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.resource -> m.0bl181r\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9"], "ground_truth": ["Singer", "Actor", "Songwriter"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.25, "ans_recall": 0.3333333333333333, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.25, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbp_k", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.season -> 1955 Major League Baseball season\n# Answer:\nm.06sbp_k", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Abraham Cohn\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> freebase.valuenotation.is_reviewed -> Player\n# Answer:\nm.0gggrzr"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.nationality -> United States of America\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> freebase.type_hints.included_types -> Topic\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> rdf-schema#range -> Theatrical Composer\n# Answer:\nComposer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.07919ln"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nAutobiography of Eleanor Roosevelt"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1"], "ground_truth": ["Islam", "Hinduism", "Catholicism", "Protestantism"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.3333333333333333, "path_recall": 0.2, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.25}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.containedby -> Buchanan County\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> common.topic.notable_types -> US President\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Aunt Betsey Trotwood -> book.book_character.appears_in_book -> David Copperfield\n# Answer:\nAunt Betsey Trotwood", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Aunt Betsey Trotwood -> common.topic.article -> m.027hpqd\n# Answer:\nAunt Betsey Trotwood", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> people.person.nationality -> United Kingdom\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.dates -> m.0c0z7kp\n# Answer:\n(1846)"], "ground_truth": ["A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Penguin Classics)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Whole Story)", "Martin Chuzzlewit", "A Tale of Two Cities (Macmillan Students' Novels)", "Great expectations", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Watermill Classic)", "A TALE OF TWO CITIES", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Christmas Carol (Take Part)", "Bleak house", "Great Expectations.", "A Tale of Two Cities (Prentice Hall Science)", "Little Dorrit", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Classic Collection)", "Our mutual friend.", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities", "A Tale of Two Cities (Oxford Playscripts)", "A Christmas Carol (Penguin Readers, Level 2)", "A Christmas Carol (Family Classics)", "Sketches by Boz", "David Copperfield", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "The life and adventures of Nicholas Nickleby", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (Simple English)", "Bleak House.", "Our mutual friend", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Saddleback Classics)", "David Copperfield.", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Read & Listen Books)", "A Christmas Carol (Apple Classics)", "The mystery of Edwin Drood", "A Tale of Two Cities (Dramatized)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Cyber Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Classic Books on Cassettes Collection)", "Dombey and son", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Adopted Classic)", "Bleak House", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Watermill Classics)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Usborne Young Reading)", "The Pickwick Papers", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Illustrated Classics)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Penguin Readers, Level 5)", "The old curiosity shop", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Wordsworth Classics)", "A CHRISTMAS CAROL", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Great Stories)", "Dombey and Son.", "A Tale of Two Cities (Classics Illustrated)", "Hard times", "A Christmas Carol (New Longman Literature)", "The Old Curiosity Shop", "A Christmas Carol (Value Books)", "A Christmas Carol (Children's Theatre Playscript)", "Great Expectations", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Ladybird Classics)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Soundings)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Progressive English)", "Great expectations.", "A Christmas Carol (Classics Illustrated)", "A Christmas Carol (Radio Theatre)", "A Christmas Carol (R)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Compact English Classics)", "The old curiosity shop.", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Longman Classics, Stage 2)", "Oliver Twist", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Scholastic Classics)", "Dombey and Son", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Cp 1135)", "The cricket on the hearth", "The Pickwick papers", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Piccolo Books)", "A Tale Of Two Cities (Adult Classics)"], "ans_acc": 0.011834319526627219, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07, "path_precision": 0.2, "path_recall": 0.04242424242424243, "path_ans_f1": 0.022346368715083796, "path_ans_precision": 0.2, "path_ans_recall": 0.011834319526627219}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_conflict -> World War II\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> 58th Army\n# Answer:\n1940\u201344 insurgency in Chechnya"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States Virgin Islands\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\ng.11b6ddwl64", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Board members\n# Answer:\nDDR Corp."], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> A Scottish Soldier -> music.recording.releases -> Scotland the Brave\n# Answer:\nA Scottish Soldier"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Behind the Scenes\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w0_mw"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.us_county.county_seat -> Franklin\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3076923076923077, "path_precision": 0.4, "path_recall": 0.25, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War"], "ground_truth": ["Battle of McDowell", "First Battle of Kernstown", "Battle of Harpers Ferry", "Manassas Station Operations", "How Few Remain", "Battle of Front Royal", "Battle of Hoke's Run", "First Battle of Winchester", "Jackson's Valley Campaign", "Romney Expedition", "First Battle of Rappahannock Station", "Battle of Hancock", "Battle of White Oak Swamp", "Battle of Cedar Mountain", "Battle of Chancellorsville", "Second Battle of Bull Run", "Battle of Chantilly", "American Civil War", "Battle of Port Republic"], "ans_acc": 0.10526315789473684, "ans_hit": 1, "ans_f1": 0.09677419354838708, "ans_precission": 0.6, "ans_recall": 0.05263157894736842, "path_f1": 0.13559322033898305, "path_precision": 0.8, "path_recall": 0.07407407407407407, "path_ans_f1": 0.18604651162790697, "path_ans_precision": 0.8, "path_ans_recall": 0.10526315789473684}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Fresco\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio"], "ground_truth": ["Vitruvian Man", "The Battle of Anghiari", "Medusa", "St. John the Baptist", "Head of a Woman", "Mona Lisa", "g.12215rxg", "g.121wt37c", "Drapery for a Seated Figure", "The Holy Infants Embracing", "g.120vt1gz", "Virgin of the Rocks", "Portrait of a Musician", "La belle ferronni\u00e8re", "Portrait of Isabella d'Este", "The Last Supper", "g.12314dm1", "Adoration of the Magi", "g.1239jd9p", "Lady with an Ermine", "Portrait of a man in red chalk", "g.1213jb_b", "g.121yh91r", "Sala delle Asse", "Ginevra de' Benci", "Madonna of the Yarnwinder", "The Virgin and Child with St Anne and St John the Baptist", "g.1219sb0g", "The Baptism of Christ", "St. Jerome in the Wilderness", "Leonardo's horse", "Madonna and Child with St Joseph", "Annunciation", "The Virgin and Child with St. Anne", "Madonna Litta", "g.1224tf0c", "Horse and Rider", "Madonna of the Carnation", "Leda and the Swan", "Bacchus", "Lucan portrait of Leonardo da Vinci", "Benois Madonna", "Portrait of a Young Fianc\u00e9e", "Salvator Mundi", "Madonna of Laroque"], "ans_acc": 0.022222222222222223, "ans_hit": 1, "ans_f1": 0.039999999999999994, "ans_precission": 0.2, "ans_recall": 0.022222222222222223, "path_f1": 0.04081632653061225, "path_precision": 0.2, "path_recall": 0.022727272727272728, "path_ans_f1": 0.039999999999999994, "path_ans_precision": 0.2, "path_ans_recall": 0.022222222222222223}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> base.thoroughbredracing.thoroughbred_racehorse_sex.horses_of_this_sex -> Apology\n# Answer:\nFemale"], "ground_truth": ["Cervical cancer"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Bifocals", "Lightning rod", "Glass harmonica", "Franklin stove"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.5, "ans_recall": 0.25, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.5, "path_ans_recall": 0.25}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer"], "ground_truth": ["Librettist", "Musician", "Composer"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.administrative_divisions -> Dresden\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nm.0102xvg7"], "ground_truth": ["Austria", "Germany", "Luxembourg", "East Germany", "Liechtenstein", "Switzerland", "Belgium"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.3870967741935483, "ans_precission": 0.6, "ans_recall": 0.2857142857142857, "path_f1": 0.23076923076923073, "path_precision": 0.6, "path_recall": 0.14285714285714285, "path_ans_f1": 0.5581395348837209, "path_ans_precision": 0.8, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Rock music\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\n.997 Radiostorm Oldies"], "ground_truth": ["Pop rock", "Blues rock", "Art rock", "Pop music", "Experimental rock", "Experimental music", "Rock music", "Psychedelic rock", "Soft rock"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.1739130434782609, "ans_precission": 0.4, "ans_recall": 0.1111111111111111, "path_f1": 0.1739130434782609, "path_precision": 0.4, "path_recall": 0.1111111111111111, "path_ans_f1": 0.32432432432432434, "path_ans_precision": 0.6, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.article -> m.027phsx\n# Answer:\nUnited States Senate election in Colorado, 2008"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> base.aareas.schema.administrative_area.administrative_parent -> Kingdom of Denmark -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> base.aareas.schema.administrative_area.administrative_parent -> Kingdom of Denmark -> location.location.contains -> Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.postal_code.country -> United States of America\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point"], "ground_truth": ["98160", "98145", "98119-4114", "98190", "98166", "98101", "98185", "98199", "98174", "98139", "98108", "98161", "98114", "98181", "98111", "98170", "98105", "98115", "98165", "98102", "98127", "98154", "98122", "98136", "98112", "98103", "98191", "98133", "98144", "98106", "98131", "98119", "98188", "98117", "98125", "98171", "98184", "98129", "98118", "98158", "98195", "98198", "98175", "98168", "98164", "98104", "98121", "98148", "98134", "98124", "98194", "98116", "98113", "98178", "98126", "98146", "98155", "98109", "98107", "98177", "98141", "98132", "98138"], "ans_acc": 0.015873015873015872, "ans_hit": 1, "ans_f1": 0.03076923076923077, "ans_precission": 0.5, "ans_recall": 0.015873015873015872, "path_f1": 0.03076923076923077, "path_precision": 0.5, "path_recall": 0.015873015873015872, "path_ans_f1": 0.03076923076923077, "path_ans_precision": 0.5, "path_ans_recall": 0.015873015873015872}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> common.topic.notable_types -> Musical genre\n# Answer:\nContemporary R&B"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia"], "ground_truth": ["Simplified Chinese character", "Traditional Chinese characters", "N\u00fcshu script", "Chinese characters", "'Phags-pa script"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.3, "ans_precission": 0.6, "ans_recall": 0.2, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.3, "path_ans_precision": 0.6, "path_ans_recall": 0.2}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.02h98gq"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.2, "path_recall": 0.14285714285714285, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdh -> tv.regular_tv_appearance.actor -> Marla Gibbs\n# Answer:\nm.03lkkdh", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_types -> TV Season\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Berlinda Tolbert", "Paul Benedict", "Damon Evans", "Mike Evans", "Sherman Hemsley", "Roxie Roker", "Isabel Sanford", "Franklin Cover", "Jay Hammer", "Marla Gibbs", "Zara Cully"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.03125, "path_precision": 0.2, "path_recall": 0.01694915254237288, "path_ans_f1": 0.12500000000000003, "path_ans_precision": 0.2, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> book.periodical.frequency_or_issues_per_year -> m.09s58j6\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.notable_for -> g.125d1300_\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.webpage -> m.03kz_rr\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906"], "ground_truth": ["AsianWeek", "Dock of the Bay", "Sing Tao Daily", "San Francisco News-Call Bulletin Newspaper", "San Francisco Chronicle", "The Daily Alta California", "Synapse", "Bay Area Reporter", "The Golden Era", "California Star", "San Francisco Business Times", "San Francisco Bay View", "San Francisco Call", "San Francisco Daily", "The San Francisco Examiner", "San Francisco Bay Guardian", "Free Society", "San Francisco Foghorn", "San Francisco Bay Times", "Street Sheet"], "ans_acc": 0.05, "ans_hit": 1, "ans_f1": 0.0923076923076923, "ans_precission": 0.6, "ans_recall": 0.05, "path_f1": 0.0923076923076923, "path_precision": 0.6, "path_recall": 0.05, "path_ans_f1": 0.0923076923076923, "path_ans_precision": 0.6, "path_ans_recall": 0.05}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Amphetamine\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology"], "ground_truth": ["Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Opsht\u0323amung fun menshen", "On the Movements and Habits of Climbing Plants", "The voyage of Charles Darwin", "Die fundamente zur entstehung der arten", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin", "Tesakneri tsagume\u030c", "Evolution by natural selection", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Notebooks on transmutation of species", "Wu zhong qi yuan", "Het uitdrukken van emoties bij mens en dier", "Memorias y epistolario i\u0301ntimo", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Darwin for Today", "Diary of the voyage of H.M.S. Beagle", "The Descent of Man, and Selection in Relation to Sex", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 12: 1864", "The foundations of the Origin of species", "On a remarkable bar of sandstone off Pernambuco", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Voyage d'un naturaliste autour du monde", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Works", "The principal works", "red notebook of Charles Darwin", "Darwin's insects", "Darwin's notebooks on transmutation of species", "Evolution and natural selection", "Volcanic Islands", "A student's introduction to Charles Darwin", "Reise um die Welt 1831 - 36", "monograph on the sub-class Cirripedia", "The Correspondence of Charles Darwin, Volume 18: 1870", "From so simple a beginning", "South American Geology", "Kleinere geologische Abhandlungen", "Rejse om jorden", "Darwin Darwin", "Darwin en Patagonia", "On Natural Selection", "Evolutionary Writings: Including the Autobiographies", "Notes on the fertilization of orchids", "The action of carbonate of ammonia on the roots of certain plants", "Beagle letters", "Charles Darwin's letters", "Diario del Viaje de Un Naturalista Alrededor", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Charles Darwin's natural selection", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 9: 1861", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "A Darwin Selection", "Reise eines Naturforschers um die Welt", "The Variation of Animals and Plants under Domestication", "Geological Observations on the Volcanic Islands", "Les mouvements et les habitudes des plantes grimpantes", "Origins", "The Life and Letters of Charles Darwin Volume 2", "The Darwin Reader Second Edition", "Leben und Briefe von Charles Darwin", "The Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 11: 1863", "Geological Observations on South America", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Darwin's journal", "Darwin on humus and the earthworm", "The Correspondence of Charles Darwin, Volume 13: 1865", "Fertilisation of Orchids", "On the tendency of species to form varieties", "El Origin De Las Especies", "The\u0301orie de l'e\u0301volution", "The education of Darwin", "Charles Darwin", "La facult\u00e9 motrice dans les plantes", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "On the origin of species by means of natural selection", "On evolution", "The living thoughts of Darwin", "Proiskhozhdenie vidov", "The Life of Erasmus Darwin", "Insectivorous Plants", "La vie et la correspondance de Charles Darwin", "Les moyens d'expression chez les animaux", "ontstaan der soorten door natuurlijke teeltkeus", "Charles Darwin's marginalia", "Human nature, Darwin's view", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Darwinism stated by Darwin himself", "The Autobiography of Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Essential Darwin", "The Darwin Reader First Edition", "Die geschlechtliche Zuchtwahl", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Life and Letters of Charles Darwin Volume 1", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 14: 1866", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "More Letters of Charles Darwin", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Evolution", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Correspondence of Charles Darwin, Volume 17: 1869", "H.M.S. Beagle in South America", "The geology of the voyage of H.M.S. Beagle", "Resa kring jorden", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Cartas de Darwin 18251859", "The portable Darwin", "The collected papers of Charles Darwin", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Expression of the Emotions in Man and Animals", "To the members of the Down Friendly Club", "Metaphysics, Materialism, & the evolution of mind", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin's Ornithological notes", "Darwin from Insectivorous Plants to Worms", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Questions about the breeding of animals", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Orgin of Species", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin-Wallace", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "vari\u00eberen der huisdieren en cultuurplanten", "The Different Forms of Flowers on Plants of the Same Species", "Del Plata a Tierra del Fuego", "Charles Darwin on the routes of male humble bees", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Structure and Distribution of Coral Reefs", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Darwin and Henslow"], "ans_acc": 0.0392156862745098, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.25806451612903225, "path_precision": 1.0, "path_recall": 0.14814814814814814, "path_ans_f1": 0.07547169811320754, "path_ans_precision": 1.0, "path_ans_recall": 0.0392156862745098}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nm.09nsgl5"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All -> music.composition.recordings -> After All (stereo mix)\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor"], "ground_truth": ["The Tracks of My Tears", "Rewind", "Tears Of A Clown", "Share It", "Heavy On Pride (Light On Love)", "Girlfriend", "Wishful Thinking", "Train of Thought", "Why", "Christmas Everyday", "You Cannot Laugh Alone", "Tell Me Tomorrow (12\\\" extended mix)", "Daylight & Darkness", "Hold on to Your Love", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "There Will Come a Day (I'm Gonna Happen to You)", "Deck the Halls", "The Love Between Me and My Kids", "One Heartbeat", "You Are So Beautiful (feat. Dave Koz)", "You've Really Got a Hold on Me", "The Agony And The Ecstasy", "Just My Soul Responding", "You Go to My Head", "Ebony Eyes (Duet with Rick James)", "We\u2019ve Come Too Far to End It Now", "Love Letters", "Virgin Man", "Medley: Never My Love / Never Can Say Goodbye", "When A Woman Cries", "Ebony Eyes", "Be Careful What You Wish For (instrumental)", "Our Love Is Here to Stay", "I\u2019ve Got You Under My Skin", "The Agony and the Ecstasy", "Easy", "Mickey's Monkey", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Being With You", "Mother's Son", "Did You Know (Berry's Theme)", "Happy (Love Theme From Lady Sings the Blues)", "Love Bath", "Why Do Happy Memories Hurt So Bad", "Everything for Christmas", "God Rest Ye Merry Gentlemen", "I Second That Emotion", "You Don't Know What It's Like", "I Love Your Face", "Yester Love", "I Can't Get Enough", "Going to a Go-Go", "Fly Me to the Moon (In Other Words)", "Be Kind To The Growing Mind (with The Temptations)", "Satisfy You", "Baby Come Close", "Let Me Be The Clock", "The Track of My Tears", "Be Careful What You Wish For", "Really Gonna Miss You", "Will You Love Me Tomorrow?", "Tracks of My Tears", "It's A Good Night", "Be Kind to the Growing Mind", "He Can Fix Anything", "Some People Will Do Anything for Love", "Noel", "You're Just My Life (feat. India.Arie)", "A Child Is Waiting", "I Can\u2019t Stand to See You Cry (Commercial version)", "The Tracks of My Tears (live)", "Christmas Every Day", "Melody Man", "Jasmin", "Let Your Light Shine On Me", "My World", "I've Got You Under My Skin", "Get Ready", "I Second That Emotions", "Just To See Her Again", "Don't Know Why", "Sweet Harmony", "No Time to Stop Believing", "Driving Thru Life in the Fast Lane", "You Made Me Feel Love", "My Girl", "The Tears Of A Clown", "Who's Sad", "Sleepless Nights", "Everything You Touch", "Time Flies", "Tracks Of My Tears (Live)", "Why Are You Running From My Love", "Keep Me", "Cruisin", "Photograph in My Mind", "There Will Come A Day ( I'm Gonna Happen To You )", "The Tracks Of My Tears", "Just Passing Through", "Will You Still Love Me Tomorrow", "Nearness of You", "It's Christmas Time", "Cruisin'", "It's Time to Stop Shoppin' Around", "Ooh Baby Baby", "And I Love Her", "You've Really Go a Hold on Me", "Gang Bangin'", "Just Another Kiss", "Love Is The Light", "If You Wanna Make Love (Come 'round Here)", "The Way You Do (The Things You Do)", "I Am, I Am", "Tears of a Sweet Free Clown", "Same Old Love", "Asleep on My Love", "Rack Me Back", "I Praise & Worship You Father", "Fulfill Your Need", "Love' n Life", "You Take Me Away", "Winter Wonderland", "Can't Fight Love", "Standing On Jesus", "I Am I Am", "Don't Play Another Love Song", "One Time", "Save Me", "Crusin", "Speak Low", "Quiet Storm (Groove Boutique remix)", "You Are Forever", "Night and Day", "I Have Prayed On It", "Tears of a Clown", "Gone Forever", "No\u00ebl", "Shoe Soul", "The Tracks of My Heart", "Just Like You", "Just a Touch Away", "A Silent Partner in a Three-Way Love Affair", "It's Fantastic", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Christmas Greeting", "Come to Me Soon", "I Love The Nearness Of You", "Wanna Know My Mind", "I Can't Find", "The Christmas Song", "When Smokey Sings Tears Of A Clown", "I Want You Back", "Double Good Everything", "Tell Me Tomorrow", "If You Wanna Make Love", "I've Made Love To You A Thousand Times", "Girl I'm Standing There", "Let Me Be the Clock", "With Your Love Came", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Time After Time", "Never My Love / Never Can Say Goodbye", "I Can't Give You Anything but Love", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "I've Made Love to You a Thousand Times", "Away in the Manger / Coventry Carol", "Skid Row", "The Family Song", "Yes It's You Lady", "The Road to Damascus", "Love So Fine", "And I Don't Love You (Larry Levan instrumental dub)", "Going to a Go Go", "I Know You by Heart", "Tracks of my Tears", "Holly", "A Tattoo", "Open", "Going to a Gogo", "Tea for Two", "Pops, We Love You", "Whatcha Gonna Do", "I'm in the Mood for Love", "Come by Here (Kum Ba Ya)", "Te Quiero Como Si No Hubiera Un Manana", "As You Do", "Blame It on Love", "Pops, We Love You (disco)", "If You Want My Love", "I'll Keep My Light In My Window", "Because of You It's the Best It's Ever Been", "In My Corner", "Ooo Baby Baby", "Quiet Storm (single version)", "Please Come Home for Christmas", "You Really Got a Hold on Me", "I Care About Detroit", "Quiet Storm", "More Than You Know", "And I Don't Love You", "The Hurt's On You", "It's a Good Feeling", "Little Girl, Little Girl", "What's Too Much", "Baby That's Backatcha", "Ooo Baby Baby (live)", "Love Don't Give No Reason", "We Are The Warriors", "I'm Glad There Is You", "I Hear The Children Singing", "Food For Thought", "Vitamin U", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Don't Wanna Be Just Physical", "She's Only a Baby Herself", "Ain't That Peculiar", "Love Don' Give No Reason (12 Inch Club Mix)", "Walk on By", "Fallin'", "Santa Claus is Coming to Town", "You're the One for Me (feat. Joss Stone)", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "So Bad", "Wedding Song", "Crusin'", "Hanging on by a Thread", "The Tears of a Clown", "Jesus Told Me To Love You", "Blame It On Love (Duet with Barbara Mitchell)", "Theme From the Big Time", "(It's The) Same Old Love", "Just to See Her", "Coincidentally", "Aqui Con Tigo (Being With You)", "Please Don't Take Your Love (feat. Carlos Santana)", "Be Who You Are", "Take Me Through The Night", "Close Encounters of the First Kind", "Love Brought Us Here", "Shop Around", "Bad Girl", "I Like Your Face", "Season's Greetings from Smokey Robinson", "Jingle Bells", "More Love", "If You Can Want", "Tell Me Tomorrow, Part 1", "My Guy", "Little Girl Little Girl", "We've Saved the Best for Last", "That Place", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "It's Her Turn to Live", "Unless You Do It Again", "Ever Had A Dream", "Will You Love Me Tomorrow"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.nationality -> United States of America\n# Answer:\nArabella Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> time.event.locations -> Czech Republic\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}

{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane\n# Answer:\n1900 Galveston hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1915 Galveston hurricane\n# Answer:\n1915 Galveston hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8\n# Answer:\nm.04j5sk8", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer\n# Answer:\nLawyer"], "ground_truth": ["Governor of Tennessee", "Speaker of the United States House of Representatives", "United States Representative"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_3vpv\n# Answer:\ng.1245_3vpv", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_4dk3\n# Answer:\ng.1245_4dk3"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe\n# Answer:\nDC Universe", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician"], "ground_truth": ["Ilyssa Fradin", "Hannah Gunn", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58\n# Answer:\nm.02kbc58", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc5c\n# Answer:\nm.02kbc5c"], "ground_truth": ["LSU Tigers men's basketball", "Boston Celtics", "Los Angeles Lakers", "Phoenix Suns", "Cleveland Cavaliers", "Miami Heat", "Orlando Magic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0cqc3ql\n# Answer:\nm.0cqc3ql", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk\n# Answer:\nm.0n2qyfk"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\ng.12tb6gh2z", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\ng.1hhc37pvk"], "ground_truth": ["Saek language", "Mlabri Language", "Khmer language", "Phu Thai language", "Nyaw Language", "Hmong language", "Cham language", "Vietnamese Language", "Thai Language", "Malay, Pattani Language", "Lao Language", "Mon Language", "Akha Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5714285714285714, "ans_recall": 0.3076923076923077, "path_f1": 0.3, "path_precision": 0.42857142857142855, "path_recall": 0.23076923076923078, "path_ans_f1": 0.4, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07\n# Answer:\nm.0_rfq07", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkghb\n# Answer:\nm.0fpkghb", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0dlskcl\n# Answer:\nm.0dlskcl", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpnkz_\n# Answer:\nm.0fpnkz_"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson"], "ground_truth": ["Architect", "Inventor", "Author", "Writer", "Farmer", "Philosopher", "Statesman", "Lawyer", "Archaeologist", "Teacher"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.5714285714285714, "ans_recall": 0.4, "path_f1": 0.3529411764705882, "path_precision": 0.42857142857142855, "path_recall": 0.3, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Celebrating Evolution the Web Way\n# Answer:\nCelebrating Evolution the Web Way", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Charles Darwin in Cyberspace\n# Answer:\nCharles Darwin in Cyberspace"], "ground_truth": ["Letters from C. Darwin, Esq., to A. Hancock, Esq", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The voyage of Charles Darwin", "The Origin Of Species", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Memorias y epistolario i\u0301ntimo", "The Origin of Species (Oxford World's Classics)", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Correspondence of Charles Darwin, Volume 8: 1860", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Autobiography of Charles Darwin", "The Expression of the Emotions in Man And Animals", "La vie et la correspondance de Charles Darwin", "The Voyage of the Beagle (Mentor)", "The\u0301orie de l'e\u0301volution", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 2", "Wu zhong qi yuan", "The foundations of the Origin of species", "Origins", "The Structure and Distribution of Coral Reefs", "The principal works", "Reise eines Naturforschers um die Welt", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Motsa ha-minim", "The Origin of Species", "The Correspondence of Charles Darwin, Volume 12", "The Autobiography Of Charles Darwin", "Darwin and Henslow", "Questions about the breeding of animals", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Autobiography of Charles Darwin, and selected letters", "Leben und Briefe von Charles Darwin", "Darwin's notebooks on transmutation of species", "The Origin of Species (Enriched Classics)", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 7", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Orgin of Species", "From So Simple a Beginning", "On the tendency of species to form varieties", "Origin of Species (Harvard Classics, Part 11)", "The collected papers of Charles Darwin", "Notebooks on transmutation of species", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Voyage Of The Beagle", "Die fundamente zur entstehung der arten", "Diary of the voyage of H.M.S. Beagle", "The autobiography of Charles Darwin", "The Origin of Species (Great Books : Learning Channel)", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Voyage of the Beagle (Adventure Classics)", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Correspondence of Charles Darwin, Volume 10", "The Correspondence of Charles Darwin, Volume 11", "The Correspondence of Charles Darwin, Volume 9", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Cartas de Darwin 18251859", "Darwin's Ornithological notes", "Kleinere geologische Abhandlungen", "Fertilisation of Orchids", "Darwin's insects", "The education of Darwin", "The Voyage of the Beagle (Great Minds Series)", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 15", "A Darwin Selection", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Metaphysics, Materialism, & the evolution of mind", "The Origin of Species (Variorum Reprint)", "The expression of the emotions in man and animals", "The Expression Of The Emotions In Man And Animals", "genese\u014ds t\u014dn eid\u014dn", "ontstaan der soorten door natuurlijke teeltkeus", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 11: 1863", "On a remarkable bar of sandstone off Pernambuco", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin-Wallace", "The Structure And Distribution of Coral Reefs", "The portable Darwin", "Les moyens d'expression chez les animaux", "The descent of man and selection in relation to sex.", "The Correspondence of Charles Darwin, Volume 13", "Charles Darwin's letters", "Voyage of the Beagle (Dover Value Editions)", "Charles Darwin on the routes of male humble bees", "Volcanic Islands", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The descent of man, and selection in relation to sex.", "On the origin of species by means of natural selection", "Voyage of the Beagle", "A student's introduction to Charles Darwin", "Evolution and natural selection", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Essential Darwin", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "More Letters of Charles Darwin", "Works", "monograph on the sub-class Cirripedia", "The Power of Movement in Plants", "On evolution", "Origin of Species", "Tesakneri tsagume\u030c", "Part I: Contributions to the Theory of Natural Selection / Part II", "The living thoughts of Darwin", "H.M.S. Beagle in South America", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Origin of Species (Mentor)", "The descent of man, and selection in relation to sex", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Voyage of the Beagle (Unabridged Classics)", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 15: 1867", "Evolution by natural selection", "Darwin on humus and the earthworm", "Charles Darwin's natural selection", "The Voyage of the Beagle (Everyman Paperbacks)", "Proiskhozhdenie vidov", "Insectivorous Plants", "Voyage of the Beagle (Harvard Classics, Part 29)", "On the Movements and Habits of Climbing Plants", "Charles Darwin's marginalia", "The Correspondence of Charles Darwin, Volume 4", "The Origin of Species (Collector's Library)", "The Descent of Man and Selection in Relation to Sex", "The origin of species", "Gesammelte kleinere Schriften", "The voyage of the Beagle.", "The structure and distribution of coral reefs.", "The Autobiography of Charles Darwin [EasyRead Edition]", "Les mouvements et les habitudes des plantes grimpantes", "The geology of the voyage of H.M.S. Beagle", "The Origin of Species (World's Classics)", "To the members of the Down Friendly Club", "The Correspondence of Charles Darwin, Volume 8", "The autobiography of Charles Darwin, 1809-1882", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "La facult\u00e9 motrice dans les plantes", "The origin of species : complete and fully illustrated", "From so simple a beginning", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The expression of the emotions in man and animals.", "The Autobiography of Charles Darwin", "The Autobiography of Charles Darwin (Dodo Press)", "El Origin De Las Especies", "From Darwin's unpublished notebooks", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 3", "Die geschlechtliche Zuchtwahl", "Darwinism stated by Darwin himself", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 14", "The structure and distribution of coral reefs", "Notes on the fertilization of orchids", "Darwin for Today", "Geological Observations on South America", "The Voyage of the Beagle", "Resa kring jorden", "The Correspondence of Charles Darwin, Volume 6", "The Correspondence of Charles Darwin, Volume 1", "Darwin Darwin", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin", "The Correspondence of Charles Darwin, Volume 10: 1862", "Darwin's journal", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Life of Erasmus Darwin", "Darwin Compendium", "Beagle letters", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Evolution", "The Origin of Species (Great Minds Series)", "The Autobiography of Charles Darwin (Large Print)", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Correspondence of Charles Darwin, Volume 5", "On Natural Selection", "The Expression of the Emotions in Man and Animals", "Rejse om jorden", "Voyage of the Beagle (NG Adventure Classics)", "Charles Darwin", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The action of carbonate of ammonia on the roots of certain plants", "The Autobiography of Charles Darwin (Great Minds Series)", "Origin of Species (Everyman's University Paperbacks)", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Darwin Reader First Edition", "Darwin en Patagonia", "The Descent of Man, and Selection in Relation to Sex", "The Variation of Animals and Plants under Domestication", "The Correspondence of Charles Darwin, Volume 18: 1870"], "ans_acc": 0.018691588785046728, "ans_hit": 1, "ans_f1": 0.027366020524515394, "ans_precission": 0.5714285714285714, "ans_recall": 0.014018691588785047, "path_f1": 0.06666666666666667, "path_precision": 1.0, "path_recall": 0.034482758620689655, "path_ans_f1": 0.03669724770642201, "path_ans_precision": 1.0, "path_ans_recall": 0.018691588785046728}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w\n# Answer:\nm.07mmh5w", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773\n# Answer:\nm.0791773", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07sgy3b\n# Answer:\nm.07sgy3b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth"], "ground_truth": ["Denver Broncos"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Brebeneskul\n# Answer:\nBrebeneskul", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103\n# Answer:\nBucura Dumbrav\u0103"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Bob Dylan\n# Answer:\nBob Dylan", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Charlotte Bront\u00eb\n# Answer:\nCharlotte Bront\u00eb"], "ground_truth": ["Bard", "Poet", "Author", "Writer"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.42857142857142855, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.42857142857142855, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard\n# Answer:\nThe Changing of the Guard", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Dangerous Games\n# Answer:\nThe Dangerous Games"], "ground_truth": ["Hayden Christensen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStratford"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Air engagements of the Gulf War\n# Answer:\nAir engagements of the Gulf War", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl\n# Answer:\nm.043wphl"], "ground_truth": ["France", "Argentina", "United Kingdom", "Australia", "Saudi Arabia", "United States of America", "Iraq"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.19047619047619047, "ans_precission": 0.2857142857142857, "ans_recall": 0.14285714285714285, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.19047619047619047, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs\n# Answer:\nm.0sgkyvs", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86\n# Answer:\nm.0sgkz86", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol\n# Answer:\nA London Carol"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Ethel\n# Answer:\nHurricane Ethel", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Frances\n# Answer:\nHurricane Frances"], "ground_truth": ["John Kasich", "Return J. Meigs, Jr.", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f\n# Answer:\nm.0_z851f"], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> book.author.works_written -> Coronado's letter to Mendoza, August 3, 1540\n# Answer:\nCoronado's letter to Mendoza, August 3, 1540", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site\n# Answer:\nCoronado Historic Site", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado National Forest\n# Answer:\nCoronado National Forest"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people\n# Answer:\nWhite people"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2\n# Answer:\nFukushima I \u2013 2"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland"], "ground_truth": ["Wales", "Northern Ireland", "England", "Scotland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7999999999999999, "ans_precission": 0.8571428571428571, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.42857142857142855, "path_recall": 0.75, "path_ans_f1": 0.7999999999999999, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.75}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__5\n# Answer:\nm.0wg8__5", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8___\n# Answer:\nm.0wg8___"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently.\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga\n# Answer:\nAlvin Plantinga"], "ground_truth": ["Philosopher", "Physician", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0\n# Answer:\nm.05cqdz0", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> (When You Gonna) Give It Up To Me\n# Answer:\n(When You Gonna) Give It Up To Me", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> Bad Bad Bad\n# Answer:\nBad Bad Bad"], "ground_truth": ["Francine Lons", "Leon Cole", "Sal Gibson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Academic Middle School\n# Answer:\nDr. Martin Luther King, Jr. Academic Middle School", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library\n# Answer:\nDr. Martin Luther King, Jr. Library", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2\n# Answer:\nm.0_714v2", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_\n# Answer:\nm.0gbz10_"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore\n# Answer:\nAlan Moore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.\n# Answer:\nAfter reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41\n# Answer:\nm.03ldb41", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.0542hwn\n# Answer:\nm.0542hwn", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09w0bqz\n# Answer:\nm.09w0bqz"], "ground_truth": ["The Tabernacle", "Fernbank Museum of Natural History", "Zoo Atlanta", "World of Coca-Cola", "Atlanta Cyclorama & Civil War Museum", "Center for Puppetry Arts", "Georgia World Congress Center", "Georgia Aquarium", "Woodruff Arts Center", "Four Seasons Hotel Atlanta", "Arbor Place Mall", "Martin Luther King, Jr. National Historic Site", "Atlanta Ballet", "Atlanta Marriott Marquis", "Georgia Dome", "CNN Center", "Fernbank Science Center", "Underground Atlanta", "Centennial Olympic Park", "Turner Field", "Atlanta Symphony Orchestra", "Fox Theatre", "Georgia State Capitol", "Variety Playhouse", "Hyatt Regency Atlanta", "Atlanta Jewish Film Festival", "Jimmy Carter Library and Museum", "Cobb Energy Performing Arts Centre", "Omni Coliseum", "Six Flags Over Georgia", "Six Flags White Water", "Atlanta History Center", "Masquerade", "Philips Arena", "Margaret Mitchell House & Museum", "Peachtree Road Race"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.13953488372093023, "ans_precission": 0.42857142857142855, "ans_recall": 0.08333333333333333, "path_f1": 0.13953488372093023, "path_precision": 0.42857142857142855, "path_recall": 0.08333333333333333, "path_ans_f1": 0.13953488372093023, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h\n# Answer:\nm.0cr301h"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy\n# Answer:\nComedy", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy-drama\n# Answer:\nComedy-drama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010\n# Answer:\nThursday 10th June 2010"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010pyd6z\n# Answer:\nm.010pyd6z", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010q5kls\n# Answer:\nm.010q5kls", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010qb6kd\n# Answer:\nm.010qb6kd"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gl\n# Answer:\nm.04lt3gl", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gr\n# Answer:\nm.04lt3gr"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr.\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton\n# Answer:\nJackie Newton"], "ground_truth": ["Carolina Panthers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer\n# Answer:\nAlfred G. Mayer", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bazabeel Norman\n# Answer:\nBazabeel Norman", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bert Myers\n# Answer:\nBert Myers"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor (to Kill a Mockingbird)\n# Answer:\nMatar Un Ruisenor (to Kill a Mockingbird)"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010l3l5d\n# Answer:\nm.010l3l5d", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.04kcmn6\n# Answer:\nm.04kcmn6"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx\n# Answer:\nm.079q3lx", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv\n# Answer:\nm.010l29pv", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.0115sdhb\n# Answer:\nm.0115sdhb"], "ground_truth": ["Gene Amondson", "Michael Peroutka", "Ralph Nader", "John Kerry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7\n# Answer:\nm.0h4nmc7", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f\n# Answer:\nm.0h4nq4f", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn\n# Answer:\nm.0pdthbn"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Baltra Island\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Bartolom\u00e9 Island\n# Answer:\nBartolom\u00e9 Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Darwin Island\n# Answer:\nDarwin Island"], "ground_truth": ["Pacific Ocean", "Gal\u00e1pagos Province", "Ecuador"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Mistletoe\n# Answer:\nMistletoe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk\n# Answer:\nm.0115qhzk"], "ground_truth": ["Recovery", "Confident", "Lolly", "Roller Coaster", "Baby", "As Long as You Love Me", "First Dance", "All That Matters", "Somebody to Love", "Die in Your Arms", "Wait for a Minute", "Home to Mama", "Never Let You Go", "Hold Tight", "Pray", "#thatPower", "Never Say Never", "Turn to You (Mother's Day Dedication)", "All Around The World", "Bad Day", "All Bad", "PYD", "Eenie Meenie", "Boyfriend", "Heartbreaker", "Thought Of You", "Live My Life", "Right Here", "Change Me", "Beauty And A Beat", "Bigger"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.10526315789473685, "ans_precission": 0.2857142857142857, "ans_recall": 0.06451612903225806, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.10526315789473685, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau"], "ground_truth": ["Writer", "Statesman", "Physician", "Journalist", "Publisher"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz\n# Answer:\nm.04st6lz", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s\n# Answer:\nm.09w1l1s", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w2hwc\n# Answer:\nm.09w2hwc"], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv\n# Answer:\nm.010_ydxv", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw\n# Answer:\nm.064_ltw"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w\n# Answer:\nm.0cs3c3w", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd\n# Answer:\nm.0948qtd", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094cr65\n# Answer:\nm.094cr65", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs8ydd\n# Answer:\nm.0cs8ydd", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0djcy_v\n# Answer:\nm.0djcy_v"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> God\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat\n# Answer:\nDay of Arafat", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura\n# Answer:\nDay of Ashura"], "ground_truth": ["Prophets in Islam", "Entering Heaven alive", "Masih ad-Dajjal", "\u1e6c\u016bb\u0101", "Tawhid", "Monotheism", "Predestination in Islam", "Mahdi", "Islamic view of angels", "Islamic holy books", "God in Islam", "Sharia", "Qiyamah"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion.\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> \u00c9mile Zola\n# Answer:\n\u00c9mile Zola", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A liberal is a power worshipper without the power.\n# Answer:\nA liberal is a power worshipper without the power.", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A tragic situation exists precisely when virtue does not triumph but when it is still felt that man is nobler than the forces which destroy him.\n# Answer:\nA tragic situation exists precisely when virtue does not triumph but when it is still felt that man is nobler than the forces which destroy him.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley\n# Answer:\nAldous Huxley", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler\n# Answer:\nArthur Koestler"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Gestapo\n# Answer:\nGestapo"], "ground_truth": ["Nazi Germany"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\ng.11b7_lvdf2", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt\n# Answer:\nm.03lpqjt", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t\n# Answer:\nm.09w699t"], "ground_truth": ["Songwriter", "Actor", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111\n# Answer:\n66111", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2001 Protection One 400\n# Answer:\n2001 Protection One 400", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z\n# Answer:\nm.0hpgj2z", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery\n# Answer:\nCypress Hills Cemetery"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln\n# Answer:\nm.07919ln", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4\n# Answer:\nm.07mmjx4", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07sg_z8\n# Answer:\nm.07sg_z8"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Eleanor and Harry\n# Answer:\nEleanor and Harry", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr.\n# Answer:\nFranklin D. Roosevelt, Jr."], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\ng.1245_67l9"], "ground_truth": ["Catholicism", "Protestantism", "Hinduism", "Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_\n# Answer:\nm.03pn4x_", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss\n# Answer:\nm.04hdfss"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm\n# Answer:\nm.011lncpm", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig\n# Answer:\nFezziwig", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet\n# Answer:\nAlphonse Daudet", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Anne Rice\n# Answer:\nAnne Rice"], "ground_truth": ["A Christmas Carol (Apple Classics)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Oxford Bookworms Library)", "Sketches by Boz", "Dombey and Son", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Konemann Classics)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Classic Retelling)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (BBC Audio Series)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "Our mutual friend", "A Tale of Two Cities (Masterworks)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (R)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Take Part)", "Dombey and Son.", "Bleak house", "The cricket on the hearth", "A Christmas Carol (Chrysalis Children's Classics Series)", "The Pickwick papers", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Soundings)", "Dombey and son", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Dramatized)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Enriched Classic)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Wordsworth Classics)", "Martin Chuzzlewit", "David Copperfield", "Great expectations.", "A Christmas Carol (Scholastic Classics)", "The mystery of Edwin Drood", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Prentice Hall Science)", "Hard times", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "Great Expectations.", "A Christmas Carol", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (The Classic Collection)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Value Books)", "A Christmas Carol (Classics Illustrated)", "A CHRISTMAS CAROL", "The Old Curiosity Shop", "The old curiosity shop", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (Progressive English)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Audio Editions)", "Our mutual friend.", "Great expectations", "Bleak House", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "Little Dorrit", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "Oliver Twist", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Longman Classics, Stage 2)", "The Pickwick Papers", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Cp 1135)", "The old curiosity shop.", "A Tale of Two Cities (Adopted Classic)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Piccolo Books)", "Great Expectations", "Bleak House.", "A Tale of Two Cities (Cyber Classics)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Bookcassette(r) Edition)", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Cover to Cover Classics)", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities", "A Christmas Carol (Puffin Choice)", "David Copperfield."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf\n# Answer:\nm.049y3kf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya\n# Answer:\n1940\u201344 insurgency in Chechnya"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp.\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions\n# Answer:\nFrontpoint Security Solutions", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Hair Club\n# Answer:\nHair Club"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Young Blood\n# Answer:\nYoung Blood", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> A Scottish Soldier\n# Answer:\nA Scottish Soldier", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne\n# Answer:\nAul Lang Syne"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk\n# Answer:\nm.03lj4lk", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1\n# Answer:\nKnight Rider - Season 1"], "ground_truth": ["William Daniels"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> location.location.contains -> Alexander Smith House\n# Answer:\nAlexander Smith House", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\ng.11x1ddsd6", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1fr8dg\n# Answer:\ng.11x1fr8dg", "# Reasoning Path:\nBrentwood -> location.location.contains -> Andrew Crockett House\n# Answer:\nAndrew Crockett House"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist\n# Answer:\nBotanist", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy\n# Answer:\nCarver Academy", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Chemist\n# Answer:\nChemist", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Inventor\n# Answer:\nInventor"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg\n# Answer:\nm.07nvttg", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt\n# Answer:\nm.07nvtvt", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtwp\n# Answer:\nm.07nvtwp"], "ground_truth": ["Tracy Pollan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd\n# Answer:\nm.04fv9nd", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam\n# Answer:\nBattle of Antietam", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain\n# Answer:\nBattle of Cedar Mountain"], "ground_truth": ["Jackson's Valley Campaign", "Battle of Hancock", "First Battle of Rappahannock Station", "First Battle of Kernstown", "Battle of Port Republic", "American Civil War", "Romney Expedition", "Battle of Chancellorsville", "Battle of Hoke's Run", "Battle of White Oak Swamp", "Manassas Station Operations", "How Few Remain", "Battle of Front Royal", "Battle of McDowell", "Second Battle of Bull Run", "Battle of Harpers Ferry", "Battle of Chantilly", "First Battle of Winchester", "Battle of Cedar Mountain"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.23076923076923078, "ans_precission": 0.42857142857142855, "ans_recall": 0.15789473684210525, "path_f1": 0.11764705882352941, "path_precision": 0.2857142857142857, "path_recall": 0.07407407407407407, "path_ans_f1": 0.23076923076923078, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006\n# Answer:\nPatrick Swayze 2006", "# Reasoning Path:\nPatrick Swayze -> tv.tv_program_guest.appeared_on -> m.0j7p8gc\n# Answer:\nm.0j7p8gc", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2\n# Answer:\nSwayze2"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation\n# Answer:\nAnnunciation"], "ground_truth": ["Portrait of a Young Fianc\u00e9e", "Madonna and Child with St Joseph", "Ginevra de' Benci", "Horse and Rider", "The Virgin and Child with St Anne and St John the Baptist", "Benois Madonna", "Lucan portrait of Leonardo da Vinci", "Madonna of Laroque", "Vitruvian Man", "Medusa", "g.1219sb0g", "The Baptism of Christ", "Madonna of the Carnation", "Portrait of Isabella d'Este", "Salvator Mundi", "Madonna of the Yarnwinder", "Bacchus", "Leda and the Swan", "g.1239jd9p", "Annunciation", "Portrait of a man in red chalk", "g.121wt37c", "Virgin of the Rocks", "g.1213jb_b", "Mona Lisa", "La belle ferronni\u00e8re", "g.121yh91r", "The Virgin and Child with St. Anne", "Leonardo's horse", "Madonna Litta", "g.120vt1gz", "Sala delle Asse", "Portrait of a Musician", "St. John the Baptist", "g.1224tf0c", "Drapery for a Seated Figure", "g.12215rxg", "g.12314dm1", "Adoration of the Magi", "Head of a Woman", "The Holy Infants Embracing", "St. Jerome in the Wilderness", "Lady with an Ermine", "The Last Supper", "The Battle of Anghiari"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.11538461538461539, "ans_precission": 0.42857142857142855, "ans_recall": 0.06666666666666667, "path_f1": 0.1176470588235294, "path_precision": 0.42857142857142855, "path_recall": 0.06818181818181818, "path_ans_f1": 0.11538461538461539, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia\n# Answer:\nCarinthia"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Discursos completos\n# Answer:\nDiscursos completos", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Essays\n# Answer:\nEssays", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Eva Per\u00f3n habla a las mujeres\n# Answer:\nEva Per\u00f3n habla a las mujeres"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism\n# Answer:\nBuddhism"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Glass harmonica", "Bifocals", "Franklin stove", "Lightning rod"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.5, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.location.contains -> Aims Community College\n# Answer:\nAims Community College", "# Reasoning Path:\nGreeley -> location.location.contains -> Bank of Colorado Arena\n# Answer:\nBank of Colorado Arena"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Charode\u01d0ka\n# Answer:\nCharode\u01d0ka", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Classic Bits & Pieces\n# Answer:\nClassic Bits & Pieces"], "ground_truth": ["Composer", "Librettist", "Musician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7\n# Answer:\nm.0102xvg7", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0103v73t\n# Answer:\nm.0103v73t", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0127rg0f\n# Answer:\nm.0127rg0f"], "ground_truth": ["Germany", "East Germany", "Belgium", "Switzerland", "Austria", "Luxembourg", "Liechtenstein"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.48979591836734687, "ans_precission": 0.42857142857142855, "ans_recall": 0.5714285714285714, "path_f1": 0.42857142857142855, "path_precision": 0.42857142857142855, "path_recall": 0.42857142857142855, "path_ans_f1": 0.48979591836734687, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock\n# Answer:\nArt rock", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies\n# Answer:\n181-greatoldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Lite)\n# Answer:\n1Club.FM: 70s (Lite)"], "ground_truth": ["Rock music", "Experimental music", "Experimental rock", "Pop rock", "Soft rock", "Psychedelic rock", "Pop music", "Art rock", "Blues rock"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 0.42857142857142855, "ans_recall": 0.3333333333333333, "path_f1": 0.375, "path_precision": 0.42857142857142855, "path_recall": 0.3333333333333333, "path_ans_f1": 0.375, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2010\n# Answer:\nUnited States Senate election in Colorado, 2010", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment\n# Answer:\n1st Colorado Infantry Regiment"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc38qmq\n# Answer:\ng.1hhc38qmq", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc390kr\n# Answer:\ng.1hhc390kr"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.location.containedby -> Area code 206\n# Answer:\nArea code 206", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.location.containedby -> King County\n# Answer:\nKing County"], "ground_truth": ["98161", "98144", "98148", "98131", "98199", "98168", "98185", "98160", "98114", "98158", "98102", "98113", "98122", "98109", "98188", "98112", "98136", "98155", "98171", "98127", "98198", "98124", "98194", "98134", "98166", "98146", "98118", "98138", "98119-4114", "98175", "98133", "98165", "98101", "98108", "98191", "98121", "98139", "98107", "98170", "98119", "98106", "98174", "98190", "98105", "98117", "98115", "98184", "98129", "98111", "98178", "98145", "98195", "98154", "98164", "98116", "98125", "98177", "98181", "98126", "98103", "98141", "98132", "98104"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08571428571428572, "ans_precission": 0.42857142857142855, "ans_recall": 0.047619047619047616, "path_f1": 0.08571428571428572, "path_precision": 0.42857142857142855, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08571428571428572, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> common.topic.webpage -> m.0cq8blb\n# Answer:\nm.0cq8blb", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance music\n# Answer:\nDance music", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance-pop\n# Answer:\nDance-pop", "# Reasoning Path:\nWillow Smith -> common.topic.webpage -> m.0d_tv_c\n# Answer:\nm.0d_tv_c"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1\n# Answer:\nm.0104b7h1", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02t0c71\n# Answer:\nm.02t0c71", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02wn10k\n# Answer:\nm.02wn10k"], "ground_truth": ["Simplified Chinese character", "Chinese characters", "N\u00fcshu script", "Traditional Chinese characters", "'Phags-pa script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.42857142857142855, "ans_recall": 0.6, "path_f1": 0.5, "path_precision": 0.42857142857142855, "path_recall": 0.6, "path_ans_f1": 0.5, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_\n# Answer:\nm.0_0cs2_"], "ground_truth": ["Pat Nixon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdh\n# Answer:\nm.03lkkdh", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdn\n# Answer:\nm.03lkkdn", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdt\n# Answer:\nm.03lkkdt", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Sherman Hemsley", "Roxie Roker", "Jay Hammer", "Franklin Cover", "Paul Benedict", "Zara Cully", "Marla Gibbs", "Damon Evans", "Isabel Sanford", "Berlinda Tolbert", "Mike Evans"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek\n# Answer:\nAsianWeek", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die\n# Answer:\n1st to Die", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 2nd Chance\n# Answer:\n2nd Chance"], "ground_truth": ["San Francisco Business Times", "The Golden Era", "The San Francisco Examiner", "Dock of the Bay", "San Francisco Bay Guardian", "Bay Area Reporter", "San Francisco Daily", "California Star", "San Francisco Bay View", "Synapse", "San Francisco Bay Times", "Street Sheet", "The Daily Alta California", "San Francisco Foghorn", "San Francisco Chronicle", "AsianWeek", "Free Society", "San Francisco News-Call Bulletin Newspaper", "San Francisco Call", "Sing Tao Daily"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.42857142857142855, "ans_recall": 0.15, "path_f1": 0.2222222222222222, "path_precision": 0.42857142857142855, "path_recall": 0.15, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales\n# Answer:\nRoman Catholic Archdiocese of Manizales"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Be a Man\n# Answer:\nBe a Man", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Feel the Madness\n# Answer:\nFeel the Madness", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Get Back\n# Answer:\nGet Back"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin\n# Answer:\nCharles Waring Darwin"], "ground_truth": ["Letters from C. Darwin, Esq., to A. Hancock, Esq", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The voyage of Charles Darwin", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Memorias y epistolario i\u0301ntimo", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Correspondence of Charles Darwin, Volume 8: 1860", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "La vie et la correspondance de Charles Darwin", "The\u0301orie de l'e\u0301volution", "Voyage d'un naturaliste autour du monde", "Wu zhong qi yuan", "The foundations of the Origin of species", "Origins", "The Structure and Distribution of Coral Reefs", "The principal works", "Reise eines Naturforschers um die Welt", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Motsa ha-minim", "Darwin and Henslow", "The Correspondence of Charles Darwin, Volume 14: 1866", "Opsht\u0323amung fun menshen", "Questions about the breeding of animals", "Leben und Briefe von Charles Darwin", "The Life and Letters of Charles Darwin Volume 1", "Darwin's notebooks on transmutation of species", "The Darwin Reader Second Edition", "The Orgin of Species", "On the tendency of species to form varieties", "The collected papers of Charles Darwin", "Notebooks on transmutation of species", "Evolutionary Writings: Including the Autobiographies", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Die fundamente zur entstehung der arten", "Diary of the voyage of H.M.S. Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Cartas de Darwin 18251859", "Darwin's Ornithological notes", "Kleinere geologische Abhandlungen", "Fertilisation of Orchids", "Darwin's insects", "The education of Darwin", "Del Plata a Tierra del Fuego", "A Darwin Selection", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Metaphysics, Materialism, & the evolution of mind", "South American Geology", "Geological Observations on the Volcanic Islands", "The Life and Letters of Charles Darwin Volume 2", "ontstaan der soorten door natuurlijke teeltkeus", "genese\u014ds t\u014dn eid\u014dn", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 11: 1863", "On a remarkable bar of sandstone off Pernambuco", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin-Wallace", "The portable Darwin", "Les moyens d'expression chez les animaux", "Charles Darwin's letters", "Charles Darwin on the routes of male humble bees", "Volcanic Islands", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "On the origin of species by means of natural selection", "A student's introduction to Charles Darwin", "Evolution and natural selection", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Essential Darwin", "More Letters of Charles Darwin", "Works", "monograph on the sub-class Cirripedia", "The Power of Movement in Plants", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "On evolution", "Tesakneri tsagume\u030c", "Part I: Contributions to the Theory of Natural Selection / Part II", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The living thoughts of Darwin", "H.M.S. Beagle in South America", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 15: 1867", "Evolution by natural selection", "Darwin on humus and the earthworm", "Charles Darwin's natural selection", "Proiskhozhdenie vidov", "Insectivorous Plants", "On the Movements and Habits of Climbing Plants", "Charles Darwin's marginalia", "Darwin from Insectivorous Plants to Worms", "Gesammelte kleinere Schriften", "Les mouvements et les habitudes des plantes grimpantes", "The geology of the voyage of H.M.S. Beagle", "To the members of the Down Friendly Club", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "La facult\u00e9 motrice dans les plantes", "From so simple a beginning", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Autobiography of Charles Darwin", "From Darwin's unpublished notebooks", "Die geschlechtliche Zuchtwahl", "El Origin De Las Especies", "Reise um die Welt 1831 - 36", "Darwinism stated by Darwin himself", "Human nature, Darwin's view", "Notes on the fertilization of orchids", "Darwin for Today", "Geological Observations on South America", "The Voyage of the Beagle", "Resa kring jorden", "Darwin Darwin", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin", "The Correspondence of Charles Darwin, Volume 10: 1862", "Darwin's journal", "Beagle letters", "The Life of Erasmus Darwin", "Darwin Compendium", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Evolution", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "On Natural Selection", "The Expression of the Emotions in Man and Animals", "Rejse om jorden", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The action of carbonate of ammonia on the roots of certain plants", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Darwin Reader First Edition", "Darwin en Patagonia", "The Descent of Man, and Selection in Relation to Sex", "The Variation of Animals and Plants under Domestication", "The Correspondence of Charles Darwin, Volume 18: 1870"], "ans_acc": 0.0196078431372549, "ans_hit": 1, "ans_f1": 0.025559105431309907, "ans_precission": 0.5714285714285714, "ans_recall": 0.013071895424836602, "path_f1": 0.07142857142857142, "path_precision": 1.0, "path_recall": 0.037037037037037035, "path_ans_f1": 0.038461538461538464, "path_ans_precision": 1.0, "path_ans_recall": 0.0196078431372549}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5\n# Answer:\nm.09nsgl5", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_\n# Answer:\nm.0_0cs2_", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb\n# Answer:\nm.09nsglb", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh\n# Answer:\nm.09nsglh"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All\n# Answer:\nAfter All"], "ground_truth": ["God Rest Ye Merry Gentlemen", "Be Kind To The Growing Mind (with The Temptations)", "Easy", "You Really Got a Hold on Me", "When Smokey Sings Tears Of A Clown", "Tracks of My Tears", "Don't Play Another Love Song", "It's Christmas Time", "No\u00ebl", "Unless You Do It Again", "Baby Come Close", "Love Don't Give No Reason", "Pops, We Love You (disco)", "Nearness of You", "A Tattoo", "Take Me Through The Night", "Aqui Con Tigo (Being With You)", "Love' n Life", "Why", "Please Don't Take Your Love (feat. Carlos Santana)", "Driving Thru Life in the Fast Lane", "Food For Thought", "Don't Wanna Be Just Physical", "Time After Time", "You Take Me Away", "Because of You It's the Best It's Ever Been", "Pops, We Love You", "Fulfill Your Need", "Girlfriend", "I Love The Nearness Of You", "Our Love Is Here to Stay", "You Made Me Feel Love", "Daylight & Darkness", "Tracks Of My Tears (Live)", "Vitamin U", "Girl I'm Standing There", "I Can\u2019t Stand to See You Cry (Commercial version)", "I Want You Back", "Will You Love Me Tomorrow?", "Noel", "The Road to Damascus", "Wanna Know My Mind", "Don't Know Why", "Night and Day", "Being With You", "The Hurt's On You", "The Agony and the Ecstasy", "Jingle Bells", "Love Letters", "Will You Love Me Tomorrow", "Coincidentally", "Time Flies", "Why Do Happy Memories Hurt So Bad", "If You Want My Love", "I Can't Give You Anything but Love", "I Can't Find", "Wishful Thinking", "Tea for Two", "Ooh Baby Baby", "Who's Sad", "You're the One for Me (feat. Joss Stone)", "Happy (Love Theme From Lady Sings the Blues)", "Quiet Storm (single version)", "The Tears of a Clown", "Be Kind to the Growing Mind", "My Girl", "Love Is The Light", "Jesus Told Me To Love You", "Open", "I've Made Love to You a Thousand Times", "A Silent Partner in a Three-Way Love Affair", "A Child Is Waiting", "One Time", "I've Made Love To You A Thousand Times", "Yester Love", "Come to Me Soon", "Just a Touch Away", "Tears of a Clown", "So Bad", "If You Can Want", "You Cannot Laugh Alone", "My World", "Let Me Be The Clock", "I Can't Get Enough", "Mickey's Monkey", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "I Am, I Am", "Sweet Harmony", "(It's The) Same Old Love", "Winter Wonderland", "Cruisin'", "You're Just My Life (feat. India.Arie)", "Little Girl Little Girl", "The Love Between Me and My Kids", "More Love", "The Tracks of My Tears", "The Tracks of My Tears (live)", "The Tracks Of My Tears", "Ever Had A Dream", "It's A Good Night", "Shoe Soul", "No Time to Stop Believing", "Really Gonna Miss You", "Jasmin", "Hold on to Your Love", "Christmas Every Day", "Just Passing Through", "I Second That Emotion", "Be Careful What You Wish For (instrumental)", "Away in the Manger / Coventry Carol", "And I Don't Love You", "The Way You Do (The Things You Do)", "You've Really Got a Hold on Me", "Yes It's You Lady", "I Hear The Children Singing", "More Than You Know", "Love Brought Us Here", "Tell Me Tomorrow (12\\\" extended mix)", "Going to a Go-Go", "I\u2019ve Got You Under My Skin", "Be Careful What You Wish For", "Same Old Love", "Going to a Go Go", "You Don't Know What It's Like", "Tell Me Tomorrow", "Skid Row", "Did You Know (Berry's Theme)", "Ooo Baby Baby", "Fallin'", "Never My Love / Never Can Say Goodbye", "Going to a Gogo", "Let Me Be the Clock", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "He Can Fix Anything", "Sleepless Nights", "Close Encounters of the First Kind", "The Family Song", "Baby That's Backatcha", "As You Do", "Holly", "Walk on By", "We Are The Warriors", "Blame It On Love (Duet with Barbara Mitchell)", "Tell Me Tomorrow, Part 1", "My Guy", "Theme From the Big Time", "Quiet Storm (Groove Boutique remix)", "Christmas Greeting", "Just My Soul Responding", "Gang Bangin'", "Train of Thought", "Please Come Home for Christmas", "Share It", "Crusin", "Some People Will Do Anything for Love", "Double Good Everything", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Be Who You Are", "The Agony And The Ecstasy", "Christmas Everyday", "I Have Prayed On It", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Cruisin", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "I Second That Emotions", "Quiet Storm", "Everything You Touch", "Gone Forever", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "There Will Come a Day (I'm Gonna Happen to You)", "Crusin'", "It's Fantastic", "That Place", "Little Girl, Little Girl", "Te Quiero Como Si No Hubiera Un Manana", "Get Ready", "The Tears Of A Clown", "Bad Girl", "I Know You by Heart", "And I Don't Love You (Larry Levan instrumental dub)", "Mother's Son", "Virgin Man", "Ebony Eyes (Duet with Rick James)", "Season's Greetings from Smokey Robinson", "Love Bath", "Shop Around", "She's Only a Baby Herself", "You Go to My Head", "Love Don' Give No Reason (12 Inch Club Mix)", "Tears of a Sweet Free Clown", "Just Another Kiss", "Just To See Her Again", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Everything for Christmas", "We've Saved the Best for Last", "There Will Come A Day ( I'm Gonna Happen To You )", "The Track of My Tears", "Heavy On Pride (Light On Love)", "Satisfy You", "Just to See Her", "In My Corner", "You Are So Beautiful (feat. Dave Koz)", "You Are Forever", "I Care About Detroit", "Melody Man", "What's Too Much", "I'll Keep My Light In My Window", "The Christmas Song", "Let Your Light Shine On Me", "You've Really Go a Hold on Me", "I Like Your Face", "It's a Good Feeling", "I Praise & Worship You Father", "Can't Fight Love", "If You Wanna Make Love (Come 'round Here)", "Fly Me to the Moon (In Other Words)", "Come by Here (Kum Ba Ya)", "It's Time to Stop Shoppin' Around", "Love So Fine", "Keep Me", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Ooo Baby Baby (live)", "Tears Of A Clown", "Medley: Never My Love / Never Can Say Goodbye", "I've Got You Under My Skin", "Speak Low", "Wedding Song", "Photograph in My Mind", "One Heartbeat", "Hanging on by a Thread", "And I Love Her", "Will You Still Love Me Tomorrow", "I'm Glad There Is You", "Rack Me Back", "I Am I Am", "We\u2019ve Come Too Far to End It Now", "Save Me", "If You Wanna Make Love", "Why Are You Running From My Love", "Just Like You", "I Love Your Face", "Rewind", "When A Woman Cries", "Blame It on Love", "The Tracks of My Heart", "I'm in the Mood for Love", "Tracks of my Tears", "Whatcha Gonna Do", "With Your Love Came", "Deck the Halls", "Standing On Jesus", "Santa Claus is Coming to Town", "Ain't That Peculiar", "Asleep on My Love", "It's Her Turn to Live", "Ebony Eyes"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Hungarian language\n# Answer:\nHungarian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Montenegrin language\n# Answer:\nMontenegrin language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}

{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.6666666666666666, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kc", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh"], "ground_truth": ["Speaker of the United States House of Representatives", "United States Representative", "Governor of Tennessee"], "effective_topk": 3, "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444444, "path_precision": 0.6666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4444444444444444, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench"], "ground_truth": ["French", "Haitian Creole"], "effective_topk": 3, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5714285714285715, "path_precision": 0.6666666666666666, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nm.02t91b7"], "ground_truth": ["Ilyssa Fradin", "Hannah Gunn", "Melinda McGraw"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qcbdr", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9532"], "ground_truth": ["Cleveland Cavaliers", "Orlando Magic", "Los Angeles Lakers", "Boston Celtics", "Miami Heat", "Phoenix Suns", "LSU Tigers men's basketball"], "effective_topk": 7, "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.21621621621621623, "path_precision": 0.5714285714285714, "path_recall": 0.13333333333333333, "path_ans_f1": 0.38095238095238093, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American"], "ground_truth": ["New Rochelle"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.countries_spoken_in -> Laos\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.region -> Asia\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Akha Language", "Mlabri Language", "Thai Language", "Lao Language", "Phu Thai language", "Khmer language", "Malay, Pattani Language", "Saek language", "Hmong language", "Cham language", "Mon Language", "Vietnamese Language", "Nyaw Language"], "effective_topk": 13, "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.45859872611464964, "ans_precission": 0.9, "ans_recall": 0.3076923076923077, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.45859872611464964, "path_ans_precision": 0.9, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Book -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer -> common.topic.notable_types -> Profession\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Book -> freebase.type_hints.included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist"], "ground_truth": ["Writer", "Farmer", "Statesman", "Archaeologist", "Philosopher", "Architect", "Inventor", "Lawyer", "Teacher", "Author"], "effective_topk": 10, "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.41379310344827586, "ans_precission": 0.6666666666666666, "ans_recall": 0.3, "path_f1": 0.2758620689655173, "path_precision": 0.4444444444444444, "path_recall": 0.2, "path_ans_f1": 0.41379310344827586, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.3}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> London\n# Answer:\nGreat Britain"], "ground_truth": ["The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Descent of Man, and Selection in Relation to Sex", "Autobiography of Charles Darwin", "Charles Darwin's natural selection", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Correspondence of Charles Darwin, Volume 8", "The Origin of Species (Oxford World's Classics)", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Correspondence of Charles Darwin, Volume 13", "The Origin Of Species", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The origin of species", "The Origin of Species", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 15", "Darwin for Today", "Questions about the breeding of animals", "The Correspondence of Charles Darwin, Volume 5", "The Voyage of the Beagle (Great Minds Series)", "Darwin Darwin", "The action of carbonate of ammonia on the roots of certain plants", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The descent of man, and selection in relation to sex.", "Leben und Briefe von Charles Darwin", "Volcanic Islands", "The Voyage of the Beagle", "Darwin's insects", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The portable Darwin", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die geschlechtliche Zuchtwahl", "H.M.S. Beagle in South America", "The structure and distribution of coral reefs.", "The Voyage of the Beagle (Adventure Classics)", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Voyage of the Beagle (Mentor)", "The Origin of Species (World's Classics)", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 6", "Darwin Compendium", "Metaphysics, Materialism, & the evolution of mind", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Correspondence of Charles Darwin, Volume 13: 1865", "Darwinism stated by Darwin himself", "The expression of the emotions in man and animals", "Charles Darwin's letters", "From so simple a beginning", "The Origin of Species (Collector's Library)", "Notebooks on transmutation of species", "vari\u00eberen der huisdieren en cultuurplanten", "Darwin en Patagonia", "The autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 4", "Tesakneri tsagume\u030c", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "A Darwin Selection", "Notes on the fertilization of orchids", "red notebook of Charles Darwin", "More Letters of Charles Darwin", "Memorias y epistolario i\u0301ntimo", "Human nature, Darwin's view", "Voyage of the Beagle (Dover Value Editions)", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "La vie et la correspondance de Charles Darwin", "The Darwin Reader First Edition", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Orgin of Species", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 12: 1864", "Charles Darwin on the routes of male humble bees", "To the members of the Down Friendly Club", "Reise um die Welt 1831 - 36", "Darwin on humus and the earthworm", "ontstaan der soorten door natuurlijke teeltkeus", "The voyage of the Beagle.", "The Correspondence of Charles Darwin, Volume 18: 1870", "The collected papers of Charles Darwin", "Insectivorous Plants", "Evolution by natural selection", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Origin of Species (Great Minds Series)", "Motsa ha-minim", "The descent of man and selection in relation to sex.", "The education of Darwin", "Darwin's notebooks on transmutation of species", "Origin of Species (Harvard Classics, Part 11)", "The Origin of Species (Great Books : Learning Channel)", "monograph on the sub-class Cirripedia", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "genese\u014ds t\u014dn eid\u014dn", "Darwin", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Die fundamente zur entstehung der arten", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Structure And Distribution of Coral Reefs", "Origin of Species (Everyman's University Paperbacks)", "On the Movements and Habits of Climbing Plants", "The Descent of Man and Selection in Relation to Sex", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 9", "Origins", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Het uitdrukken van emoties bij mens en dier", "Les mouvements et les habitudes des plantes grimpantes", "The Different Forms of Flowers on Plants of the Same Species", "Diary of the voyage of H.M.S. Beagle", "The Voyage of the Beagle (Everyman Paperbacks)", "The Autobiography of Charles Darwin (Large Print)", "The Autobiography Of Charles Darwin", "The geology of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Kleinere geologische Abhandlungen", "The Correspondence of Charles Darwin, Volume 14: 1866", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Cartas de Darwin 18251859", "Voyage Of The Beagle", "Darwin and Henslow", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 17: 1869", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The autobiography of Charles Darwin, 1809-1882", "Fertilisation of Orchids", "Geological Observations on South America", "On Natural Selection", "The Life of Erasmus Darwin", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Origin of Species", "The Autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 1", "Les moyens d'expression chez les animaux", "The Origin of Species (Mentor)", "From So Simple a Beginning", "Evolution and natural selection", "Works", "On the tendency of species to form varieties", "Part I: Contributions to the Theory of Natural Selection / Part II", "The structure and distribution of coral reefs", "Voyage of the Beagle (Harvard Classics, Part 29)", "On evolution", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 15: 1867", "Diario del Viaje de Un Naturalista Alrededor", "The Origin of Species (Enriched Classics)", "The voyage of Charles Darwin", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 12", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Expression of the Emotions in Man And Animals", "The principal works", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 2", "The Autobiography of Charles Darwin [EasyRead Edition]", "Proiskhozhdenie vidov", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The\u0301orie de l'e\u0301volution", "The living thoughts of Darwin", "The Correspondence of Charles Darwin, Volume 14", "The Essential Darwin", "Voyage d'un naturaliste autour du monde", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Resa kring jorden", "The Expression Of The Emotions In Man And Animals", "Opsht\u0323amung fun menshen", "The Autobiography of Charles Darwin (Great Minds Series)", "The expression of the emotions in man and animals.", "The Correspondence of Charles Darwin, Volume 10", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 9: 1861", "Reise eines Naturforschers um die Welt", "The Voyage of the Beagle (Unabridged Classics)", "Darwin-Wallace", "Charles Darwin's marginalia", "Wu zhong qi yuan", "Gesammelte kleinere Schriften", "Voyage of the Beagle", "The descent of man, and selection in relation to sex", "The Correspondence of Charles Darwin, Volume 11", "Voyage of the Beagle (NG Adventure Classics)", "Beagle letters", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "A student's introduction to Charles Darwin", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "Darwin's Ornithological notes", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The origin of species : complete and fully illustrated", "The Correspondence of Charles Darwin, Volume 3", "The Autobiography of Charles Darwin (Dodo Press)", "El Origin De Las Especies", "The Variation of Animals and Plants under Domestication", "The Correspondence of Charles Darwin, Volume 7", "On a remarkable bar of sandstone off Pernambuco", "The foundations of the Origin of species", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Structure and Distribution of Coral Reefs", "Evolution", "Rejse om jorden", "The Autobiography of Charles Darwin, and selected letters"], "effective_topk": 214, "ans_acc": 0.06542056074766354, "ans_hit": 1, "ans_f1": 0.018306636155606407, "ans_precission": 0.4444444444444444, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.12280701754385964, "path_ans_precision": 1.0, "path_ans_recall": 0.06542056074766354}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nm.04nb7z0"], "ground_truth": ["Florida Gators football"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nm.07mmh5w"], "ground_truth": ["Denver Broncos"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter"], "ground_truth": ["Author", "Writer", "Poet", "Bard"], "effective_topk": 4, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf"], "ground_truth": ["Hayden Christensen"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b"], "ground_truth": ["Super Bowl XLI"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada"], "ground_truth": ["Canada"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05ckldy\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.locations -> Iraqi Kurdistan\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Iraq", "Saudi Arabia", "United Kingdom", "United States of America", "France", "Australia", "Argentina"], "effective_topk": 7, "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.19047619047619047, "ans_precission": 0.2857142857142857, "ans_recall": 0.14285714285714285, "path_f1": 0.125, "path_precision": 0.5714285714285714, "path_recall": 0.07017543859649122, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ym"], "ground_truth": ["Brenda Song"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr"], "ground_truth": ["Return J. Meigs, Jr.", "Ted Strickland", "John Kasich"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9021c"], "ground_truth": ["LA Galaxy"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.3333333333333333, "path_recall": 0.14285714285714285, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca"], "ground_truth": ["Salamanca"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald -> people.person.children -> Eunice Fitzgerald\n# Answer:\nJohn F. Fitzgerald"], "ground_truth": ["Lyndon B. Johnson"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma"], "ground_truth": ["Okuma", "Japan"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales"], "ground_truth": ["Scotland", "England", "Wales", "Northern Ireland"], "effective_topk": 4, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone"], "ground_truth": ["Central Time Zone"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Philosopher", "Writer", "Physician"], "effective_topk": 3, "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.6666666666666666, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole"], "ground_truth": ["Leon Cole", "Sal Gibson", "Francine Lons"], "effective_topk": 3, "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system"], "ground_truth": ["Semi-presidential system", "Provisional government"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis"], "ground_truth": ["Memphis"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> influence.influence_node.influenced_by -> Arthur Conan Doyle\n# Answer:\nAgatha Christie"], "ground_truth": ["Baltimore"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.geolocation -> m.0clwfck\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> g.11b7v_3l1h\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome"], "ground_truth": ["Martin Luther King, Jr. National Historic Site", "Jimmy Carter Library and Museum", "Hyatt Regency Atlanta", "Margaret Mitchell House & Museum", "Atlanta Ballet", "Philips Arena", "Six Flags White Water", "Masquerade", "The Tabernacle", "Four Seasons Hotel Atlanta", "CNN Center", "Atlanta Jewish Film Festival", "Cobb Energy Performing Arts Centre", "Turner Field", "Underground Atlanta", "Georgia Aquarium", "Omni Coliseum", "Woodruff Arts Center", "Atlanta History Center", "Georgia State Capitol", "Fernbank Science Center", "Atlanta Symphony Orchestra", "Atlanta Marriott Marquis", "Arbor Place Mall", "World of Coca-Cola", "Centennial Olympic Park", "Fernbank Museum of Natural History", "Variety Playhouse", "Six Flags Over Georgia", "Georgia World Congress Center", "Center for Puppetry Arts", "Atlanta Cyclorama & Civil War Museum", "Fox Theatre", "Zoo Atlanta", "Peachtree Road Race", "Georgia Dome"], "effective_topk": 36, "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15238095238095237, "ans_precission": 0.8888888888888888, "ans_recall": 0.08333333333333333, "path_f1": 0.15238095238095237, "path_precision": 0.8888888888888888, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19753086419753085, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick"], "ground_truth": ["Electoral district of South Brisbane"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren"], "ground_truth": ["William Roache"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow"], "ground_truth": ["2005"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island"], "ground_truth": ["AUD"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4"], "ground_truth": ["Central European Time Zone"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpgnsj"], "ground_truth": ["Carolina Panthers"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland"], "ground_truth": ["Frederick County"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone"], "ground_truth": ["Mountain Time Zone"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.ethnicity -> White people\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2"], "ground_truth": ["Gene Amondson", "John Kerry", "Ralph Nader", "Michael Peroutka"], "effective_topk": 4, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57"], "ground_truth": ["Ayaan Hirsi Ali"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador"], "ground_truth": ["Ecuador", "Pacific Ocean", "Gal\u00e1pagos Province"], "effective_topk": 3, "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Andre Harris\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.album.release_type -> Single\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All That Matters -> music.composition.composer -> Andre Harris\n# Answer:\nAll That Matters", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All That Matters -> award.award_nominated_work.award_nominations -> m.0_vw6nn\n# Answer:\nAll That Matters", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx"], "ground_truth": ["Bigger", "Somebody to Love", "Right Here", "Die in Your Arms", "Turn to You (Mother's Day Dedication)", "Lolly", "Live My Life", "#thatPower", "Wait for a Minute", "Change Me", "Beauty And A Beat", "Baby", "Roller Coaster", "As Long as You Love Me", "PYD", "Heartbreaker", "All Bad", "All Around The World", "Pray", "All That Matters", "Never Say Never", "First Dance", "Boyfriend", "Confident", "Recovery", "Thought Of You", "Bad Day", "Eenie Meenie", "Hold Tight", "Home to Mama", "Never Let You Go"], "effective_topk": 31, "ans_acc": 0.12903225806451613, "ans_hit": 1, "ans_f1": 0.16901408450704225, "ans_precission": 0.6666666666666666, "ans_recall": 0.0967741935483871, "path_f1": 0.056338028169014086, "path_precision": 0.2222222222222222, "path_recall": 0.03225806451612903, "path_ans_f1": 0.22134387351778656, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.12903225806451613}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist"], "ground_truth": ["Publisher", "Writer", "Journalist", "Statesman", "Physician"], "effective_topk": 5, "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.6, "ans_recall": 0.4, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06"], "ground_truth": ["Saguaro"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados"], "ground_truth": ["Saint Michael Parish"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt"], "ground_truth": ["1841-03-04"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills"], "ground_truth": ["Mission Hills"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.geolocation -> m.0239ks3\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> common.topic.notable_types -> Organization founder -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization founder"], "ground_truth": ["Suwon"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir"], "ground_truth": ["Mahdi", "Prophets in Islam", "Sharia", "God in Islam", "Monotheism", "Masih ad-Dajjal", "Tawhid", "Predestination in Islam", "Entering Heaven alive", "Islamic view of angels", "Islamic holy books", "\u1e6c\u016bb\u0101", "Qiyamah"], "effective_topk": 13, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2"], "ground_truth": ["Jamie Dornan"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis"], "ground_truth": ["Tuberculosis"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.0pz073c"], "ground_truth": ["Nazi Germany"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.6666666666666666, "path_recall": 0.18181818181818182, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist -> freebase.type_hints.included_types -> Topic\n# Answer:\nMusical Artist"], "ground_truth": ["Singer", "Actor", "Songwriter"], "effective_topk": 3, "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.6666666666666666, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> sports.sports_team_roster.team -> Kansas City Monarchs\n# Answer:\nm.0ncxlxp", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0ncxlxp", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nm.0gggrzr"], "ground_truth": ["UCLA Bruins football"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City"], "ground_truth": ["1976-08-10"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile"], "ground_truth": ["Mobile"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan"], "ground_truth": ["Manhattan"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z"], "ground_truth": ["Hinduism", "Protestantism", "Islam", "Catholicism"], "effective_topk": 4, "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666665, "path_precision": 0.75, "path_recall": 0.6, "path_ans_f1": 0.75, "path_ans_precision": 0.75, "path_ans_recall": 0.75}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subjects -> Ammunition belt\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subjects -> Springfield Armory, Inc.\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> symbols.namesake.named_after -> Andrew Jackson\n# Answer:\nAndrew Johnson"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> book.book_character.appears_in_book -> Great Expectations\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alfred Lammle -> book.book_character.appears_in_book -> Our Mutual Friend\n# Answer:\nAlfred Lammle", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> Gustave Flaubert\n# Answer:\nAlphonse Daudet", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> tv.tv_character.appeared_in_tv_program -> m.0j7c9jh\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alfred Lammle -> fictional_universe.fictional_character.occupation -> Adventurer\n# Answer:\nAlfred Lammle", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.dates -> m.0c0z7kp\n# Answer:\n(1846)"], "ground_truth": ["A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol (Saddleback Classics)", "Bleak House", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Acting Edition)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (R)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Soundings)", "The life and adventures of Nicholas Nickleby", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Collector's Library)", "Martin Chuzzlewit", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Clear Print)", "David Copperfield.", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Isis Clear Type Classic)", "Sketches by Boz", "A Tale of Two Cities (Dramatized)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Green Integer, 50)", "Little Dorrit", "A Christmas Carol (Value Books)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Dramascripts S.)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Classic Literature with Classical Music)", "David Copperfield", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Everyman's Library (Paper))", "Hard times", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Classic Retelling)", "Dombey and Son.", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Children's Theatre Playscript)", "Great expectations", "The Pickwick Papers", "A Tale of Two Cities (Oxford Playscripts)", "The cricket on the hearth", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Adopted Classic)", "A Christmas Carol (Bantam Classic)", "The old curiosity shop", "A Tale of Two Cities", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Penguin Readers, Level 2)", "Bleak House.", "Great Expectations.", "A Christmas Carol (Ladybird Classics)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Puffin Choice)", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "Dombey and Son", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (Classics Illustrated Notes)", "Great Expectations", "A Christmas Carol (Reissue)", "The old curiosity shop.", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Penguin Readers, Level 5)", "Great expectations.", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Penguin Classics)", "The Old Curiosity Shop", "The mystery of Edwin Drood", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Masterworks)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Through the Magic Window Series)", "Our mutual friend", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Children's Classics)", "Dombey and son", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Cyber Classics)", "Oliver Twist", "A Tale of Two Cities (Unabridged Classics)", "Our mutual friend.", "A Christmas Carol. (Lernmaterialien)", "Bleak house", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Wordsworth Classics)", "A CHRISTMAS CAROL", "A Tale of Two Cities (Progressive English)", "A Christmas Carol (Young Reading Series 2)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Compact English Classics)", "The Pickwick papers", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Classic Fiction)"], "effective_topk": 169, "ans_acc": 0.047337278106508875, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.09145607701564382, "path_precision": 0.2222222222222222, "path_recall": 0.05757575757575758, "path_ans_f1": 0.08290155440414507, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.047337278106508875}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr"], "ground_truth": ["Vladimir Lenin"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Panama\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> common.topic.article -> m.09nqr\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar"], "ground_truth": ["United States Dollar"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.risk_factor.diseases -> Ptosis\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor"], "ground_truth": ["Lung cancer", "Brain tumor"], "effective_topk": 3, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nm.02h9cb0"], "ground_truth": ["William Daniels"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee"], "ground_truth": ["Williamson County"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond"], "ground_truth": ["Diamond"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.36363636363636365, "path_precision": 0.6666666666666666, "path_recall": 0.25, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9m2 -> military.military_command.military_conflict -> First Battle of Rappahannock Station\n# Answer:\nm.04fv9m2", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\ng.11bcf3yybd", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> time.event.included_in_event -> Northern Virginia Campaign\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign"], "ground_truth": ["Battle of Chancellorsville", "Battle of White Oak Swamp", "Battle of Harpers Ferry", "How Few Remain", "Battle of Chantilly", "Jackson's Valley Campaign", "First Battle of Kernstown", "First Battle of Winchester", "Romney Expedition", "American Civil War", "Battle of Cedar Mountain", "Manassas Station Operations", "First Battle of Rappahannock Station", "Battle of Hoke's Run", "Battle of Front Royal", "Battle of Hancock", "Second Battle of Bull Run", "Battle of McDowell", "Battle of Port Republic"], "effective_topk": 19, "ans_acc": 0.3157894736842105, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.6, "ans_recall": 0.15789473684210525, "path_f1": 0.2445414847161572, "path_precision": 0.7, "path_recall": 0.14814814814814814, "path_ans_f1": 0.45283018867924535, "path_ans_precision": 0.8, "path_ans_recall": 0.3157894736842105}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language"], "ground_truth": ["Maasai Language"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin"], "ground_truth": ["Deborah Read"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.3333333333333333, "path_recall": 0.25, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.risk_factor.diseases -> Jaundice\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Gingivitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.symptoms -> Abdominal mass\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Fresco\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> Godbey School of Art\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.media -> Gesso\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini"], "ground_truth": ["g.120vt1gz", "St. Jerome in the Wilderness", "Benois Madonna", "g.121wt37c", "Madonna of the Carnation", "Adoration of the Magi", "Sala delle Asse", "Bacchus", "Horse and Rider", "Lucan portrait of Leonardo da Vinci", "The Last Supper", "Ginevra de' Benci", "g.121yh91r", "St. John the Baptist", "Lady with an Ermine", "The Holy Infants Embracing", "Medusa", "Portrait of a man in red chalk", "Portrait of a Young Fianc\u00e9e", "Portrait of Isabella d'Este", "Madonna of Laroque", "The Baptism of Christ", "g.1213jb_b", "The Virgin and Child with St Anne and St John the Baptist", "Salvator Mundi", "g.1224tf0c", "Mona Lisa", "Madonna of the Yarnwinder", "The Virgin and Child with St. Anne", "Vitruvian Man", "Madonna Litta", "Head of a Woman", "The Battle of Anghiari", "g.12314dm1", "g.1239jd9p", "Annunciation", "Leonardo's horse", "Virgin of the Rocks", "Leda and the Swan", "Portrait of a Musician", "La belle ferronni\u00e8re", "g.1219sb0g", "g.12215rxg", "Drapery for a Seated Figure", "Madonna and Child with St Joseph"], "effective_topk": 45, "ans_acc": 0.022222222222222223, "ans_hit": 1, "ans_f1": 0.04040404040404041, "ans_precission": 0.2222222222222222, "ans_recall": 0.022222222222222223, "path_f1": 0.041237113402061855, "path_precision": 0.2222222222222222, "path_recall": 0.022727272727272728, "path_ans_f1": 0.04040404040404041, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 0.022222222222222223}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal"], "ground_truth": ["Nepal"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica"], "ground_truth": ["Lightning rod", "Franklin stove", "Bifocals", "Glass harmonica"], "effective_topk": 4, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano"], "ground_truth": ["Composer", "Musician", "Librettist"], "effective_topk": 3, "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4444444444444444, "ans_precission": 0.6666666666666666, "ans_recall": 0.3333333333333333, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4444444444444444, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> common.topic.notable_types -> Country\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nm.0102xvg7", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Gamprin\n# Answer:\nLiechtenstein"], "ground_truth": ["Belgium", "Austria", "East Germany", "Germany", "Liechtenstein", "Luxembourg", "Switzerland"], "effective_topk": 7, "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5357142857142858, "ans_precission": 0.7142857142857143, "ans_recall": 0.42857142857142855, "path_f1": 0.40816326530612246, "path_precision": 0.7142857142857143, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.subject_of -> Stephen Melton\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.notable_types -> Musical genre\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> music.genre.parent_genre -> Rock and roll\n# Answer:\nPop music"], "ground_truth": ["Rock music", "Experimental music", "Experimental rock", "Blues rock", "Soft rock", "Psychedelic rock", "Pop music", "Art rock", "Pop rock"], "effective_topk": 9, "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0.3555555555555555, "ans_precission": 0.8888888888888888, "ans_recall": 0.2222222222222222, "path_f1": 0.3555555555555555, "path_precision": 0.8888888888888888, "path_recall": 0.2222222222222222, "path_ans_f1": 0.5925925925925926, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nm.02_wstr"], "ground_truth": ["Michael Bennet", "Mark Udall"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSeattle -> location.location.containedby -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Hillman City, Seattle -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nHillman City, Seattle", "# Reasoning Path:\nSeattle -> location.location.containedby -> Area code 206 -> common.topic.notable_types -> Area code\n# Answer:\nArea code 206", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Hillman City, Seattle -> location.location.containedby -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nHillman City, Seattle", "# Reasoning Path:\nSeattle -> location.location.containedby -> Area code 206 -> common.topic.article -> m.0c810g\n# Answer:\nArea code 206", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> Fifteen Twenty-One Second Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102"], "ground_truth": ["98119-4114", "98101", "98181", "98102", "98170", "98103", "98114", "98109", "98171", "98198", "98133", "98134", "98164", "98119", "98141", "98175", "98174", "98116", "98139", "98108", "98121", "98104", "98129", "98131", "98178", "98126", "98118", "98112", "98155", "98185", "98199", "98154", "98168", "98117", "98115", "98113", "98184", "98127", "98132", "98158", "98124", "98148", "98194", "98191", "98190", "98160", "98145", "98105", "98136", "98195", "98138", "98166", "98177", "98144", "98106", "98111", "98122", "98188", "98161", "98165", "98107", "98125", "98146"], "effective_topk": 63, "ans_acc": 0.031746031746031744, "ans_hit": 1, "ans_f1": 0.057971014492753624, "ans_precission": 0.3333333333333333, "ans_recall": 0.031746031746031744, "path_f1": 0.057971014492753624, "path_precision": 0.3333333333333333, "path_recall": 0.031746031746031744, "path_ans_f1": 0.057971014492753624, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.031746031746031744}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters"], "ground_truth": ["Chinese characters", "N\u00fcshu script", "'Phags-pa script", "Traditional Chinese characters", "Simplified Chinese character"], "effective_topk": 5, "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq"], "ground_truth": ["Pat Nixon"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.3333333333333333, "path_recall": 0.14285714285714285, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdh -> tv.regular_tv_appearance.actor -> Marla Gibbs\n# Answer:\nm.03lkkdh", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdt -> tv.regular_tv_appearance.actor -> Paul Benedict\n# Answer:\nm.03lkkdt", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdn -> tv.regular_tv_appearance.actor -> Franklin Cover\n# Answer:\nm.03lkkdn", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_mc"], "ground_truth": ["Zara Cully", "Franklin Cover", "Damon Evans", "Berlinda Tolbert", "Isabel Sanford", "Sherman Hemsley", "Roxie Roker", "Mike Evans", "Marla Gibbs", "Jay Hammer", "Paul Benedict"], "effective_topk": 11, "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14705882352941177, "path_precision": 0.5555555555555556, "path_recall": 0.0847457627118644, "path_ans_f1": 0.36585365853658536, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> Novel\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.newspaper.issues -> San Francisco Bay Guardian, 24 Nov 1999\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101"], "ground_truth": ["San Francisco Bay View", "Dock of the Bay", "The Daily Alta California", "San Francisco Daily", "Synapse", "The San Francisco Examiner", "San Francisco Bay Guardian", "San Francisco Foghorn", "San Francisco Business Times", "AsianWeek", "California Star", "San Francisco Call", "San Francisco Chronicle", "Sing Tao Daily", "Free Society", "San Francisco News-Call Bulletin Newspaper", "Bay Area Reporter", "San Francisco Bay Times", "The Golden Era", "Street Sheet"], "effective_topk": 20, "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.25149700598802394, "ans_precission": 0.7777777777777778, "ans_recall": 0.15, "path_f1": 0.25149700598802394, "path_precision": 0.7777777777777778, "path_recall": 0.15, "path_ans_f1": 0.25149700598802394, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope"], "ground_truth": ["Europe"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1851-1855\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 16: 1868", "The Descent of Man, and Selection in Relation to Sex", "Charles Darwin's natural selection", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "La facult\u00e9 motrice dans les plantes", "The Life and Letters of Charles Darwin Volume 1", "Darwin for Today", "Questions about the breeding of animals", "Darwin Darwin", "The action of carbonate of ammonia on the roots of certain plants", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Leben und Briefe von Charles Darwin", "Volcanic Islands", "The Voyage of the Beagle", "Darwin's insects", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The portable Darwin", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die geschlechtliche Zuchtwahl", "H.M.S. Beagle in South America", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The Expression of the Emotions in Man and Animals", "Darwin Compendium", "Metaphysics, Materialism, & the evolution of mind", "South American Geology", "The Correspondence of Charles Darwin, Volume 13: 1865", "Darwinism stated by Darwin himself", "Charles Darwin's letters", "From so simple a beginning", "Notebooks on transmutation of species", "vari\u00eberen der huisdieren en cultuurplanten", "Darwin en Patagonia", "Tesakneri tsagume\u030c", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "A Darwin Selection", "Notes on the fertilization of orchids", "red notebook of Charles Darwin", "More Letters of Charles Darwin", "Memorias y epistolario i\u0301ntimo", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Life and Letters of Charles Darwin Volume 2", "La vie et la correspondance de Charles Darwin", "The Darwin Reader First Edition", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Orgin of Species", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 12: 1864", "Charles Darwin on the routes of male humble bees", "To the members of the Down Friendly Club", "Reise um die Welt 1831 - 36", "Darwin on humus and the earthworm", "ontstaan der soorten door natuurlijke teeltkeus", "The Correspondence of Charles Darwin, Volume 18: 1870", "The collected papers of Charles Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Evolution by natural selection", "Motsa ha-minim", "The education of Darwin", "Darwin's notebooks on transmutation of species", "monograph on the sub-class Cirripedia", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "genese\u014ds t\u014dn eid\u014dn", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Darwin", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Die fundamente zur entstehung der arten", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "On the Movements and Habits of Climbing Plants", "From Darwin's unpublished notebooks", "Origins", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Het uitdrukken van emoties bij mens en dier", "Les mouvements et les habitudes des plantes grimpantes", "The Different Forms of Flowers on Plants of the Same Species", "Diary of the voyage of H.M.S. Beagle", "The geology of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Kleinere geologische Abhandlungen", "The Correspondence of Charles Darwin, Volume 14: 1866", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Cartas de Darwin 18251859", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Darwin and Henslow", "The Correspondence of Charles Darwin, Volume 17: 1869", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Fertilisation of Orchids", "Geological Observations on South America", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "On Natural Selection", "The Life of Erasmus Darwin", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Autobiography of Charles Darwin", "Les moyens d'expression chez les animaux", "Evolution and natural selection", "Works", "On the tendency of species to form varieties", "Part I: Contributions to the Theory of Natural Selection / Part II", "On evolution", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 15: 1867", "Diario del Viaje de Un Naturalista Alrededor", "The voyage of Charles Darwin", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The principal works", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Darwin's journal", "Proiskhozhdenie vidov", "Darwin from Insectivorous Plants to Worms", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The\u0301orie de l'e\u0301volution", "The living thoughts of Darwin", "The Essential Darwin", "Voyage d'un naturaliste autour du monde", "Evolutionary Writings: Including the Autobiographies", "Resa kring jorden", "Opsht\u0323amung fun menshen", "Geological Observations on the Volcanic Islands", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 9: 1861", "Reise eines Naturforschers um die Welt", "Darwin-Wallace", "Charles Darwin's marginalia", "Wu zhong qi yuan", "Gesammelte kleinere Schriften", "Beagle letters", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "A student's introduction to Charles Darwin", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 11: 1863", "Darwin's Ornithological notes", "Les r\u00e9cifs de corail, leur structure et leur distribution", "El Origin De Las Especies", "The Variation of Animals and Plants under Domestication", "On a remarkable bar of sandstone off Pernambuco", "The foundations of the Origin of species", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Structure and Distribution of Coral Reefs", "Evolution", "Rejse om jorden", "Insectivorous Plants"], "effective_topk": 153, "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.02564102564102564, "ans_precission": 0.6666666666666666, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k"], "ground_truth": ["New York City"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All -> music.composition.recordings -> After All (stereo mix)\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician"], "ground_truth": ["And I Don't Love You", "Never My Love / Never Can Say Goodbye", "Noel", "Quiet Storm", "We've Saved the Best for Last", "Shoe Soul", "You Made Me Feel Love", "And I Love Her", "Will You Love Me Tomorrow?", "Double Good Everything", "You Really Got a Hold on Me", "It's Her Turn to Live", "Time After Time", "Virgin Man", "I've Made Love to You a Thousand Times", "You're Just My Life (feat. India.Arie)", "Standing On Jesus", "The Agony and the Ecstasy", "Girlfriend", "I Want You Back", "Let Me Be the Clock", "Going to a Go-Go", "I Love The Nearness Of You", "Mother's Son", "The Road to Damascus", "Christmas Every Day", "The Tracks Of My Tears", "Why Are You Running From My Love", "Sweet Harmony", "The Tears Of A Clown", "Ooo Baby Baby (live)", "I Praise & Worship You Father", "The Tears of a Clown", "Ebony Eyes", "I'm Glad There Is You", "You Take Me Away", "Why", "Going to a Go Go", "Tears of a Clown", "Just Like You", "She's Only a Baby Herself", "My Girl", "Some People Will Do Anything for Love", "If You Want My Love", "Just to See Her", "Whatcha Gonna Do", "Love Don' Give No Reason (12 Inch Club Mix)", "I'll Keep My Light In My Window", "Tracks Of My Tears (Live)", "Night and Day", "The Hurt's On You", "I Have Prayed On It", "Same Old Love", "Love Brought Us Here", "What's Too Much", "Unless You Do It Again", "Deck the Halls", "Rack Me Back", "Open", "Daylight & Darkness", "Christmas Everyday", "Be Careful What You Wish For", "Don't Play Another Love Song", "Nearness of You", "There Will Come a Day (I'm Gonna Happen to You)", "Cruisin'", "Quiet Storm (Groove Boutique Chill Jazz mix)", "As You Do", "Wanna Know My Mind", "Medley: Never My Love / Never Can Say Goodbye", "Tell Me Tomorrow (12\\\" extended mix)", "Tea for Two", "I've Made Love To You A Thousand Times", "Just Passing Through", "There Will Come A Day ( I'm Gonna Happen To You )", "Girl I'm Standing There", "The Christmas Song", "And I Don't Love You (Larry Levan instrumental dub)", "Season's Greetings from Smokey Robinson", "Hold on to Your Love", "More Than You Know", "You Are Forever", "Gang Bangin'", "Walk on By", "Blame It On Love (Duet with Barbara Mitchell)", "The Tracks of My Tears", "Speak Low", "You've Really Got a Hold on Me", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "When A Woman Cries", "Ooo Baby Baby", "Aqui Con Tigo (Being With You)", "Let Your Light Shine On Me", "Love Bath", "God Rest Ye Merry Gentlemen", "Mickey's Monkey", "Coincidentally", "A Tattoo", "In My Corner", "Going to a Gogo", "You Cannot Laugh Alone", "You're the One for Me (feat. Joss Stone)", "Wishful Thinking", "Just To See Her Again", "My Guy", "It's A Good Night", "I Can't Get Enough", "Be Careful What You Wish For (instrumental)", "Love Letters", "Rewind", "Please Don't Take Your Love (feat. Carlos Santana)", "Sleepless Nights", "Shop Around", "Melody Man", "I\u2019ve Got You Under My Skin", "Fly Me to the Moon (In Other Words)", "Driving Thru Life in the Fast Lane", "Heavy On Pride (Light On Love)", "Bad Girl", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Who's Sad", "Just a Touch Away", "Everything You Touch", "I Care About Detroit", "I Can't Give You Anything but Love", "He Can Fix Anything", "Train of Thought", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Easy", "Te Quiero Como Si No Hubiera Un Manana", "I Can't Find", "I Know You by Heart", "The Track of My Tears", "Baby Come Close", "I'm in the Mood for Love", "Jingle Bells", "You Are So Beautiful (feat. Dave Koz)", "Save Me", "Vitamin U", "Happy (Love Theme From Lady Sings the Blues)", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Being With You", "Get Ready", "Can't Fight Love", "That Place", "Don't Know Why", "I Love Your Face", "Will You Love Me Tomorrow", "Tell Me Tomorrow, Part 1", "Come to Me Soon", "The Agony And The Ecstasy", "Love' n Life", "The Tracks of My Tears (live)", "Just My Soul Responding", "Ebony Eyes (Duet with Rick James)", "Photograph in My Mind", "A Child Is Waiting", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Pops, We Love You (disco)", "Wedding Song", "Will You Still Love Me Tomorrow", "No\u00ebl", "I've Got You Under My Skin", "Baby That's Backatcha", "Skid Row", "Our Love Is Here to Stay", "Ooh Baby Baby", "My World", "Tears Of A Clown", "Don't Wanna Be Just Physical", "Ever Had A Dream", "Be Kind To The Growing Mind (with The Temptations)", "Santa Claus is Coming to Town", "Love So Fine", "Please Come Home for Christmas", "With Your Love Came", "Yes It's You Lady", "The Love Between Me and My Kids", "Tell Me Tomorrow", "Love Is The Light", "I Second That Emotions", "You Don't Know What It's Like", "It's Fantastic", "Because of You It's the Best It's Ever Been", "Cruisin", "I Can\u2019t Stand to See You Cry (Commercial version)", "Fulfill Your Need", "Share It", "Come by Here (Kum Ba Ya)", "Gone Forever", "It's Christmas Time", "Hanging on by a Thread", "Tears of a Sweet Free Clown", "Why Do Happy Memories Hurt So Bad", "We Are The Warriors", "One Heartbeat", "So Bad", "Quiet Storm (Groove Boutique remix)", "Tracks of my Tears", "One Time", "Away in the Manger / Coventry Carol", "Food For Thought", "Crusin", "Close Encounters of the First Kind", "When Smokey Sings Tears Of A Clown", "Let Me Be The Clock", "Take Me Through The Night", "Time Flies", "I Am, I Am", "Just Another Kiss", "If You Can Want", "Yester Love", "Fallin'", "It's a Good Feeling", "We\u2019ve Come Too Far to End It Now", "The Tracks of My Heart", "Satisfy You", "Ain't That Peculiar", "Did You Know (Berry's Theme)", "If You Wanna Make Love", "Quiet Storm (single version)", "No Time to Stop Believing", "I Second That Emotion", "Everything for Christmas", "Little Girl Little Girl", "Christmas Greeting", "Theme From the Big Time", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Crusin'", "I Like Your Face", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Really Gonna Miss You", "More Love", "I Am I Am", "Winter Wonderland", "Little Girl, Little Girl", "Holly", "Tracks of My Tears", "It's Time to Stop Shoppin' Around", "Asleep on My Love", "You've Really Go a Hold on Me", "I Hear The Children Singing", "Pops, We Love You", "Be Who You Are", "Jasmin", "Love Don't Give No Reason", "Jesus Told Me To Love You", "You Go to My Head", "Be Kind to the Growing Mind", "A Silent Partner in a Three-Way Love Affair", "If You Wanna Make Love (Come 'round Here)", "Keep Me", "Blame It on Love", "(It's The) Same Old Love", "The Family Song", "The Way You Do (The Things You Do)"], "effective_topk": 256, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Lyndon B. Johnson"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language"], "ground_truth": ["Serbian language"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}

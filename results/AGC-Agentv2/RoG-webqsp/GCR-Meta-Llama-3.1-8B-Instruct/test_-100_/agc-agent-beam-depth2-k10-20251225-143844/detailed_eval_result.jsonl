{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Belize\n# Answer:\nTropical Storm Keith"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Tennessee\n# Answer:\nUnited States of America"], "ground_truth": ["Speaker of the United States House of Representatives", "United States Representative", "Governor of Tennessee"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1818181818181818, "path_precision": 0.2, "path_recall": 0.16666666666666666, "path_ans_f1": 0.25, "path_ans_precision": 0.2, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician"], "ground_truth": ["Melinda McGraw", "Ilyssa Fradin", "Hannah Gunn"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer"], "ground_truth": ["Cleveland Cavaliers", "LSU Tigers men's basketball", "Los Angeles Lakers", "Phoenix Suns", "Orlando Magic", "Boston Celtics", "Miami Heat"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American\n# Answer:\nScottish American"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Malay, Pattani Language", "Akha Language", "Lao Language", "Thai Language", "Phu Thai language", "Khmer language", "Mlabri Language", "Cham language", "Vietnamese Language", "Hmong language", "Mon Language", "Saek language", "Nyaw Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 1.0, "ans_recall": 0.3076923076923077, "path_f1": 0.3157894736842105, "path_precision": 0.5, "path_recall": 0.23076923076923078, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 1.0, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Facebook, Inc.\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.image -> Growth in Social Network Patent Applications\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> computer.software_genre.software_in_genre -> 2go\n# Answer:\nSocial networking service"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson"], "ground_truth": ["Writer", "Inventor", "Lawyer", "Author", "Archaeologist", "Architect", "Teacher", "Farmer", "Philosopher", "Statesman"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.5714285714285714, "ans_recall": 0.4, "path_f1": 0.3529411764705882, "path_precision": 0.42857142857142855, "path_recall": 0.3, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Biology\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Evolution\n# Answer:\nDarwin"], "ground_truth": ["Origin of Species", "The Correspondence of Charles Darwin, Volume 18: 1870", "The principal works", "The Expression Of The Emotions In Man And Animals", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "From Darwin's unpublished notebooks", "Voyage of the Beagle (Dover Value Editions)", "The Autobiography Of Charles Darwin", "Darwin", "On evolution", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 5", "The Structure And Distribution of Coral Reefs", "The Origin of Species (Mentor)", "monograph on the sub-class Cirripedia", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 8: 1860", "Tesakneri tsagume\u030c", "Notes on the fertilization of orchids", "H.M.S. Beagle in South America", "The Correspondence of Charles Darwin, Volume 12: 1864", "The origin of species : complete and fully illustrated", "Origins", "The Correspondence of Charles Darwin, Volume 9: 1861", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 1", "Het uitdrukken van emoties bij mens en dier", "Darwinism stated by Darwin himself", "The Correspondence of Charles Darwin, Volume 16: 1868", "The descent of man and selection in relation to sex.", "More Letters of Charles Darwin", "Wu zhong qi yuan", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Origin of Species (Harvard Classics, Part 11)", "Metaphysics, Materialism, & the evolution of mind", "The Origin of Species (Oxford World's Classics)", "The Expression of the Emotions in Man And Animals", "The Correspondence of Charles Darwin, Volume 2", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Variation of Animals and Plants under Domestication", "The living thoughts of Darwin", "The Autobiography of Charles Darwin (Dodo Press)", "Gesammelte kleinere Schriften", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The Orgin of Species", "The Origin of Species (Great Minds Series)", "The Power of Movement in Plants", "Darwin's notebooks on transmutation of species", "The Essential Darwin", "The Life of Erasmus Darwin", "Charles Darwin's letters", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Autobiography of Charles Darwin, and selected letters", "Notebooks on transmutation of species", "A student's introduction to Charles Darwin", "Charles Darwin on the routes of male humble bees", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Expression of the Emotions in Man and Animals", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The portable Darwin", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Origin Of Species", "The Voyage of the Beagle (Unabridged Classics)", "La facult\u00e9 motrice dans les plantes", "Die fundamente zur entstehung der arten", "On a remarkable bar of sandstone off Pernambuco", "The Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 14", "Darwin Compendium", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Die geschlechtliche Zuchtwahl", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Origin of Species (Great Books : Learning Channel)", "Opsht\u0323amung fun menshen", "On the Movements and Habits of Climbing Plants", "The Autobiography of Charles Darwin [EasyRead Edition]", "Geological Observations on South America", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Questions about the breeding of animals", "From so simple a beginning", "The Descent of Man, and Selection in Relation to Sex", "The Origin of Species (Enriched Classics)", "The autobiography of Charles Darwin", "The expression of the emotions in man and animals.", "Reise um die Welt 1831 - 36", "Charles Darwin's marginalia", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin for Today", "The descent of man, and selection in relation to sex", "The Correspondence of Charles Darwin, Volume 8", "Resa kring jorden", "Cartas de Darwin 18251859", "The voyage of Charles Darwin", "Diario del Viaje de Un Naturalista Alrededor", "Darwin Darwin", "Voyage of the Beagle (Harvard Classics, Part 29)", "Kleinere geologische Abhandlungen", "The structure and distribution of coral reefs.", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Correspondence of Charles Darwin, Volume 11", "Darwin's Ornithological notes", "The Correspondence of Charles Darwin, Volume 3", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Diary of the voyage of H.M.S. Beagle", "The Voyage of the Beagle (Great Minds Series)", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Proiskhozhdenie vidov", "The Formation of Vegetable Mould through the Action of Worms", "El Origin De Las Especies", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The collected papers of Charles Darwin", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Rejse om jorden", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Part I: Contributions to the Theory of Natural Selection / Part II", "Charles Darwin's natural selection", "The Correspondence of Charles Darwin, Volume 15", "The Voyage of the Beagle (Everyman Paperbacks)", "ontstaan der soorten door natuurlijke teeltkeus", "The Correspondence of Charles Darwin, Volume 7", "Memorias y epistolario i\u0301ntimo", "The Correspondence of Charles Darwin, Volume 4", "Works", "Darwin en Patagonia", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Motsa ha-minim", "The Autobiography of Charles Darwin", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 13", "Voyage of the Beagle", "The Autobiography of Charles Darwin (Large Print)", "Darwin's insects", "The education of Darwin", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 6", "The Origin of Species (Collector's Library)", "red notebook of Charles Darwin", "On Natural Selection", "The Darwin Reader First Edition", "Human nature, Darwin's view", "Voyage of the Beagle (NG Adventure Classics)", "The Correspondence of Charles Darwin, Volume 9", "On the origin of species by means of natural selection", "The Structure and Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 10", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Evolution by natural selection", "The Darwin Reader Second Edition", "The geology of the voyage of H.M.S. Beagle", "Volcanic Islands", "The Different Forms of Flowers on Plants of the Same Species", "Leben und Briefe von Charles Darwin", "The foundations of the Origin of species", "Beagle letters", "The autobiography of Charles Darwin, 1809-1882", "Voyage Of The Beagle", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Darwin on humus and the earthworm", "La vie et la correspondance de Charles Darwin", "A Darwin Selection", "Fertilisation of Orchids", "The Origin of Species", "The expression of the emotions in man and animals", "The Voyage of the Beagle (Mentor)", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Les mouvements et les habitudes des plantes grimpantes", "From So Simple a Beginning", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Correspondence of Charles Darwin, Volume 12", "The Voyage of the Beagle (Adventure Classics)", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Descent of Man and Selection in Relation to Sex", "Darwin-Wallace", "The voyage of the Beagle.", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "On the tendency of species to form varieties", "Charles Darwin", "The structure and distribution of coral reefs", "Evolution", "The descent of man, and selection in relation to sex.", "Reise eines Naturforschers um die Welt", "vari\u00eberen der huisdieren en cultuurplanten", "The action of carbonate of ammonia on the roots of certain plants", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The origin of species", "The\u0301orie de l'e\u0301volution", "Autobiography of Charles Darwin", "Origin of Species (Everyman's University Paperbacks)", "The Correspondence of Charles Darwin, Volume 15: 1867", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Les moyens d'expression chez les animaux", "Darwin and Henslow", "The Origin of Species (World's Classics)", "Voyage d'un naturaliste autour du monde", "Evolution and natural selection", "To the members of the Down Friendly Club"], "ans_acc": 0.07009345794392523, "ans_hit": 1, "ans_f1": 0.01840490797546012, "ans_precission": 0.6, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.13100436681222707, "path_ans_precision": 1.0, "path_ans_recall": 0.07009345794392523}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender"], "ground_truth": ["Denver Broncos"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Poland\n# Answer:\nPoland", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Brebeneskul\n# Answer:\nBrebeneskul", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> location.location.containedby -> Europe\n# Answer:\nBucura Dumbrav\u0103", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> location.location.containedby -> Bucegi Mountains\n# Answer:\nBucura Dumbrav\u0103", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> common.topic.notable_for -> g.125d3kbg5\n# Answer:\nBucura Dumbrav\u0103", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> location.location.containedby -> Romania\n# Answer:\nBucura Dumbrav\u0103"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Bob Dylan\n# Answer:\nBob Dylan", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Charlotte Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nCharlotte Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Governess\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.ethnicity -> Irish people in Great Britain\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Novelist\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Writer", "Author", "Poet", "Bard"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStratford"], "ground_truth": ["Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Air engagements of the Gulf War\n# Answer:\nAir engagements of the Gulf War"], "ground_truth": ["United States of America", "Argentina", "Australia", "France", "United Kingdom", "Saudi Arabia", "Iraq"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.23529411764705882, "ans_precission": 0.6666666666666666, "ans_recall": 0.14285714285714285, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.23529411764705882, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol\n# Answer:\nA London Carol", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> Ala-ka-scram!\n# Answer:\nAla-ka-scram!"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> common.topic.notable_for -> g.1255tjcrg\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio"], "ground_truth": ["John Kasich", "Return J. Meigs, Jr.", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": [], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site\n# Answer:\nCoronado Historic Site", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado National Forest\n# Answer:\nCoronado National Forest", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald\n# Answer:\nJohn F. Fitzgerald", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 3 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 3", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2 -> common.topic.notable_for -> g.1257z60_h\n# Answer:\nFukushima I \u2013 2", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2 -> base.infrastructure.nuclear_power_plant.reactor_type -> Boiling water reactor\n# Answer:\nFukushima I \u2013 2", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 3 -> common.topic.notable_for -> g.125bty6m2\n# Answer:\nFukushima I \u2013 3", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 3 -> base.infrastructure.nuclear_power_plant.coolant -> Water\n# Answer:\nFukushima I \u2013 3"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.internet_tld -> eu\n# Answer:\nNorthern Ireland"], "ground_truth": ["Northern Ireland", "England", "Wales", "Scotland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Lake Maurepas\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8__h\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Livingston Parish\n# Answer:\nAmite River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently.\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Physician\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.nationality -> United Kingdom\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> By faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity. -> media_common.quotation.subjects -> Faithfulness\n# Answer:\nBy faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> book.author.works_written -> A short history of ethics\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Writer", "Physician", "Philosopher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> (When You Gonna) Give It Up To Me\n# Answer:\n(When You Gonna) Give It Up To Me", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons"], "ground_truth": ["Leon Cole", "Sal Gibson", "Francine Lons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library -> location.location.geolocation -> m.0wmmldf\n# Answer:\nDr. Martin Luther King, Jr. Library", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library -> common.topic.image -> 2008-0817-SJSU-MLKlib\n# Answer:\nDr. Martin Luther King, Jr. Library", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library -> common.topic.image -> Southeast entrance of the King Library\n# Answer:\nDr. Martin Luther King, Jr. Library"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore\n# Answer:\nAlan Moore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.\n# Answer:\nAfter reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel -> book.author.school_or_movement -> Modernism\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel -> people.person.gender -> Male\n# Answer:\nAlbrecht Behmel"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Cobb County Airport\n# Answer:\nCobb County Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> common.topic.notable_types -> Airport\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Atlanta Regional Airport -> aviation.airport.serves -> Peachtree City\n# Answer:\nAtlanta Regional Airport"], "ground_truth": ["Arbor Place Mall", "Fox Theatre", "Centennial Olympic Park", "Georgia Aquarium", "Fernbank Museum of Natural History", "Atlanta Symphony Orchestra", "Masquerade", "Cobb Energy Performing Arts Centre", "Atlanta Marriott Marquis", "World of Coca-Cola", "Underground Atlanta", "Georgia State Capitol", "The Tabernacle", "Center for Puppetry Arts", "Atlanta Jewish Film Festival", "Atlanta Ballet", "Georgia Dome", "Omni Coliseum", "CNN Center", "Fernbank Science Center", "Four Seasons Hotel Atlanta", "Atlanta History Center", "Six Flags Over Georgia", "Martin Luther King, Jr. National Historic Site", "Variety Playhouse", "Margaret Mitchell House & Museum", "Six Flags White Water", "Peachtree Road Race", "Turner Field", "Atlanta Cyclorama & Civil War Museum", "Hyatt Regency Atlanta", "Woodruff Arts Center", "Jimmy Carter Library and Museum", "Zoo Atlanta", "Georgia World Congress Center", "Philips Arena"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.13636363636363638, "ans_precission": 0.375, "ans_recall": 0.08333333333333333, "path_f1": 0.13636363636363638, "path_precision": 0.375, "path_recall": 0.08333333333333333, "path_ans_f1": 0.13636363636363638, "path_ans_precision": 0.375, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.contains -> Assumption College, Warwick\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.contains -> Australian Rodeo Heritage Centre\n# Answer:\nWarwick"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alf Roberts\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Annie Walker\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> Alakdana\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> media_common.literary_genre.books_in_this_genre -> Amis and Amiloun\n# Answer:\nChivalric romance"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nCocos (Keeling) Islands"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr.\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton\n# Answer:\nJackie Newton"], "ground_truth": ["Carolina Panthers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer\n# Answer:\nAlfred G. Mayer", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bazabeel Norman\n# Answer:\nBazabeel Norman", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bert Myers\n# Answer:\nBert Myers"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> common.topic.notable_for -> g.125dtp7bg\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney\n# Answer:\nDick Cheney"], "ground_truth": ["Gene Amondson", "John Kerry", "Ralph Nader", "Michael Peroutka"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson\n# Answer:\nThomas Ferguson"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Floreana Island\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Floreana Island\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.notable_for -> g.1255fs0l4\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Tortuga Bay\n# Answer:\nTortuga Bay", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island\n# Answer:\nBaltra Island"], "ground_truth": ["Gal\u00e1pagos Province", "Pacific Ocean", "Ecuador"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Dre & Vidal\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.album.release_type -> Single\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Andre Harris\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> common.topic.notable_for -> g.1yl5r3f4x\n# Answer:\nAll Bad"], "ground_truth": ["Boyfriend", "PYD", "First Dance", "Recovery", "Thought Of You", "Turn to You (Mother's Day Dedication)", "Beauty And A Beat", "Right Here", "Baby", "Hold Tight", "Eenie Meenie", "#thatPower", "Live My Life", "Confident", "Pray", "All Bad", "All Around The World", "Never Say Never", "Wait for a Minute", "Bigger", "Home to Mama", "Bad Day", "All That Matters", "Heartbreaker", "Die in Your Arms", "Never Let You Go", "Somebody to Love", "Roller Coaster", "As Long as You Love Me", "Change Me", "Lolly"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.17647058823529413, "ans_precission": 1.0, "ans_recall": 0.0967741935483871, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.17647058823529413, "path_ans_precision": 1.0, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> common.topic.subjects -> Ciro Pellegrino\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Publisher\n# Answer:\nPublisher", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Vera Douka\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> Officeholder\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A doctor, like anyone else who has to deal with human beings, each of them unique, cannot be a scientist; he is either, like the surgeon, a craftsman, or, like the physician and the psychologist, an artist. This means that in order to be a good doctor a man must also have a good character, that is to say, whatever weaknesses and foibles he may have, he must love his fellow human beings in the concrete and desire their good before his own.\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> base.descriptive_names.names.descriptive_name -> m.0105cq_d\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> location.location.containedby -> Paris\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau"], "ground_truth": ["Writer", "Publisher", "Physician", "Journalist", "Statesman"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.5, "ans_recall": 0.6, "path_f1": 0.5454545454545454, "path_precision": 0.5, "path_recall": 0.6, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.5, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.strict_included_types -> Administrative Division\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.equivalent_topic -> U.S. state\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> type.type.expected_by -> Indiana\n# Answer:\nUS State"], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaint Michael Parish"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Iowa\n# Answer:\nUnited States of America"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> God\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura\n# Answer:\nDay of Ashura", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat -> common.topic.article -> m.027cs4d\n# Answer:\nDay of Arafat", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat -> common.topic.notable_for -> g.125dtpf6f\n# Answer:\nDay of Arafat", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat -> time.day_of_year.calendar_system -> Islamic calendar\n# Answer:\nDay of Arafat"], "ground_truth": ["Prophets in Islam", "Qiyamah", "Islamic view of angels", "Mahdi", "Masih ad-Dajjal", "Islamic holy books", "Predestination in Islam", "Entering Heaven alive", "Tawhid", "God in Islam", "Monotheism", "Sharia", "\u1e6c\u016bb\u0101"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.prevention_factors -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_for -> g.12599cgh4\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.prevention_factors -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> \u00c9mile Zola -> food.diet_follower.follows_diet -> Vegetarian food\n# Answer:\n\u00c9mile Zola"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler -> organization.organization.place_founded -> Nazi Germany\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler -> base.schemastaging.context_name.pronunciation -> m.01314fx8\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.25, "path_recall": 0.09090909090909091, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger"], "ground_truth": ["Songwriter", "Actor", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9090909090909091, "ans_precission": 0.8333333333333334, "ans_recall": 1.0, "path_f1": 0.9090909090909091, "path_precision": 0.8333333333333334, "path_recall": 1.0, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City\n# Answer:\nKansas City", "# Reasoning Path:\nKansas Speedway\n# Answer:\nKansas Speedway"], "ground_truth": ["Wyandotte County"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Abraham Cohn\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.article -> m.03vbmy\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Alexander Walters\n# Answer:\nCypress Hills Cemetery"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr.\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Eleanor and Harry -> book.written_work.subjects -> 20th century\n# Answer:\nEleanor and Harry", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Eleanor and Harry -> book.book_edition.isbn -> 9780743202435\n# Answer:\nEleanor and Harry", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Eleanor and Harry -> book.written_work.subjects -> Franklin D. Roosevelt\n# Answer:\nEleanor and Harry", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Eleanor and Harry -> book.written_work.subjects -> History of the United States\n# Answer:\nEleanor and Harry"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": [], "ground_truth": ["Protestantism", "Islam", "Hinduism", "Catholicism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0, "ans_recall": 0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0, "path_ans_recall": 0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Rifle\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Semi-automatic firearm\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> common.topic.notable_types -> US President\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846)\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> Ada Lovelace asked to see him when she was dying in 1847.\n# Answer:\nAda Lovelace asked to see him when she was dying in 1847.", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> people.person.nationality -> United Kingdom\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nAlphonse Daudet", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> After 80 readings, his health was clearly affected, but he kept up the pace. -> base.kwebbase.kwsentence.previous_sentence -> When he finished it,  in November, 1865, he accepted a reading tour that took him to North America in late 1867.\n# Answer:\nAfter 80 readings, his health was clearly affected, but he kept up the pace."], "ground_truth": ["A Christmas Carol (New Longman Literature)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Bantam Classic)", "The old curiosity shop", "Bleak house", "A Tale of Two Cities (Macmillan Students' Novels)", "Oliver Twist", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (Watermill Classics)", "The Pickwick Papers", "A Christmas Carol (Value Books)", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Classic Literature with Classical Music)", "Bleak House", "A Christmas Carol (Large Print)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "David Copperfield", "A CHRISTMAS CAROL", "Our mutual friend", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Cyber Classics)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol (Saddleback Classics)", "Dombey and Son.", "Dombey and Son", "A Christmas Carol (Penguin Student Editions)", "Hard times", "A Christmas Carol (Acting Edition)", "The life and adventures of Nicholas Nickleby", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "The mystery of Edwin Drood", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Collector's Library)", "The Mystery of Edwin Drood", "Great Expectations", "The Old Curiosity Shop", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (The Greatest Historical Novels)", "The old curiosity shop.", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Enriched Classic)", "David Copperfield.", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Apple Classics)", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Puffin Choice)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Classic Collection)", "Great Expectations.", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Classic Fiction)", "Sketches by Boz", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Children's Classics)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (R)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol", "A Christmas Carol (Soundings)", "A Christmas Carol (Ladybird Classics)", "The cricket on the hearth", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Dramascripts S.)", "A TALE OF TWO CITIES", "A Christmas Carol (The Kennett Library)", "The Pickwick papers", "A Tale of Two Cities (Naxos AudioBooks)", "Dombey and son", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Illustrated Classics)", "Great expectations.", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Oxford Bookworms Library)", "Little Dorrit", "A Christmas Carol (Classic Fiction)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Classics Illustrated)", "Bleak House.", "A Christmas Carol (Family Classics)", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (Longman Fiction)", "Great expectations", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Gollancz Children's Classics)", "Our mutual friend.", "A Christmas Carol (Radio Theatre)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Dramatized)", "Martin Chuzzlewit", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Soundings)"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings\n# Answer:\n1977 Moscow bombings"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> base.folklore.mythical_creature.similar_mythical_creature_s -> m.05d0ybv\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> common.topic.notable_types -> Literature Subject\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> award.ranked_item.appears_in_ranked_lists -> m.0h7g_lj\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions\n# Answer:\nFrontpoint Security Solutions", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Hair Club\n# Answer:\nHair Club", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Retail\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Board members\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> common.topic.subject_of -> NYSE\n# Answer:\nDDR Corp."], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> A Scottish Soldier -> music.recording.releases -> Scotland the Brave\n# Answer:\nA Scottish Soldier", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Ballard of Glencoe -> music.recording.releases -> Scotland the Brave\n# Answer:\nBallard of Glencoe", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne -> common.topic.notable_for -> g.126t2bpvr\n# Answer:\nAul Lang Syne", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne -> common.topic.notable_types -> Musical Recording\n# Answer:\nAul Lang Syne"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.episodes -> Burial Ground\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4m5\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> common.topic.notable_for -> g.125f8s1f9\n# Answer:\nKnight Rider - Season 4"], "ground_truth": ["William Daniels"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.contains -> Alexander Smith House\n# Answer:\nAlexander Smith House", "# Reasoning Path:\nBrentwood -> location.location.contains -> Andrew Crockett House\n# Answer:\nAndrew Crockett House", "# Reasoning Path:\nBrentwood -> location.location.contains -> Constantine Sneed House\n# Answer:\nConstantine Sneed House"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist\n# Answer:\nBotanist", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy\n# Answer:\nCarver Academy", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Chemist\n# Answer:\nChemist", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Inventor\n# Answer:\nInventor", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> education.educational_institution.school_type -> High school\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> location.location.geolocation -> m.01259rzq\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> education.educational_institution.school_type -> Magnet school\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> education.educational_institution.school_type -> Public school\n# Answer:\nCarver Center for Arts and Technology"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox\n# Answer:\nAquinnah Kathleen Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> base.schemastaging.context_name.pronunciation -> g.125_ks95v\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> time.event.included_in_event -> American Civil War\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.military_personnel_involved -> Albion P. Howe\n# Answer:\nBattle of Antietam", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> common.topic.notable_types -> Military Conflict\n# Answer:\nBattle of Antietam"], "ground_truth": ["Second Battle of Bull Run", "Battle of Chancellorsville", "Manassas Station Operations", "Battle of Harpers Ferry", "American Civil War", "Jackson's Valley Campaign", "Battle of White Oak Swamp", "Battle of Hoke's Run", "First Battle of Rappahannock Station", "First Battle of Winchester", "Battle of Chantilly", "Battle of Front Royal", "Battle of McDowell", "Romney Expedition", "Battle of Port Republic", "Battle of Hancock", "First Battle of Kernstown", "How Few Remain", "Battle of Cedar Mountain"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.24742268041237112, "ans_precission": 0.5714285714285714, "ans_recall": 0.15789473684210525, "path_f1": 0.12631578947368421, "path_precision": 0.42857142857142855, "path_recall": 0.07407407407407407, "path_ans_f1": 0.24742268041237112, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Phonology\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Detailed Description\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump\n# Answer:\nMaasai-jump", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Genesis Translation\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.size -> m.02bc9fn\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> British American\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2 -> common.image.size -> m.03sznp2\n# Answer:\nSwayze2", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Pancreatectomy\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Sala delle Asse\n# Answer:\nSala delle Asse", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Giorgio Vasari -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nGiorgio Vasari", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Giorgio Vasari -> people.person.gender -> Male\n# Answer:\nGiorgio Vasari"], "ground_truth": ["The Virgin and Child with St. Anne", "Madonna of the Carnation", "Lucan portrait of Leonardo da Vinci", "The Last Supper", "Madonna and Child with St Joseph", "The Virgin and Child with St Anne and St John the Baptist", "Horse and Rider", "Head of a Woman", "g.120vt1gz", "St. Jerome in the Wilderness", "g.12314dm1", "g.1219sb0g", "Annunciation", "g.121wt37c", "Leonardo's horse", "g.1224tf0c", "Benois Madonna", "Madonna Litta", "The Baptism of Christ", "Drapery for a Seated Figure", "Mona Lisa", "Medusa", "The Holy Infants Embracing", "St. John the Baptist", "Virgin of the Rocks", "Portrait of Isabella d'Este", "g.1213jb_b", "Madonna of Laroque", "Portrait of a Young Fianc\u00e9e", "Vitruvian Man", "g.1239jd9p", "La belle ferronni\u00e8re", "Portrait of a Musician", "Adoration of the Magi", "Leda and the Swan", "g.121yh91r", "Portrait of a man in red chalk", "Sala delle Asse", "The Battle of Anghiari", "Salvator Mundi", "Lady with an Ermine", "Ginevra de' Benci", "Madonna of the Yarnwinder", "g.12215rxg", "Bacchus"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1090909090909091, "ans_precission": 0.3, "ans_recall": 0.06666666666666667, "path_f1": 0.11111111111111109, "path_precision": 0.3, "path_recall": 0.06818181818181818, "path_ans_f1": 0.1090909090909091, "path_ans_precision": 0.3, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia\n# Answer:\nCarinthia"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Essays\n# Answer:\nEssays", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> Anticancer\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.article -> m.0c3tv_n\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.notable_types -> Museum\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Astrology\n# Answer:\nBuddhism"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Other instruments\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica"], "ground_truth": ["Franklin stove", "Glass harmonica", "Lightning rod", "Bifocals"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8135593220338982, "ans_precission": 0.8888888888888888, "ans_recall": 0.75, "path_f1": 0.8135593220338982, "path_precision": 0.8888888888888888, "path_recall": 0.75, "path_ans_f1": 0.8135593220338982, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.contains -> Aims Community College\n# Answer:\nAims Community College", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpt45\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Charode\u01d0ka\n# Answer:\nCharode\u01d0ka", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_for -> g.12599cm43\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_types -> Book\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Classic Bits & Pieces -> common.topic.notable_types -> Book\n# Answer:\nClassic Bits & Pieces", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Classic Bits & Pieces -> common.topic.notable_for -> g.125d8f763\n# Answer:\nClassic Bits & Pieces"], "ground_truth": ["Composer", "Librettist", "Musician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard\n# Answer:\nGerman, Standard"], "ground_truth": ["Austria", "Germany", "Luxembourg", "Switzerland", "Liechtenstein", "Belgium", "East Germany"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6486486486486486, "ans_precission": 0.75, "ans_recall": 0.5714285714285714, "path_f1": 0.5454545454545454, "path_precision": 0.75, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6486486486486486, "path_ans_precision": 0.75, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> EYE97\n# Answer:\nEYE97", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies\n# Answer:\n181-greatoldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Clock\n# Answer:\nClock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Erection\n# Answer:\nErection"], "ground_truth": ["Art rock", "Rock music", "Psychedelic rock", "Pop music", "Blues rock", "Experimental music", "Experimental rock", "Soft rock", "Pop rock"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.3333333333333333, "ans_recall": 0.3333333333333333, "path_f1": 0.3333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2010 -> common.topic.image -> Michael Bennet Official Photo\n# Answer:\nUnited States Senate election in Colorado, 2010", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> military.military_unit.unit_size -> Regiment\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Infantry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.article -> m.07fjvw\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment -> military.military_unit.armed_force -> Union Army\n# Answer:\n1st Colorado Infantry Regiment"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.22222222222222224, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 0.5}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries\n# Answer:\nNordic countries"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Pike Place Market\n# Answer:\nPike Place Market", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> common.topic.notable_types -> Postal Code\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1201 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geometry -> m.055f4wk\n# Answer:\n98102"], "ground_truth": ["98154", "98148", "98118", "98101", "98164", "98102", "98171", "98168", "98134", "98139", "98184", "98160", "98115", "98175", "98188", "98165", "98111", "98124", "98104", "98199", "98127", "98119-4114", "98190", "98170", "98122", "98178", "98194", "98132", "98198", "98108", "98191", "98109", "98112", "98129", "98121", "98158", "98114", "98113", "98161", "98195", "98106", "98145", "98185", "98136", "98105", "98166", "98133", "98107", "98141", "98103", "98181", "98155", "98138", "98125", "98174", "98131", "98177", "98144", "98146", "98116", "98117", "98126", "98119"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08974358974358974, "ans_precission": 0.7777777777777778, "ans_recall": 0.047619047619047616, "path_f1": 0.08974358974358974, "path_precision": 0.7777777777777778, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08974358974358974, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance music\n# Answer:\nDance music", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance-pop\n# Answer:\nDance-pop", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_sy\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Fuzhou dialect\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.type_of_writing -> Ideogram\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Goguryeo language\n# Answer:\nChinese characters"], "ground_truth": ["Traditional Chinese characters", "'Phags-pa script", "Chinese characters", "Simplified Chinese character", "N\u00fcshu script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.711864406779661, "ans_precission": 0.875, "ans_recall": 0.6, "path_f1": 0.711864406779661, "path_precision": 0.875, "path_recall": 0.6, "path_ans_f1": 0.711864406779661, "path_ans_precision": 0.875, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> common.topic.notable_for -> g.125b3gc_h\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.deceased_person.place_of_death -> La Habra\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Sarah Ann Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Donald Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_types -> TV Season\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Isabel Sanford", "Jay Hammer", "Paul Benedict", "Zara Cully", "Franklin Cover", "Mike Evans", "Damon Evans", "Marla Gibbs", "Berlinda Tolbert", "Sherman Hemsley", "Roxie Roker"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> book.periodical.frequency_or_issues_per_year -> m.09s58j6\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.newspaper.issues -> San Francisco Bay Guardian, 24 Nov 1999\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.notable_for -> g.125d1300_\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.webpage -> m.03kz_rr\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906\n# Answer:\n1906"], "ground_truth": ["Synapse", "Free Society", "San Francisco Bay View", "San Francisco Chronicle", "San Francisco Foghorn", "California Star", "San Francisco Daily", "Dock of the Bay", "San Francisco Bay Guardian", "San Francisco Business Times", "Sing Tao Daily", "The San Francisco Examiner", "The Golden Era", "San Francisco Bay Times", "AsianWeek", "Street Sheet", "The Daily Alta California", "Bay Area Reporter", "San Francisco Call", "San Francisco News-Call Bulletin Newspaper"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.2526315789473684, "ans_precission": 0.8, "ans_recall": 0.15, "path_f1": 0.2526315789473684, "path_precision": 0.8, "path_recall": 0.15, "path_ans_f1": 0.2526315789473684, "path_ans_precision": 0.8, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales -> location.location.geolocation -> m.012p0bp1\n# Answer:\nRoman Catholic Archdiocese of Manizales", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales -> religion.religious_leadership_jurisdiction.size_or_type -> Archdiocese\n# Answer:\nRoman Catholic Archdiocese of Manizales", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales -> common.topic.article -> m.03qkh47\n# Answer:\nRoman Catholic Archdiocese of Manizales"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Feel the Madness -> common.topic.notable_types -> Canonical Version\n# Answer:\nFeel the Madness", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_98hd\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Be a Man -> music.album.genre -> Hip hop music\n# Answer:\nBe a Man", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Get Back -> music.recording.releases -> Be a Man\n# Answer:\nGet Back"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin\n# Answer:\nCharles Waring Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 18: 1870", "The principal works", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "From Darwin's unpublished notebooks", "Darwin", "On evolution", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "monograph on the sub-class Cirripedia", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 8: 1860", "Tesakneri tsagume\u030c", "Notes on the fertilization of orchids", "Origins", "The Correspondence of Charles Darwin, Volume 12: 1864", "H.M.S. Beagle in South America", "The Correspondence of Charles Darwin, Volume 9: 1861", "Del Plata a Tierra del Fuego", "Het uitdrukken van emoties bij mens en dier", "Darwinism stated by Darwin himself", "The Correspondence of Charles Darwin, Volume 16: 1868", "More Letters of Charles Darwin", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Metaphysics, Materialism, & the evolution of mind", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Variation of Animals and Plants under Domestication", "The living thoughts of Darwin", "Gesammelte kleinere Schriften", "The Orgin of Species", "The Power of Movement in Plants", "Darwin's notebooks on transmutation of species", "The Essential Darwin", "The Life of Erasmus Darwin", "Charles Darwin's letters", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Notebooks on transmutation of species", "A student's introduction to Charles Darwin", "Charles Darwin on the routes of male humble bees", "The Expression of the Emotions in Man and Animals", "The portable Darwin", "La facult\u00e9 motrice dans les plantes", "Die fundamente zur entstehung der arten", "On a remarkable bar of sandstone off Pernambuco", "The Voyage of the Beagle", "Darwin Compendium", "Die geschlechtliche Zuchtwahl", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Opsht\u0323amung fun menshen", "On the Movements and Habits of Climbing Plants", "Geological Observations on South America", "Questions about the breeding of animals", "From so simple a beginning", "The Descent of Man, and Selection in Relation to Sex", "Reise um die Welt 1831 - 36", "Charles Darwin's marginalia", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin for Today", "Resa kring jorden", "The voyage of Charles Darwin", "Cartas de Darwin 18251859", "Diario del Viaje de Un Naturalista Alrededor", "Darwin Darwin", "Kleinere geologische Abhandlungen", "The Correspondence of Charles Darwin, Volume 14: 1866", "Darwin's Ornithological notes", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Diary of the voyage of H.M.S. Beagle", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Proiskhozhdenie vidov", "The Formation of Vegetable Mould through the Action of Worms", "El Origin De Las Especies", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The collected papers of Charles Darwin", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Rejse om jorden", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Part I: Contributions to the Theory of Natural Selection / Part II", "Charles Darwin's natural selection", "ontstaan der soorten door natuurlijke teeltkeus", "Memorias y epistolario i\u0301ntimo", "Works", "Darwin en Patagonia", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Motsa ha-minim", "The Autobiography of Charles Darwin", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 10: 1862", "Darwin's insects", "The education of Darwin", "genese\u014ds t\u014dn eid\u014dn", "red notebook of Charles Darwin", "On Natural Selection", "The Darwin Reader First Edition", "Human nature, Darwin's view", "On the origin of species by means of natural selection", "The Structure and Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Evolution by natural selection", "The Darwin Reader Second Edition", "The geology of the voyage of H.M.S. Beagle", "Volcanic Islands", "Geological Observations on the Volcanic Islands", "The Different Forms of Flowers on Plants of the Same Species", "Leben und Briefe von Charles Darwin", "The foundations of the Origin of species", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Beagle letters", "South American Geology", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Darwin on humus and the earthworm", "La vie et la correspondance de Charles Darwin", "A Darwin Selection", "Fertilisation of Orchids", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Les mouvements et les habitudes des plantes grimpantes", "The Correspondence of Charles Darwin, Volume 11: 1863", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Darwin-Wallace", "Evolution", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "On the tendency of species to form varieties", "Charles Darwin", "Darwin from Insectivorous Plants to Worms", "The Life and Letters of Charles Darwin Volume 1", "The action of carbonate of ammonia on the roots of certain plants", "Reise eines Naturforschers um die Welt", "vari\u00eberen der huisdieren en cultuurplanten", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The\u0301orie de l'e\u0301volution", "Evolutionary Writings: Including the Autobiographies", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 15: 1867", "Les moyens d'expression chez les animaux", "Darwin and Henslow", "Voyage d'un naturaliste autour du monde", "The Life and Letters of Charles Darwin Volume 2", "Evolution and natural selection", "To the members of the Down Friendly Club"], "ans_acc": 0.0392156862745098, "ans_hit": 1, "ans_f1": 0.02566452795600367, "ans_precission": 0.7, "ans_recall": 0.013071895424836602, "path_f1": 0.25806451612903225, "path_precision": 1.0, "path_recall": 0.14814814814814814, "path_ans_f1": 0.07547169811320754, "path_ans_precision": 1.0, "path_ans_recall": 0.0392156862745098}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Come Spy with Me\n# Answer:\nCome Spy with Me", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Don't Look Back\n# Answer:\nDon't Look Back", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> people.profession.specializations -> Record producer\n# Answer:\nMusic executive"], "ground_truth": ["Melody Man", "Jesus Told Me To Love You", "Blame It On Love (Duet with Barbara Mitchell)", "It's Time to Stop Shoppin' Around", "We\u2019ve Come Too Far to End It Now", "Tears of a Clown", "Speak Low", "Skid Row", "I Second That Emotion", "Sweet Harmony", "Yes It's You Lady", "I Praise & Worship You Father", "Unless You Do It Again", "Rack Me Back", "Just Another Kiss", "Quiet Storm", "You Are So Beautiful (feat. Dave Koz)", "You've Really Got a Hold on Me", "Train of Thought", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "As You Do", "Tears of a Sweet Free Clown", "Share It", "She's Only a Baby Herself", "Mickey's Monkey", "The Agony and the Ecstasy", "Tea for Two", "No Time to Stop Believing", "Hold on to Your Love", "Coincidentally", "I Can't Give You Anything but Love", "I\u2019ve Got You Under My Skin", "I Second That Emotions", "Jingle Bells", "Tell Me Tomorrow, Part 1", "I've Made Love To You A Thousand Times", "Quiet Storm (Groove Boutique remix)", "Be Careful What You Wish For", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "I Know You by Heart", "Tell Me Tomorrow (12\\\" extended mix)", "Close Encounters of the First Kind", "Crusin'", "Quiet Storm (single version)", "Wishful Thinking", "We Are The Warriors", "Christmas Greeting", "The Hurt's On You", "There Will Come A Day ( I'm Gonna Happen To You )", "Ooo Baby Baby", "When A Woman Cries", "I Like Your Face", "Who's Sad", "Medley: Never My Love / Never Can Say Goodbye", "It's a Good Feeling", "Walk on By", "(It's The) Same Old Love", "Time Flies", "Asleep on My Love", "The Tracks Of My Tears", "And I Don't Love You", "Be Careful What You Wish For (instrumental)", "Rewind", "Fallin'", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "I'm Glad There Is You", "Pops, We Love You (disco)", "With Your Love Came", "A Child Is Waiting", "You Cannot Laugh Alone", "Satisfy You", "Cruisin'", "And I Love Her", "Gang Bangin'", "Mother's Son", "Will You Love Me Tomorrow", "Baby That's Backatcha", "I Can\u2019t Stand to See You Cry (Commercial version)", "Love Brought Us Here", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "I Can't Get Enough", "Girlfriend", "I Love The Nearness Of You", "One Heartbeat", "Get Ready", "You Made Me Feel Love", "There Will Come a Day (I'm Gonna Happen to You)", "The Family Song", "Happy (Love Theme From Lady Sings the Blues)", "Fulfill Your Need", "We've Saved the Best for Last", "Ebony Eyes", "Food For Thought", "Just To See Her Again", "Please Don't Take Your Love (feat. Carlos Santana)", "Open", "You Take Me Away", "Bad Girl", "Deck the Halls", "Never My Love / Never Can Say Goodbye", "Will You Love Me Tomorrow?", "Love Letters", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "The Tears of a Clown", "Away in the Manger / Coventry Carol", "Girl I'm Standing There", "It's Her Turn to Live", "Don't Know Why", "Being With You", "My World", "Theme From the Big Time", "Te Quiero Como Si No Hubiera Un Manana", "Driving Thru Life in the Fast Lane", "He Can Fix Anything", "If You Wanna Make Love", "Easy", "Take Me Through The Night", "Wanna Know My Mind", "Let Me Be The Clock", "No\u00ebl", "The Tears Of A Clown", "The Love Between Me and My Kids", "Season's Greetings from Smokey Robinson", "Keep Me", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "The Road to Damascus", "Baby Come Close", "The Tracks of My Heart", "I'll Keep My Light In My Window", "Blame It on Love", "Tears Of A Clown", "The Christmas Song", "Because of You It's the Best It's Ever Been", "Heavy On Pride (Light On Love)", "Why Do Happy Memories Hurt So Bad", "Noel", "Daylight & Darkness", "Christmas Every Day", "My Guy", "Tracks Of My Tears (Live)", "Going to a Go Go", "Hanging on by a Thread", "My Girl", "Really Gonna Miss You", "I Have Prayed On It", "You Go to My Head", "Little Girl, Little Girl", "In My Corner", "I Want You Back", "Sleepless Nights", "Love Bath", "It's Fantastic", "Same Old Love", "You're the One for Me (feat. Joss Stone)", "Quiet Storm (Groove Boutique Chill Jazz mix)", "I Hear The Children Singing", "The Tracks of My Tears (live)", "Wedding Song", "Just Like You", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Whatcha Gonna Do", "Standing On Jesus", "Everything You Touch", "You've Really Go a Hold on Me", "Don't Play Another Love Song", "The Way You Do (The Things You Do)", "Don't Wanna Be Just Physical", "Double Good Everything", "Aqui Con Tigo (Being With You)", "Yester Love", "Love So Fine", "Did You Know (Berry's Theme)", "Come to Me Soon", "Be Kind To The Growing Mind (with The Temptations)", "If You Can Want", "A Silent Partner in a Three-Way Love Affair", "Why", "It's A Good Night", "What's Too Much", "I've Made Love to You a Thousand Times", "Love Is The Light", "Please Come Home for Christmas", "Let Your Light Shine On Me", "Crusin", "I Am I Am", "Time After Time", "I'm in the Mood for Love", "I've Got You Under My Skin", "God Rest Ye Merry Gentlemen", "Ooo Baby Baby (live)", "Gone Forever", "Why Are You Running From My Love", "Night and Day", "Going to a Go-Go", "Winter Wonderland", "I Am, I Am", "You're Just My Life (feat. India.Arie)", "The Agony And The Ecstasy", "Shoe Soul", "It's Christmas Time", "Ever Had A Dream", "Save Me", "You Really Got a Hold on Me", "Pops, We Love You", "I Care About Detroit", "Holly", "So Bad", "A Tattoo", "Just a Touch Away", "More Than You Know", "If You Wanna Make Love (Come 'round Here)", "Christmas Everyday", "Nearness of You", "Love Don't Give No Reason", "Cruisin", "Tell Me Tomorrow", "Tracks of my Tears", "Love Don' Give No Reason (12 Inch Club Mix)", "Love' n Life", "The Track of My Tears", "You Are Forever", "Can't Fight Love", "Be Who You Are", "The Tracks of My Tears", "I Love Your Face", "Fly Me to the Moon (In Other Words)", "Jasmin", "Just My Soul Responding", "Our Love Is Here to Stay", "Let Me Be the Clock", "One Time", "Photograph in My Mind", "Tracks of My Tears", "Virgin Man", "That Place", "Be Kind to the Growing Mind", "Ain't That Peculiar", "You Don't Know What It's Like", "If You Want My Love", "Santa Claus is Coming to Town", "Shop Around", "Everything for Christmas", "When Smokey Sings Tears Of A Clown", "And I Don't Love You (Larry Levan instrumental dub)", "Ebony Eyes (Duet with Rick James)", "I Can't Find", "Little Girl Little Girl", "Will You Still Love Me Tomorrow", "Ooh Baby Baby", "More Love", "Some People Will Do Anything for Love", "Vitamin U", "Going to a Gogo", "Come by Here (Kum Ba Ya)", "Just to See Her", "Just Passing Through"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Hungarian language\n# Answer:\nHungarian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Montenegrin language\n# Answer:\nMontenegrin language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.article -> m.0l8p56r\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.sports -> Basketball\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Greece\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}

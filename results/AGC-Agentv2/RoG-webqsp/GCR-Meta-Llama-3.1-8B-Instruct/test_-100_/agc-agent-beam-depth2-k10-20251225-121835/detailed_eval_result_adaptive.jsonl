{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.04469y8"], "ground_truth": ["Governor of Tennessee", "United States Representative", "Speaker of the United States House of Representatives"], "effective_topk": 3, "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.3333333333333333, "path_recall": 0.16666666666666666, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97"], "ground_truth": ["Haitian Creole", "French"], "effective_topk": 5, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk"], "ground_truth": ["Ilyssa Fradin", "Hannah Gunn", "Melinda McGraw"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.season -> 1994\u201395 NBA season\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nm.02kbc58"], "ground_truth": ["Boston Celtics", "Phoenix Suns", "Miami Heat", "LSU Tigers men's basketball", "Orlando Magic", "Los Angeles Lakers", "Cleveland Cavaliers"], "effective_topk": 7, "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2702702702702703, "path_precision": 0.7142857142857143, "path_recall": 0.16666666666666666, "path_ans_f1": 0.40816326530612246, "path_ans_precision": 0.7142857142857143, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Joey Tribbiani\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nm.04fk_g9"], "ground_truth": ["New Rochelle"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.countries_spoken_in -> Laos\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.region -> Asia\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Cham language", "Saek language", "Hmong language", "Mon Language", "Lao Language", "Thai Language", "Nyaw Language", "Akha Language", "Phu Thai language", "Mlabri Language", "Malay, Pattani Language", "Khmer language", "Vietnamese Language"], "effective_topk": 13, "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.45859872611464964, "ans_precission": 0.9, "ans_recall": 0.3076923076923077, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.45859872611464964, "path_ans_precision": 0.9, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer -> common.topic.notable_types -> Profession\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician"], "ground_truth": ["Statesman", "Architect", "Archaeologist", "Lawyer", "Farmer", "Inventor", "Teacher", "Author", "Philosopher", "Writer"], "effective_topk": 10, "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.509090909090909, "path_ans_precision": 0.7, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1837-1843\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nBiology"], "ground_truth": ["The Voyage of the Beagle (Everyman Paperbacks)", "Darwin", "Fertilisation of Orchids", "Die geschlechtliche Zuchtwahl", "The structure and distribution of coral reefs.", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 14", "Charles Darwin on the routes of male humble bees", "Voyage of the Beagle", "A Darwin Selection", "The Voyage of the Beagle (Adventure Classics)", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Voyage d'un naturaliste autour du monde", "The Darwin Reader First Edition", "Questions about the breeding of animals", "Evolution", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Proiskhozhdenie vidov", "Insectivorous Plants", "The expression of the emotions in man and animals.", "The Expression Of The Emotions In Man And Animals", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Different Forms of Flowers on Plants of the Same Species", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 7", "Wu zhong qi yuan", "Notes on the fertilization of orchids", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 3", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 15", "Darwin Darwin", "Darwinism stated by Darwin himself", "The Voyage of the Beagle", "Darwin-Wallace", "Les moyens d'expression chez les animaux", "The Origin of Species (Enriched Classics)", "The principal works", "Voyage of the Beagle (Harvard Classics, Part 29)", "The Voyage of the Beagle (Unabridged Classics)", "El Origin De Las Especies", "The foundations of the Origin of species", "Voyage of the Beagle (NG Adventure Classics)", "Notebooks on transmutation of species", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Origin of Species (Great Minds Series)", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Motsa ha-minim", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Voyage of the Beagle (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 2", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Resa kring jorden", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Variation of Animals and Plants under Domestication", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "On evolution", "Origin of Species", "The Correspondence of Charles Darwin, Volume 13", "The action of carbonate of ammonia on the roots of certain plants", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The Formation of Vegetable Mould through the Action of Worms", "The Autobiography of Charles Darwin, and selected letters", "The Autobiography of Charles Darwin", "On the Movements and Habits of Climbing Plants", "Metaphysics, Materialism, & the evolution of mind", "The origin of species : complete and fully illustrated", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 8", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 8: 1860", "The autobiography of Charles Darwin", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Darwin for Today", "The descent of man, and selection in relation to sex", "The Correspondence of Charles Darwin, Volume 9: 1861", "Reise eines Naturforschers um die Welt", "The Origin of Species (Mentor)", "The Autobiography Of Charles Darwin", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Works", "The Origin of Species (Variorum Reprint)", "The autobiography of Charles Darwin, 1809-1882", "Voyage of the Beagle (Dover Value Editions)", "Reise um die Welt 1831 - 36", "Les mouvements et les habitudes des plantes grimpantes", "From Darwin's unpublished notebooks", "The origin of species", "Charles Darwin's natural selection", "The Descent of Man, and Selection in Relation to Sex", "Het uitdrukken van emoties bij mens en dier", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Beagle letters", "Origins", "The collected papers of Charles Darwin", "On Natural Selection", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Origin of Species (Great Books : Learning Channel)", "The Correspondence of Charles Darwin, Volume 16: 1868", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Autobiography of Charles Darwin (Dodo Press)", "From So Simple a Beginning", "The Origin of Species", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 5", "The Correspondence of Charles Darwin, Volume 10", "The Essential Darwin", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 12: 1864", "Die fundamente zur entstehung der arten", "The Correspondence of Charles Darwin, Volume 12", "The Correspondence of Charles Darwin, Volume 6", "On the tendency of species to form varieties", "The Correspondence of Charles Darwin, Volume 11", "The descent of man, and selection in relation to sex.", "Darwin and Henslow", "Autobiography of Charles Darwin", "vari\u00eberen der huisdieren en cultuurplanten", "The Voyage of the Beagle (Mentor)", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Structure and Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 10: 1862", "Memorias y epistolario i\u0301ntimo", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Descent of Man and Selection in Relation to Sex", "Origin of Species (Harvard Classics, Part 11)", "Charles Darwin's marginalia", "The structure and distribution of coral reefs", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Rejse om jorden", "Darwin on humus and the earthworm", "Darwin en Patagonia", "The Origin Of Species", "Volcanic Islands", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 9", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Expression of the Emotions in Man And Animals", "Del Plata a Tierra del Fuego", "The Darwin Reader Second Edition", "Part I: Contributions to the Theory of Natural Selection / Part II", "The living thoughts of Darwin", "The Correspondence of Charles Darwin, Volume 4", "More Letters of Charles Darwin", "The Autobiography of Charles Darwin [EasyRead Edition]", "The\u0301orie de l'e\u0301volution", "Darwin's journal", "The descent of man and selection in relation to sex.", "The voyage of the Beagle.", "Darwin's Ornithological notes", "The Correspondence of Charles Darwin, Volume 14: 1866", "H.M.S. Beagle in South America", "The voyage of Charles Darwin", "ontstaan der soorten door natuurlijke teeltkeus", "The Origin of Species (Oxford World's Classics)", "On a remarkable bar of sandstone off Pernambuco", "The Structure And Distribution of Coral Reefs", "The Origin of Species (Collector's Library)", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "To the members of the Down Friendly Club", "Human nature, Darwin's view", "Origin of Species (Everyman's University Paperbacks)", "The Correspondence of Charles Darwin, Volume 1", "The Orgin of Species", "The expression of the emotions in man and animals", "The geology of the voyage of H.M.S. Beagle", "Evolution and natural selection", "Tesakneri tsagume\u030c", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 17: 1869", "Diary of the voyage of H.M.S. Beagle", "Darwin's insects", "The Power of Movement in Plants", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Origin of Species (World's Classics)", "Voyage Of The Beagle", "La facult\u00e9 motrice dans les plantes", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Diario del Viaje de Un Naturalista Alrededor", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "red notebook of Charles Darwin", "Evolution by natural selection", "Leben und Briefe von Charles Darwin", "The Autobiography of Charles Darwin (Large Print)", "Darwin's notebooks on transmutation of species", "monograph on the sub-class Cirripedia", "Charles Darwin's letters", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Life of Erasmus Darwin", "La vie et la correspondance de Charles Darwin", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Geological Observations on South America", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 11: 1863", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The portable Darwin", "Kleinere geologische Abhandlungen", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle"], "effective_topk": 214, "ans_acc": 0.07009345794392523, "ans_hit": 1, "ans_f1": 0.018306636155606407, "ans_precission": 0.4444444444444444, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.13100436681222707, "path_ans_precision": 1.0, "path_ans_recall": 0.07009345794392523}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nm.07mmh5w"], "ground_truth": ["Denver Broncos"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> \u013dadov\u00fd \u0161t\u00edt -> location.location.containedby -> Europe\n# Answer:\n\u013dadov\u00fd \u0161t\u00edt", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard"], "ground_truth": ["Writer", "Author", "Bard", "Poet"], "effective_topk": 4, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nm.02sg5s6"], "ground_truth": ["Hayden Christensen"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Super Bowl XLI"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada"], "ground_truth": ["Canada"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05ckldy\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.locations -> Iraqi Kurdistan\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05cklf2\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Argentina", "France", "Iraq", "Australia", "United States of America", "Saudi Arabia", "United Kingdom"], "effective_topk": 7, "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.21428571428571427, "ans_precission": 0.42857142857142855, "ans_recall": 0.14285714285714285, "path_f1": 0.09375, "path_precision": 0.42857142857142855, "path_recall": 0.05263157894736842, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ym"], "ground_truth": ["Brenda Song"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr"], "ground_truth": ["Return J. Meigs, Jr.", "Ted Strickland", "John Kasich"], "effective_topk": 3, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5"], "ground_truth": ["LA Galaxy"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca"], "ground_truth": ["Salamanca"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Lyndon B. Johnson"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan"], "ground_truth": ["Okuma", "Japan"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Scotland", "Wales", "Northern Ireland"], "effective_topk": 4, "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.25, "path_precision": 0.25, "path_recall": 0.25, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River"], "ground_truth": ["Central Time Zone"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Philosopher", "Writer", "Physician"], "effective_topk": 3, "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.6666666666666666, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons"], "ground_truth": ["Sal Gibson", "Francine Lons", "Leon Cole"], "effective_topk": 3, "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system"], "ground_truth": ["Provisional government", "Semi-presidential system"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh"], "ground_truth": ["Memphis"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> influence.influence_node.influenced_by -> Arthur Conan Doyle\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore -> influence.influence_node.influenced_by -> H. P. Lovecraft\n# Answer:\nAlan Moore"], "ground_truth": ["Baltimore"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.geolocation -> m.0clwfck\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> g.11b7v_3l1h\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> m.042znwp\n# Answer:\nArbor Place Mall"], "ground_truth": ["Arbor Place Mall", "Fernbank Science Center", "Underground Atlanta", "Variety Playhouse", "Six Flags Over Georgia", "Martin Luther King, Jr. National Historic Site", "Georgia World Congress Center", "Six Flags White Water", "Fernbank Museum of Natural History", "Atlanta Ballet", "Centennial Olympic Park", "CNN Center", "Fox Theatre", "Omni Coliseum", "Atlanta Cyclorama & Civil War Museum", "Margaret Mitchell House & Museum", "The Tabernacle", "Zoo Atlanta", "Cobb Energy Performing Arts Centre", "Philips Arena", "Atlanta Symphony Orchestra", "Turner Field", "Georgia Dome", "Center for Puppetry Arts", "Georgia Aquarium", "Atlanta Marriott Marquis", "Masquerade", "Atlanta History Center", "Four Seasons Hotel Atlanta", "Georgia State Capitol", "World of Coca-Cola", "Jimmy Carter Library and Museum", "Peachtree Road Race", "Hyatt Regency Atlanta", "Woodruff Arts Center", "Atlanta Jewish Film Festival"], "effective_topk": 36, "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15384615384615385, "ans_precission": 1.0, "ans_recall": 0.08333333333333333, "path_f1": 0.15384615384615385, "path_precision": 1.0, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 1.0, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w"], "ground_truth": ["Electoral district of South Brisbane"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alf Roberts\n# Answer:\nTony Warren"], "ground_truth": ["William Roache"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["2005"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia"], "ground_truth": ["AUD"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj"], "ground_truth": ["Central European Time Zone"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr."], "ground_truth": ["Carolina Panthers"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith"], "ground_truth": ["Mountain Time Zone"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.ethnicity -> White people\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2"], "ground_truth": ["John Kerry", "Gene Amondson", "Michael Peroutka", "Ralph Nader"], "effective_topk": 4, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nm.0j4jq57"], "ground_truth": ["Ayaan Hirsi Ali"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province"], "ground_truth": ["Gal\u00e1pagos Province", "Pacific Ocean", "Ecuador"], "effective_topk": 3, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Journals -> common.topic.notable_for -> g.1ypm_b95r\n# Answer:\nJournals", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Mistletoe -> music.album.release_type -> Single\n# Answer:\nMistletoe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nm.0115qhzk"], "ground_truth": ["Home to Mama", "All Around The World", "Never Say Never", "Thought Of You", "First Dance", "All Bad", "Baby", "Change Me", "Roller Coaster", "All That Matters", "Somebody to Love", "Right Here", "Never Let You Go", "Hold Tight", "Eenie Meenie", "Beauty And A Beat", "Wait for a Minute", "PYD", "Live My Life", "Pray", "As Long as You Love Me", "Heartbreaker", "Turn to You (Mother's Day Dedication)", "Bigger", "#thatPower", "Bad Day", "Confident", "Boyfriend", "Recovery", "Lolly", "Die in Your Arms"], "effective_topk": 31, "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.056338028169014086, "ans_precission": 0.2222222222222222, "ans_recall": 0.03225806451612903, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.1081081081081081, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman"], "ground_truth": ["Statesman", "Publisher", "Journalist", "Writer", "Physician"], "effective_topk": 5, "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.6, "ans_recall": 0.4, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nm.04st6lz", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w1gvc"], "ground_truth": ["Saguaro"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr"], "ground_truth": ["Saint Michael Parish"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America"], "ground_truth": ["1841-03-04"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles County\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills"], "ground_truth": ["Mission Hills"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> 700-160\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> 702-701\n# Answer:\nDaegu"], "ground_truth": ["Suwon"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions"], "ground_truth": ["Tawhid", "Masih ad-Dajjal", "Sharia", "Islamic holy books", "Predestination in Islam", "Islamic view of angels", "\u1e6c\u016bb\u0101", "God in Islam", "Entering Heaven alive", "Monotheism", "Mahdi", "Qiyamah", "Prophets in Islam"], "effective_topk": 13, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAldous Huxley"], "ground_truth": ["Tuberculosis"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nm.0pz073c"], "ground_truth": ["Nazi Germany"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25000000000000006, "path_precision": 0.4, "path_recall": 0.18181818181818182, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\ng.11b7_lvdf2"], "ground_truth": ["Actor", "Songwriter", "Singer"], "effective_topk": 3, "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.6666666666666666, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111"], "ground_truth": ["Wyandotte County"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\nm.06sbpz2"], "ground_truth": ["UCLA Bruins football"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.nationality -> United States of America\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.properties -> Composer\n# Answer:\nPlay"], "ground_truth": ["1976-08-10"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nm.03gkqtp"], "ground_truth": ["Mobile"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan"], "ground_truth": ["Manhattan"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z"], "ground_truth": ["Hinduism", "Islam", "Protestantism", "Catholicism"], "effective_topk": 4, "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666665, "path_precision": 0.75, "path_recall": 0.6, "path_ans_f1": 0.75, "path_ans_precision": 0.75, "path_ans_recall": 0.75}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subjects -> Ammunition belt\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subjects -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph"], "ground_truth": ["Firearm"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nFezziwig", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.010p_33b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> common.topic.image -> A Christmas Carol - Mr\n# Answer:\nFezziwig", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> common.topic.image -> Scrooges third visitor-John Leech,1843\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> film.film_character.portrayed_in_films -> m.010p_0hz\n# Answer:\nFezziwig"], "ground_truth": ["A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Puffin Classics)", "The Pickwick papers", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol (Ladybird Classics)", "David Copperfield", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Christmas Carol (Soundings)", "A Christmas Carol (Classic Fiction)", "Sketches by Boz", "A Tale of Two Cities (Courage Literary Classics)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Pacemaker Classic)", "Martin Chuzzlewit", "A Christmas Carol", "David Copperfield.", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Watermill Classic)", "The Old Curiosity Shop", "Bleak house", "A Tale of Two Cities (Isis Clear Type Classic)", "The cricket on the hearth", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Dramascripts Classic Texts)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Adopted Classic)", "Our mutual friend.", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Gollancz Children's Classics)", "Great expectations.", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Audio Editions)", "Oliver Twist", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Children's Classics)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Whole Story)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Compact English Classics)", "Little Dorrit", "A TALE OF TWO CITIES", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Penguin Readers, Level 5)", "The old curiosity shop", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (New Longman Literature)", "Bleak House.", "A Christmas Carol (Value Books)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Cyber Classics)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Collector's Library)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Classic Fiction)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Illustrated Classics)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Watermill Classics)", "A CHRISTMAS CAROL", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities", "A Christmas Carol (Ladybird Children's Classics)", "Great Expectations.", "A Tale of Two Cities (Piccolo Books)", "The mystery of Edwin Drood", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (BBC Audio Series)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (The Classic Collection)", "Dombey and son", "The Pickwick Papers", "Great expectations", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Cover to Cover Classics)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (Oxford Playscripts)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Tor Classics)", "Great Expectations", "Bleak House", "Dombey and Son.", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Penguin Classics)", "A Christmas Carol (R)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Aladdin Classics)", "Dombey and Son", "A Christmas Carol (Classic Books on Cassettes Collection)", "Our mutual friend", "The old curiosity shop.", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Puffin Choice)", "Hard times", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (The Kennett Library)", "The life and adventures of Nicholas Nickleby"], "effective_topk": 169, "ans_acc": 0.011834319526627219, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.21951219512195122, "path_precision": 0.3333333333333333, "path_recall": 0.16363636363636364, "path_ans_f1": 0.023054755043227664, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.011834319526627219}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nm.049y3kf"], "ground_truth": ["Vladimir Lenin"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Panama\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> base.folklore.mythical_creature.similar_mythical_creature_s -> m.05d0ybv\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar"], "ground_truth": ["United States Dollar"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.risk_factor.diseases -> Ptosis\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nLung cancer"], "ground_truth": ["Lung cancer", "Brain tumor"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk -> tv.regular_tv_appearance.actor -> Patricia McPherson\n# Answer:\nm.03lj4lk"], "ground_truth": ["William Daniels"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4"], "ground_truth": ["Williamson County"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond"], "ground_truth": ["Diamond"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn"], "ground_truth": ["Tracy Pollan"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nm.04fv9nd", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of Port Republic\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.locations -> Shenandoah Valley\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War"], "ground_truth": ["First Battle of Kernstown", "Battle of Hancock", "Battle of White Oak Swamp", "Battle of Port Republic", "Battle of Hoke's Run", "How Few Remain", "First Battle of Rappahannock Station", "Romney Expedition", "Jackson's Valley Campaign", "Battle of Cedar Mountain", "Battle of Chancellorsville", "Battle of McDowell", "Battle of Chantilly", "Battle of Harpers Ferry", "First Battle of Winchester", "American Civil War", "Manassas Station Operations", "Second Battle of Bull Run", "Battle of Front Royal"], "effective_topk": 19, "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.17910447761194026, "ans_precission": 0.6, "ans_recall": 0.10526315789473684, "path_f1": 0.2376237623762376, "path_precision": 0.6, "path_recall": 0.14814814814814814, "path_ans_f1": 0.40723981900452483, "path_ans_precision": 0.9, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language"], "ground_truth": ["Maasai Language"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin"], "ground_truth": ["Deborah Read"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.risk_factor.diseases -> Jaundice\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.risk_factor.diseases -> Jaundice\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing -> common.topic.image -> Holy-infs\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing -> common.topic.article -> m.02815wh\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing -> common.topic.notable_for -> g.1257xhv1y\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nAntonio da Correggio"], "ground_truth": ["g.1239jd9p", "Portrait of a man in red chalk", "g.121wt37c", "Adoration of the Magi", "Salvator Mundi", "Ginevra de' Benci", "g.120vt1gz", "g.1213jb_b", "Annunciation", "Portrait of Isabella d'Este", "Vitruvian Man", "Madonna of the Carnation", "St. Jerome in the Wilderness", "The Virgin and Child with St. Anne", "Lucan portrait of Leonardo da Vinci", "Madonna of Laroque", "Portrait of a Musician", "The Last Supper", "Leda and the Swan", "g.121yh91r", "Benois Madonna", "Head of a Woman", "St. John the Baptist", "The Holy Infants Embracing", "Bacchus", "Portrait of a Young Fianc\u00e9e", "The Baptism of Christ", "Sala delle Asse", "Madonna and Child with St Joseph", "Lady with an Ermine", "The Virgin and Child with St Anne and St John the Baptist", "Madonna Litta", "Madonna of the Yarnwinder", "Leonardo's horse", "Horse and Rider", "Mona Lisa", "La belle ferronni\u00e8re", "The Battle of Anghiari", "g.12215rxg", "Virgin of the Rocks", "Drapery for a Seated Figure", "g.1224tf0c", "g.1219sb0g", "Medusa", "g.12314dm1"], "effective_topk": 45, "ans_acc": 0.022222222222222223, "ans_hit": 1, "ans_f1": 0.04166666666666667, "ans_precission": 0.3333333333333333, "ans_recall": 0.022222222222222223, "path_f1": 0.04255319148936171, "path_precision": 0.3333333333333333, "path_recall": 0.022727272727272728, "path_ans_f1": 0.04166666666666667, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.022222222222222223}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg -> base.aareas.schema.administrative_area.administrative_children -> Hallein District\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg -> base.aareas.schema.administrative_area.administrative_children -> Salzburg-Umgebung District\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "effective_topk": 5, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu"], "ground_truth": ["Nepal"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5"], "ground_truth": ["Franklin stove", "Lightning rod", "Bifocals", "Glass harmonica"], "effective_topk": 4, "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician"], "ground_truth": ["Composer", "Librettist", "Musician"], "effective_topk": 3, "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.3333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Arkadii Dragomoshchenko\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.administrative_divisions -> Dresden\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nGermany"], "ground_truth": ["Germany", "Austria", "Liechtenstein", "Switzerland", "East Germany", "Luxembourg", "Belgium"], "effective_topk": 7, "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.42857142857142855, "path_precision": 0.8571428571428571, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.subject_of -> Stephen Melton\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock"], "ground_truth": ["Pop music", "Experimental rock", "Experimental music", "Pop rock", "Psychedelic rock", "Soft rock", "Blues rock", "Rock music", "Art rock"], "effective_topk": 9, "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.48484848484848486, "ans_precission": 0.8888888888888888, "ans_recall": 0.3333333333333333, "path_f1": 0.48484848484848486, "path_precision": 0.8888888888888888, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6837606837606838, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.05kfbwl"], "ground_truth": ["Michael Bennet", "Mark Udall"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk"], "ground_truth": ["Denmark"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> location.location.containedby -> United States of America\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geometry -> m.055f4wk\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11btt54h7d\n# Answer:\ng.11btt54h7d", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geolocation -> m.03dyr0d\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> common.topic.notable_types -> Postal Code\n# Answer:\n98103"], "ground_truth": ["98168", "98194", "98138", "98190", "98125", "98131", "98164", "98161", "98129", "98113", "98111", "98118", "98191", "98134", "98105", "98165", "98126", "98171", "98141", "98132", "98160", "98198", "98144", "98117", "98124", "98188", "98115", "98178", "98107", "98154", "98148", "98146", "98116", "98121", "98185", "98145", "98170", "98101", "98127", "98195", "98136", "98155", "98181", "98166", "98114", "98158", "98199", "98184", "98139", "98133", "98106", "98174", "98122", "98175", "98119-4114", "98103", "98104", "98108", "98112", "98102", "98119", "98109", "98177"], "effective_topk": 63, "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08917197452229297, "ans_precission": 0.7, "ans_recall": 0.047619047619047616, "path_f1": 0.08917197452229297, "path_precision": 0.7, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08917197452229297, "path_ans_precision": 0.7, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith"], "ground_truth": ["Jada Pinkett Smith"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters"], "ground_truth": ["Chinese characters", "'Phags-pa script", "Simplified Chinese character", "N\u00fcshu script", "Traditional Chinese characters"], "effective_topk": 5, "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq"], "ground_truth": ["Pat Nixon"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.2, "path_recall": 0.14285714285714285, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_tlrg2 -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_tlrg2", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdh -> tv.regular_tv_appearance.actor -> Marla Gibbs\n# Answer:\nm.03lkkdh", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdt -> tv.regular_tv_appearance.actor -> Paul Benedict\n# Answer:\nm.03lkkdt", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdn -> tv.regular_tv_appearance.actor -> Franklin Cover\n# Answer:\nm.03lkkdn", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdt -> tv.regular_tv_appearance.character -> Harry Bentley\n# Answer:\nm.03lkkdt"], "ground_truth": ["Berlinda Tolbert", "Sherman Hemsley", "Marla Gibbs", "Jay Hammer", "Mike Evans", "Damon Evans", "Paul Benedict", "Zara Cully", "Roxie Roker", "Franklin Cover", "Isabel Sanford"], "effective_topk": 11, "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1176470588235294, "path_precision": 0.4444444444444444, "path_recall": 0.06779661016949153, "path_ans_f1": 0.3380281690140845, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> book.written_work.subjects -> Adventure\n# Answer:\n1st to Die", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.newspaper.issues -> San Francisco Bay Guardian, 24 Nov 1999\n# Answer:\nSan Francisco Bay Guardian"], "ground_truth": ["AsianWeek", "The Daily Alta California", "Bay Area Reporter", "San Francisco Daily", "San Francisco News-Call Bulletin Newspaper", "San Francisco Bay Times", "California Star", "San Francisco Bay Guardian", "Free Society", "San Francisco Call", "Sing Tao Daily", "Synapse", "San Francisco Bay View", "San Francisco Chronicle", "Dock of the Bay", "Street Sheet", "San Francisco Foghorn", "The Golden Era", "San Francisco Business Times", "The San Francisco Examiner"], "effective_topk": 20, "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.25149700598802394, "ans_precission": 0.7777777777777778, "ans_recall": 0.15, "path_f1": 0.25149700598802394, "path_precision": 0.7777777777777778, "path_recall": 0.15, "path_ans_f1": 0.25149700598802394, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope"], "ground_truth": ["Europe"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1837-1843\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Darwin", "Fertilisation of Orchids", "Die geschlechtliche Zuchtwahl", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Charles Darwin on the routes of male humble bees", "Darwin from Insectivorous Plants to Worms", "A Darwin Selection", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Voyage d'un naturaliste autour du monde", "The Darwin Reader First Edition", "Questions about the breeding of animals", "Evolution", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Proiskhozhdenie vidov", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Different Forms of Flowers on Plants of the Same Species", "The Expression of the Emotions in Man and Animals", "The Life and Letters of Charles Darwin Volume 1", "Notes on the fertilization of orchids", "Wu zhong qi yuan", "A student's introduction to Charles Darwin", "Darwin Darwin", "Darwinism stated by Darwin himself", "The Voyage of the Beagle", "Darwin-Wallace", "Les moyens d'expression chez les animaux", "The principal works", "South American Geology", "El Origin De Las Especies", "Geological Observations on the Volcanic Islands", "The foundations of the Origin of species", "Motsa ha-minim", "Notebooks on transmutation of species", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The education of Darwin", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Resa kring jorden", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Variation of Animals and Plants under Domestication", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "On evolution", "The action of carbonate of ammonia on the roots of certain plants", "The Formation of Vegetable Mould through the Action of Worms", "The Autobiography of Charles Darwin", "On the Movements and Habits of Climbing Plants", "Metaphysics, Materialism, & the evolution of mind", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "From so simple a beginning", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin for Today", "The Correspondence of Charles Darwin, Volume 9: 1861", "Reise eines Naturforschers um die Welt", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Works", "Reise um die Welt 1831 - 36", "Les mouvements et les habitudes des plantes grimpantes", "From Darwin's unpublished notebooks", "Charles Darwin's natural selection", "The Descent of Man, and Selection in Relation to Sex", "Het uitdrukken van emoties bij mens en dier", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Beagle letters", "Origins", "The collected papers of Charles Darwin", "On Natural Selection", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 16: 1868", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Darwin Compendium", "The Essential Darwin", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 12: 1864", "Die fundamente zur entstehung der arten", "On the tendency of species to form varieties", "Darwin and Henslow", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Structure and Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 10: 1862", "Memorias y epistolario i\u0301ntimo", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 15: 1867", "Charles Darwin's marginalia", "Rejse om jorden", "Darwin on humus and the earthworm", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Darwin en Patagonia", "Volcanic Islands", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 18: 1870", "Evolutionary Writings: Including the Autobiographies", "Del Plata a Tierra del Fuego", "The Darwin Reader Second Edition", "Part I: Contributions to the Theory of Natural Selection / Part II", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The living thoughts of Darwin", "More Letters of Charles Darwin", "The\u0301orie de l'e\u0301volution", "Darwin's journal", "Darwin's Ornithological notes", "The Correspondence of Charles Darwin, Volume 14: 1866", "ontstaan der soorten door natuurlijke teeltkeus", "The voyage of Charles Darwin", "On a remarkable bar of sandstone off Pernambuco", "H.M.S. Beagle in South America", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "To the members of the Down Friendly Club", "Human nature, Darwin's view", "The Life and Letters of Charles Darwin Volume 2", "The Orgin of Species", "The geology of the voyage of H.M.S. Beagle", "Evolution and natural selection", "Tesakneri tsagume\u030c", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 17: 1869", "Diary of the voyage of H.M.S. Beagle", "Darwin's insects", "The Power of Movement in Plants", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "La facult\u00e9 motrice dans les plantes", "Diario del Viaje de Un Naturalista Alrededor", "red notebook of Charles Darwin", "Evolution by natural selection", "Leben und Briefe von Charles Darwin", "Darwin's notebooks on transmutation of species", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "monograph on the sub-class Cirripedia", "Charles Darwin's letters", "The Life of Erasmus Darwin", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "La vie et la correspondance de Charles Darwin", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Geological Observations on South America", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 11: 1863", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The portable Darwin", "Kleinere geologische Abhandlungen", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle"], "effective_topk": 153, "ans_acc": 0.0457516339869281, "ans_hit": 1, "ans_f1": 0.02554278416347382, "ans_precission": 0.5555555555555556, "ans_recall": 0.013071895424836602, "path_f1": 0.3125, "path_precision": 1.0, "path_recall": 0.18518518518518517, "path_ans_f1": 0.0875, "path_ans_precision": 1.0, "path_ans_recall": 0.0457516339869281}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.02shm15"], "ground_truth": ["New York City"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness -> music.recording.releases -> Gettin' Ready\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> I've Been Good to You -> music.composition.recordings -> I've Been Good To You\n# Answer:\nI've Been Good to You"], "ground_truth": ["Be Kind to the Growing Mind", "Love Brought Us Here", "You Really Got a Hold on Me", "You've Really Got a Hold on Me", "If You Wanna Make Love", "I've Made Love To You A Thousand Times", "Tell Me Tomorrow", "Asleep on My Love", "Be Who You Are", "It's Time to Stop Shoppin' Around", "Tell Me Tomorrow, Part 1", "Will You Still Love Me Tomorrow", "Gang Bangin'", "You're Just My Life (feat. India.Arie)", "Little Girl, Little Girl", "Come to Me Soon", "My Guy", "Happy (Love Theme From Lady Sings the Blues)", "It's Christmas Time", "Tracks Of My Tears (Live)", "Fallin'", "Just Another Kiss", "Time After Time", "I Like Your Face", "Save Me", "Who's Sad", "It's Fantastic", "Mother's Son", "Let Me Be The Clock", "Same Old Love", "Skid Row", "Be Careful What You Wish For (instrumental)", "Jasmin", "Everything for Christmas", "Theme From the Big Time", "Winter Wonderland", "Shop Around", "We've Saved the Best for Last", "Blame It on Love", "Did You Know (Berry's Theme)", "(It's The) Same Old Love", "Ain't That Peculiar", "Why", "Open", "Blame It On Love (Duet with Barbara Mitchell)", "Just to See Her", "More Than You Know", "Crusin", "Walk on By", "Let Your Light Shine On Me", "Photograph in My Mind", "The Christmas Song", "I Hear The Children Singing", "Be Careful What You Wish For", "Train of Thought", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Tell Me Tomorrow (12\\\" extended mix)", "As You Do", "No Time to Stop Believing", "You Don't Know What It's Like", "Holly", "Nearness of You", "Close Encounters of the First Kind", "Mickey's Monkey", "Let Me Be the Clock", "Be Kind To The Growing Mind (with The Temptations)", "Will You Love Me Tomorrow?", "The Agony and the Ecstasy", "She's Only a Baby Herself", "Being With You", "I Second That Emotions", "Crusin'", "Just Passing Through", "Medley: Never My Love / Never Can Say Goodbye", "The Hurt's On You", "I Love The Nearness Of You", "Why Do Happy Memories Hurt So Bad", "Away in the Manger / Coventry Carol", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "I'll Keep My Light In My Window", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "The Love Between Me and My Kids", "Just Like You", "Tea for Two", "There Will Come A Day ( I'm Gonna Happen To You )", "Quiet Storm (Groove Boutique remix)", "Christmas Everyday", "You Are Forever", "Take Me Through The Night", "One Time", "Never My Love / Never Can Say Goodbye", "Going to a Go-Go", "Speak Low", "The Road to Damascus", "Pops, We Love You", "The Family Song", "Satisfy You", "Just a Touch Away", "Ooo Baby Baby (live)", "I Praise & Worship You Father", "I Have Prayed On It", "Night and Day", "Cruisin'", "Christmas Greeting", "Ebony Eyes", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Baby Come Close", "Love Don' Give No Reason (12 Inch Club Mix)", "The Way You Do (The Things You Do)", "The Tears Of A Clown", "Please Come Home for Christmas", "It's A Good Night", "Food For Thought", "Love Don't Give No Reason", "I Can't Find", "It's a Good Feeling", "Will You Love Me Tomorrow", "You Are So Beautiful (feat. Dave Koz)", "You Go to My Head", "You've Really Go a Hold on Me", "Love Letters", "Yes It's You Lady", "Quiet Storm", "Fly Me to the Moon (In Other Words)", "Can't Fight Love", "I Second That Emotion", "Don't Wanna Be Just Physical", "Wedding Song", "There Will Come a Day (I'm Gonna Happen to You)", "Coincidentally", "Share It", "Keep Me", "A Silent Partner in a Three-Way Love Affair", "Ever Had A Dream", "I Care About Detroit", "Ebony Eyes (Duet with Rick James)", "Love Bath", "Our Love Is Here to Stay", "You Take Me Away", "My Girl", "Jesus Told Me To Love You", "The Tears of a Clown", "Season's Greetings from Smokey Robinson", "Rewind", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Time Flies", "Love Is The Light", "I Am I Am", "More Love", "Hold on to Your Love", "Ooo Baby Baby", "When A Woman Cries", "Deck the Halls", "Double Good Everything", "I Can't Get Enough", "A Child Is Waiting", "Just My Soul Responding", "Little Girl Little Girl", "Sleepless Nights", "Sweet Harmony", "Girl I'm Standing There", "Because of You It's the Best It's Ever Been", "With Your Love Came", "I Can't Give You Anything but Love", "Easy", "If You Can Want", "We\u2019ve Come Too Far to End It Now", "In My Corner", "It's Her Turn to Live", "That Place", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Jingle Bells", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Wanna Know My Mind", "Tears Of A Clown", "Virgin Man", "And I Don't Love You (Larry Levan instrumental dub)", "Rack Me Back", "Going to a Gogo", "I've Got You Under My Skin", "Why Are You Running From My Love", "Driving Thru Life in the Fast Lane", "Noel", "Girlfriend", "The Track of My Tears", "The Tracks of My Tears (live)", "Going to a Go Go", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Baby That's Backatcha", "Aqui Con Tigo (Being With You)", "Bad Girl", "Daylight & Darkness", "Love' n Life", "So Bad", "Don't Know Why", "You Made Me Feel Love", "Standing On Jesus", "Quiet Storm (single version)", "Get Ready", "Cruisin", "Gone Forever", "You're the One for Me (feat. Joss Stone)", "He Can Fix Anything", "The Tracks of My Heart", "Shoe Soul", "Love So Fine", "You Cannot Laugh Alone", "Everything You Touch", "My World", "God Rest Ye Merry Gentlemen", "Tracks of My Tears", "The Tracks of My Tears", "Come by Here (Kum Ba Ya)", "One Heartbeat", "Tears of a Clown", "I Want You Back", "Really Gonna Miss You", "I'm Glad There Is You", "I've Made Love to You a Thousand Times", "Heavy On Pride (Light On Love)", "Melody Man", "Vitamin U", "Christmas Every Day", "Whatcha Gonna Do", "Santa Claus is Coming to Town", "I Can\u2019t Stand to See You Cry (Commercial version)", "Unless You Do It Again", "I\u2019ve Got You Under My Skin", "Don't Play Another Love Song", "Wishful Thinking", "The Agony And The Ecstasy", "The Tracks Of My Tears", "When Smokey Sings Tears Of A Clown", "I Know You by Heart", "Yester Love", "And I Love Her", "If You Want My Love", "What's Too Much", "Tears of a Sweet Free Clown", "Just To See Her Again", "If You Wanna Make Love (Come 'round Here)", "Hanging on by a Thread", "And I Don't Love You", "I Love Your Face", "No\u00ebl", "I Am, I Am", "Fulfill Your Need", "Tracks of my Tears", "Pops, We Love You (disco)", "We Are The Warriors", "Ooh Baby Baby", "Some People Will Do Anything for Love", "Please Don't Take Your Love (feat. Carlos Santana)", "A Tattoo", "Te Quiero Como Si No Hubiera Un Manana", "I'm in the Mood for Love"], "effective_topk": 256, "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Lyndon B. Johnson"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language"], "ground_truth": ["Serbian language"], "effective_topk": 5, "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
